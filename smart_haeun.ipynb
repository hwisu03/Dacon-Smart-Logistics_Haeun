{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d46022e-5b86-49e6-874f-8eaa3d79b9b5",
   "metadata": {},
   "source": [
    "# 스마트 해운 물류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f8b0a48-4015-4ee2-8847-fc86834a6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv('./open/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0150d1ad-1734-4adc-98d5-02a335ec2862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>X_01</th>\n",
       "      <th>X_02</th>\n",
       "      <th>X_03</th>\n",
       "      <th>X_04</th>\n",
       "      <th>X_05</th>\n",
       "      <th>X_06</th>\n",
       "      <th>X_07</th>\n",
       "      <th>X_08</th>\n",
       "      <th>X_09</th>\n",
       "      <th>...</th>\n",
       "      <th>X_43</th>\n",
       "      <th>X_44</th>\n",
       "      <th>X_45</th>\n",
       "      <th>X_46</th>\n",
       "      <th>X_47</th>\n",
       "      <th>X_48</th>\n",
       "      <th>X_49</th>\n",
       "      <th>X_50</th>\n",
       "      <th>X_51</th>\n",
       "      <th>X_52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.248234</td>\n",
       "      <td>0.521686</td>\n",
       "      <td>0.507419</td>\n",
       "      <td>0.391153</td>\n",
       "      <td>0.583795</td>\n",
       "      <td>0.663798</td>\n",
       "      <td>0.501200</td>\n",
       "      <td>0.571666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260703</td>\n",
       "      <td>0.428539</td>\n",
       "      <td>0.583749</td>\n",
       "      <td>0.746367</td>\n",
       "      <td>0.389120</td>\n",
       "      <td>0.524662</td>\n",
       "      <td>0.532655</td>\n",
       "      <td>0.600385</td>\n",
       "      <td>0.327990</td>\n",
       "      <td>0.577894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.237060</td>\n",
       "      <td>0.537939</td>\n",
       "      <td>0.545298</td>\n",
       "      <td>0.359449</td>\n",
       "      <td>0.657034</td>\n",
       "      <td>0.647725</td>\n",
       "      <td>0.501224</td>\n",
       "      <td>0.586882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253675</td>\n",
       "      <td>0.374611</td>\n",
       "      <td>0.657051</td>\n",
       "      <td>0.768609</td>\n",
       "      <td>0.373918</td>\n",
       "      <td>0.497134</td>\n",
       "      <td>0.492148</td>\n",
       "      <td>0.612316</td>\n",
       "      <td>0.306123</td>\n",
       "      <td>0.640685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.244556</td>\n",
       "      <td>0.541783</td>\n",
       "      <td>0.511458</td>\n",
       "      <td>0.380849</td>\n",
       "      <td>0.673393</td>\n",
       "      <td>0.649568</td>\n",
       "      <td>0.485117</td>\n",
       "      <td>0.565430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262817</td>\n",
       "      <td>0.442951</td>\n",
       "      <td>0.673385</td>\n",
       "      <td>0.750324</td>\n",
       "      <td>0.372285</td>\n",
       "      <td>0.566680</td>\n",
       "      <td>0.527558</td>\n",
       "      <td>0.592817</td>\n",
       "      <td>0.319520</td>\n",
       "      <td>0.573768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.241627</td>\n",
       "      <td>0.600781</td>\n",
       "      <td>0.514907</td>\n",
       "      <td>0.374210</td>\n",
       "      <td>0.618073</td>\n",
       "      <td>0.668874</td>\n",
       "      <td>0.494310</td>\n",
       "      <td>0.584442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262562</td>\n",
       "      <td>0.428725</td>\n",
       "      <td>0.618055</td>\n",
       "      <td>0.748490</td>\n",
       "      <td>0.372270</td>\n",
       "      <td>0.408100</td>\n",
       "      <td>0.532064</td>\n",
       "      <td>0.601000</td>\n",
       "      <td>0.313188</td>\n",
       "      <td>0.566182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.251017</td>\n",
       "      <td>0.504123</td>\n",
       "      <td>0.512723</td>\n",
       "      <td>0.378423</td>\n",
       "      <td>0.614282</td>\n",
       "      <td>0.644375</td>\n",
       "      <td>0.456430</td>\n",
       "      <td>0.553999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263064</td>\n",
       "      <td>0.442768</td>\n",
       "      <td>0.614234</td>\n",
       "      <td>0.751743</td>\n",
       "      <td>0.387120</td>\n",
       "      <td>0.578171</td>\n",
       "      <td>0.517872</td>\n",
       "      <td>0.604191</td>\n",
       "      <td>0.318684</td>\n",
       "      <td>0.600983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID   X_01      X_02      X_03      X_04      X_05      X_06  \\\n",
       "0  TEST_00000  0.027  0.248234  0.521686  0.507419  0.391153  0.583795   \n",
       "1  TEST_00001  0.021  0.237060  0.537939  0.545298  0.359449  0.657034   \n",
       "2  TEST_00002  0.020  0.244556  0.541783  0.511458  0.380849  0.673393   \n",
       "3  TEST_00003  0.011  0.241627  0.600781  0.514907  0.374210  0.618073   \n",
       "4  TEST_00004  0.019  0.251017  0.504123  0.512723  0.378423  0.614282   \n",
       "\n",
       "       X_07      X_08      X_09  ...      X_43      X_44      X_45      X_46  \\\n",
       "0  0.663798  0.501200  0.571666  ...  0.260703  0.428539  0.583749  0.746367   \n",
       "1  0.647725  0.501224  0.586882  ...  0.253675  0.374611  0.657051  0.768609   \n",
       "2  0.649568  0.485117  0.565430  ...  0.262817  0.442951  0.673385  0.750324   \n",
       "3  0.668874  0.494310  0.584442  ...  0.262562  0.428725  0.618055  0.748490   \n",
       "4  0.644375  0.456430  0.553999  ...  0.263064  0.442768  0.614234  0.751743   \n",
       "\n",
       "       X_47      X_48      X_49      X_50      X_51      X_52  \n",
       "0  0.389120  0.524662  0.532655  0.600385  0.327990  0.577894  \n",
       "1  0.373918  0.497134  0.492148  0.612316  0.306123  0.640685  \n",
       "2  0.372285  0.566680  0.527558  0.592817  0.319520  0.573768  \n",
       "3  0.372270  0.408100  0.532064  0.601000  0.313188  0.566182  \n",
       "4  0.387120  0.578171  0.517872  0.604191  0.318684  0.600983  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "996cf7ad-e7a0-4474-8eb2-9530124ecfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./open/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5564514e-c443-49af-a767-c80fc39a2fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>X_01</th>\n",
       "      <th>X_02</th>\n",
       "      <th>X_03</th>\n",
       "      <th>X_04</th>\n",
       "      <th>X_05</th>\n",
       "      <th>X_06</th>\n",
       "      <th>X_07</th>\n",
       "      <th>X_08</th>\n",
       "      <th>X_09</th>\n",
       "      <th>...</th>\n",
       "      <th>X_44</th>\n",
       "      <th>X_45</th>\n",
       "      <th>X_46</th>\n",
       "      <th>X_47</th>\n",
       "      <th>X_48</th>\n",
       "      <th>X_49</th>\n",
       "      <th>X_50</th>\n",
       "      <th>X_51</th>\n",
       "      <th>X_52</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.242994</td>\n",
       "      <td>0.538536</td>\n",
       "      <td>0.522295</td>\n",
       "      <td>0.374494</td>\n",
       "      <td>0.555348</td>\n",
       "      <td>0.639091</td>\n",
       "      <td>0.494800</td>\n",
       "      <td>0.584233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.435885</td>\n",
       "      <td>0.555359</td>\n",
       "      <td>0.751714</td>\n",
       "      <td>0.376801</td>\n",
       "      <td>0.466993</td>\n",
       "      <td>0.527585</td>\n",
       "      <td>0.598101</td>\n",
       "      <td>0.312160</td>\n",
       "      <td>0.582797</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.240380</td>\n",
       "      <td>0.517223</td>\n",
       "      <td>0.538976</td>\n",
       "      <td>0.371149</td>\n",
       "      <td>0.693825</td>\n",
       "      <td>0.663667</td>\n",
       "      <td>0.530931</td>\n",
       "      <td>0.577200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479859</td>\n",
       "      <td>0.693855</td>\n",
       "      <td>0.748955</td>\n",
       "      <td>0.356118</td>\n",
       "      <td>0.613461</td>\n",
       "      <td>0.508069</td>\n",
       "      <td>0.569814</td>\n",
       "      <td>0.313351</td>\n",
       "      <td>0.570513</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.248946</td>\n",
       "      <td>0.547109</td>\n",
       "      <td>0.466713</td>\n",
       "      <td>0.415830</td>\n",
       "      <td>0.656887</td>\n",
       "      <td>0.681782</td>\n",
       "      <td>0.580773</td>\n",
       "      <td>0.527069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416115</td>\n",
       "      <td>0.656884</td>\n",
       "      <td>0.750059</td>\n",
       "      <td>0.417200</td>\n",
       "      <td>0.525393</td>\n",
       "      <td>0.551947</td>\n",
       "      <td>0.639860</td>\n",
       "      <td>0.342627</td>\n",
       "      <td>0.573139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.245877</td>\n",
       "      <td>0.527870</td>\n",
       "      <td>0.515534</td>\n",
       "      <td>0.379199</td>\n",
       "      <td>0.594391</td>\n",
       "      <td>0.663816</td>\n",
       "      <td>0.494931</td>\n",
       "      <td>0.581796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436761</td>\n",
       "      <td>0.594364</td>\n",
       "      <td>0.746297</td>\n",
       "      <td>0.374659</td>\n",
       "      <td>0.694290</td>\n",
       "      <td>0.532705</td>\n",
       "      <td>0.581142</td>\n",
       "      <td>0.316417</td>\n",
       "      <td>0.562656</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.239237</td>\n",
       "      <td>0.566087</td>\n",
       "      <td>0.514384</td>\n",
       "      <td>0.378451</td>\n",
       "      <td>0.610543</td>\n",
       "      <td>0.644811</td>\n",
       "      <td>0.508567</td>\n",
       "      <td>0.593614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422407</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>0.749565</td>\n",
       "      <td>0.372742</td>\n",
       "      <td>0.565549</td>\n",
       "      <td>0.535573</td>\n",
       "      <td>0.580484</td>\n",
       "      <td>0.314982</td>\n",
       "      <td>0.591692</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID   X_01      X_02      X_03      X_04      X_05      X_06  \\\n",
       "0  TRAIN_00000  0.016  0.242994  0.538536  0.522295  0.374494  0.555348   \n",
       "1  TRAIN_00001  0.019  0.240380  0.517223  0.538976  0.371149  0.693825   \n",
       "2  TRAIN_00002  0.012  0.248946  0.547109  0.466713  0.415830  0.656887   \n",
       "3  TRAIN_00003  0.013  0.245877  0.527870  0.515534  0.379199  0.594391   \n",
       "4  TRAIN_00004  0.024  0.239237  0.566087  0.514384  0.378451  0.610543   \n",
       "\n",
       "       X_07      X_08      X_09  ...      X_44      X_45      X_46      X_47  \\\n",
       "0  0.639091  0.494800  0.584233  ...  0.435885  0.555359  0.751714  0.376801   \n",
       "1  0.663667  0.530931  0.577200  ...  0.479859  0.693855  0.748955  0.356118   \n",
       "2  0.681782  0.580773  0.527069  ...  0.416115  0.656884  0.750059  0.417200   \n",
       "3  0.663816  0.494931  0.581796  ...  0.436761  0.594364  0.746297  0.374659   \n",
       "4  0.644811  0.508567  0.593614  ...  0.422407  0.610526  0.749565  0.372742   \n",
       "\n",
       "       X_48      X_49      X_50      X_51      X_52  target  \n",
       "0  0.466993  0.527585  0.598101  0.312160  0.582797       0  \n",
       "1  0.613461  0.508069  0.569814  0.313351  0.570513      20  \n",
       "2  0.525393  0.551947  0.639860  0.342627  0.573139       1  \n",
       "3  0.694290  0.532705  0.581142  0.316417  0.562656      19  \n",
       "4  0.565549  0.535573  0.580484  0.314982  0.591692      15  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0433913c-7517-462d-a646-25f1fc21abfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0     1033\n",
      "20    1033\n",
      "1     1033\n",
      "19    1033\n",
      "15    1033\n",
      "8     1033\n",
      "16    1033\n",
      "12    1033\n",
      "14    1033\n",
      "18    1033\n",
      "3     1033\n",
      "4     1033\n",
      "5     1033\n",
      "11    1033\n",
      "13    1033\n",
      "6     1033\n",
      "10    1033\n",
      "2     1033\n",
      "9     1033\n",
      "17    1033\n",
      "7     1033\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96e977c-088e-4d78-8fa7-b41788cc9391",
   "metadata": {},
   "source": [
    "### 데이터 전처리(1) </br> \n",
    "필요없는 ID 열 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d635b18-3791-4e9c-bb83-14845b4e9d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new = train.drop(\"ID\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "213d8ccf-58f8-4ecf-b785-bbb4af478dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new_label = train_new.iloc[:,-1]\n",
    "train_new_data = train_new.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbad7d-9068-4dcf-8e79-77df6c0f9550",
   "metadata": {},
   "source": [
    "### 전처리(1)을 실행한 뒤의 학습 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2675f0f-aad9-4ea2-8de6-ee06f8a220eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7665360682184835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_new_data, train_new_label, test_size = 0.2, random_state = 42)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 100, random_state = 42, n_jobs = -1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_val)\n",
    "\n",
    "print(accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4832b7aa-a182-4687-8223-1cc01bf3fe01",
   "metadata": {},
   "source": [
    "#### 혼동행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "414dc3c0-2e7b-43ab-81d4-80d77cb8e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 49892 (\\N{HANGUL SYLLABLE SIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51228 (\\N{HANGUL SYLLABLE JE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45813 (\\N{HANGUL SYLLABLE DAB}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 47784 (\\N{HANGUL SYLLABLE MO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45944 (\\N{HANGUL SYLLABLE DEL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54775 (\\N{HANGUL SYLLABLE HES}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44040 (\\N{HANGUL SYLLABLE GAL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 47140 (\\N{HANGUL SYLLABLE RYEO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 54616 (\\N{HANGUL SYLLABLE HA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 45716 (\\N{HANGUL SYLLABLE NEUN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 48512 (\\N{HANGUL SYLLABLE BU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 48516 (\\N{HANGUL SYLLABLE BUN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 52286 (\\N{HANGUL SYLLABLE CAJ}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 44592 (\\N{HANGUL SYLLABLE GI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 51032 (\\N{HANGUL SYLLABLE YI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 50696 (\\N{HANGUL SYLLABLE YE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 52769 (\\N{HANGUL SYLLABLE CEUG}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLcAAASJCAYAAAAqiP7TAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3Xd0VFWjhvF3EtIoKSR0adKkSZcuVRAUQZBiBZQi2EB6UxEhFKUoTZr0JoodFLuI3A+w00SlKaGkEEpCAsncP5DoSBJmQsKejc/vrlnrcs6ZM08Ox5BvZ88eh9PpdAoAAAAAAACwkI/pAAAAAAAAACCrGNwCAAAAAACAtRjcAgAAAAAAgLUY3AIAAAAAAIC1GNwCAAAAAACAtRjcAgAAAAAAgLUY3AIAAAAAAIC1GNwCAAAAAACAtRjcAgAAAAAAgLUY3AIAAAAAAIC1GNwCAOSYH3/8UT179lTp0qUVGBiovHnzqmbNmpo8ebJiY2Nz9LW/++47NWnSRCEhIXI4HJo+fXq2v4bD4dBzzz2X7ee9ksWLF8vhcMjhcOjzzz+/bL/T6VTZsmXlcDjUtGnTLL3G7NmztXjxYo+e8/nnn2fYlF3mz5+v0NDQDB8VKlTIkePSs3v37kyfGxoaqn379nn9cRkpU6ZMps997bXXPDouPd27d8/0uY888ohHxwEAgP+mXKYDAADXp/nz56t///6qUKGChgwZokqVKun8+fPavn275s6dq2+++Ubr16/Psdd/+OGHdfbsWa1evVphYWEqVapUtr/GN998oxtuuCHbz+uufPnyaeHChZcNYH3xxRf67bfflC9fviyfe/bs2YqIiFCPHj3cfk7NmjX1zTffqFKlSll+3StJSkrS448/rhdeeOGyfRcuXEj7+8ju49KTkpKiKlWqaPPmzenub9SokVJSUrz+uIzEx8crOjpauXJd/uPi6NGjlZSU5NFx6Tl79qzWrVunli1bXrbv448/1oIFCzw6DgAA/DcxuAUAyHbffPON+vXrp9tuu01vvfWWAgIC0vbddtttGjRokDZu3JijDT///LN69+6tNm3a5Nhr1KtXL8fO7Y6uXbtqxYoVmjVrloKDg9O2L1y4UPXr19epU6euScf58+flcDgUHBxs/JoAAADgv4e3JQIAst2ECRPkcDg0b948l4GtS/z9/XXXXXel/Tk1NVWTJ0/WTTfdpICAABUsWFAPPfSQ/vjjD5fnNW3aVFWqVNG2bdvUuHFj5c6dWzfeeKMmTpyo1NRUSX+/Ze/ChQuaM2dO2tv3JOm5555L+///6dJzDhw4kLbt008/VdOmTRUeHq6goCCVKFFCnTp1UkJCQtox6b0t8eeff1b79u0VFhamwMBAVa9eXUuWLHE55tLb91atWqVRo0apaNGiCg4OVsuWLbV37173LrKke++9V5K0atWqtG3x8fF644039PDDD6f7nLFjx6pu3brKnz+/goODVbNmTS1cuFBOpzPtmFKlSmnnzp364osv0q7fpZlvl9qXLVumQYMGqVixYgoICNCvv/562dsSo6OjVbx4cTVo0EDnz59PO/+uXbuUJ08ePfjgg25/rQAAAEBGGNwCAGSrlJQUffrpp6pVq5aKFy/u1nP69eunYcOG6bbbbtM777yjcePGaePGjWrQoIGio6Ndjj169Kjuv/9+PfDAA3rnnXfUpk0bjRgxQsuXL5ck3XHHHfrmm28kSffcc4+++eabtD+768CBA7rjjjvk7++vRYsWaePGjZo4caLy5Mmj5OTkDJ+3d+9eNWjQQDt37tTLL7+sN998U5UqVVKPHj00efLky44fOXKkDh48qAULFmjevHnat2+f2rVrl+lbxf4pODhY99xzjxYtWpS2bdWqVfLx8VHXrl0z/Nr69u2rtWvX6s0331THjh31xBNPaNy4cWnHrF+/XjfeeKNq1KiRdv3+/RbSESNG6NChQ5o7d67effddFSxY8LLXioiI0OrVq7Vt2zYNGzZMkpSQkKDOnTurRIkSmjt3btqxlwbGTKxhBgAAALvxtkQAQLaKjo5WQkKCSpcu7dbxe/bs0bx589S/f3+98soradtr1KihunXratq0aRo/fnza9piYGH3wwQe65ZZbJEktW7bU559/rpUrV+qhhx5SgQIFVKBAAUlSoUKFsvQ2uR07dujcuXOaMmWKqlWrlrb9vvvuy/R5zz33nJKTk/XZZ5+lDey1bdtWJ0+e1NixY9W3b1+FhISkHV+pUqW0QTlJ8vX1VZcuXbRt2za3ux9++GE1a9ZMO3fuVOXKlbVo0SJ17tw5w/W2/rm4d2pqqpo2bSqn06kZM2ZozJgxcjgcqlGjhoKCgjJ9m2GZMmX0+uuvX7GvYcOGGj9+vIYNG6Zbb71Vb731lvbv36//+7//U548edKOczgc8vX1lY8Pv3cDAACAZ/gJEgBg1GeffSZJly1cfsstt6hixYr65JNPXLYXLlw4bWDrkptvvlkHDx7Mtqbq1avL399fffr00ZIlS/T777+79bxPP/1ULVq0uGzGWo8ePZSQkHDZDLJ/vjVTuvh1SPLoa2nSpInKlCmjRYsW6aefftK2bdsyfEvipcaWLVsqJCREvr6+8vPz0zPPPKOYmBgdP37c7dft1KmT28cOGTJEd9xxh+69914tWbJEr7zyiqpWrXrZ13HhwgU988wzbp8XAAAAkBjcAgBks4iICOXOnVv79+936/iYmBhJUpEiRS7bV7Ro0bT9l4SHh192XEBAgBITE7NQm74yZcro448/VsGCBfXYY4+pTJkyKlOmjGbMmJHp82JiYjL8Oi7t/6d/fy2X1ifz5GtxOBzq2bOnli9frrlz56p8+fJq3Lhxusf+73//U6tWrSRd/DTLr7/+Wtu2bdOoUaM8ft30vs7MGnv06KFz586pcOHCrLUFAACAbMXgFgAgW/n6+qpFixbasWPHZQvCp+fSAE9UVNRl+44cOaKIiIhsawsMDJQkJSUluWz/97pektS4cWO9++67io+P19atW1W/fn0NGDBAq1evzvD84eHhGX4dkrL1a/mnHj16KDo6WnPnzlXPnj0zPG716tXy8/PTe++9py5duqhBgwaqXbt2ll4zvYX5MxIVFaXHHntM1atXV0xMjAYPHpyl1wQAAADSw+AWACDbjRgxQk6nU7179053Afbz58/r3XfflSQ1b95cklzWnpKkbdu2affu3WrRokW2dV36xL8ff/zRZfullvT4+vqqbt26mjVrliTp22+/zfDYFi1a6NNPP00bzLpk6dKlyp07d5bW/3JHsWLFNGTIELVr107du3fP8DiHw6FcuXLJ19c3bVtiYqKWLVt22bHZNRsuJSVF9957rxwOhzZs2KDIyEi98sorevPNN6/63AAAAIDEgvIAgBxQv359zZkzR/3791etWrXUr18/Va5cWefPn9d3332nefPmqUqVKmrXrp0qVKigPn366JVXXpGPj4/atGmjAwcOaMyYMSpevLgGDhyYbV1t27ZV/vz59cgjj+j5559Xrly5tHjxYh0+fNjluLlz5+rTTz/VHXfcoRIlSujcuXNpn0jYsmXLDM//7LPP6r333lOzZs30zDPPKH/+/FqxYoXef/99TZ482WUx+ew2ceLEKx5zxx13aOrUqbrvvvvUp08fxcTE6MUXX0x7O+Q/Va1aVatXr9aaNWt04403KjAw8LJ1stzx7LPP6quvvtJHH32kwoULa9CgQfriiy/0yCOPqEaNGmkfPPDFF1+oRYsWeuaZZ1h3CwAAAB5hcAsAkCN69+6tW265RdOmTdOkSZN09OhR+fn5qXz58rrvvvv0+OOPpx07Z84clSlTRgsXLtSsWbMUEhKi22+/XZGRkemusZVVwcHB2rhxowYMGKAHHnhAoaGh6tWrl9q0aaNevXqlHVe9enV99NFHevbZZ3X06FHlzZtXVapU0TvvvJO2ZlV6KlSooC1btmjkyJF67LHHlJiYqIoVK+q11167bMF8E5o3b65FixZp0qRJateunYoVK6bevXurYMGCeuSRR1yOHTt2rKKiotS7d2+dPn1aJUuW1IEDBzx6vU2bNikyMlJjxoxxmYG3ePFi1ahRQ127dtXmzZvl7+8vp9OplJQUpaamZseXCgAAgP8QBrcAADmmWrVqWrx48RWP8/Hx0dChQzV06NBMj/v888/T3Z7eazidznSPrVOnjr7++uvLtv9zcKdevXpuvW0uvde4NAiWmaZNm6b73FKlSmXY/U89evRwa7Ds559/vmxbz549012X69+fsFiyZEl9+OGHlx2XUXt6+2677TalpKRcdlz+/Pkv+0TIzM4LAAAAZIY1twAAAAAAAGAtZm4BAABr+Pv76+WXX9bMmTPT3X/pbazZfVx6fH199cMPPyg0NDTd/SkpKfLx8fH64zKSN2/eDD/hMzU1VdOmTfPouPQEBQWpU6dO6X76ptPp1N133+3RcQAA4L/J4eQ9AAAAAAAAALAUb0sEAAAAAACAtRjcAgAAAAAAgLUY3AIAAAAAAIC1rssF5X84fNp0wlWpUCSf6YT/pF+PnTGdkGVlC+U1nQDATb8dO2s64aoUDw8ynZBl/rn4nR48l5pq7/K0B6ITTCdkWcmI3KYTsszX5/IPPsC1cTLhvOmELAvN7Wc6IcsCr8tRhSsLqvG46YQckfhd+h+y4+34KQ8AAAAAAADWYnALAAAAAAAA1mJwCwAAAAAAANb6j747FgAAAAAAIIsczBXyJvxtAAAAAAAAwFoMbgEAAAAAAMBaDG4BAAAAAADAWqy5BQAAAAAA4AmHw3QB/oGZWwAAAAAAALAWg1sAAAAAAACwFoNbAAAAAAAAsBZrbgEAAAAAAHjCwVwhb8LfBgAAAAAAAKzF4BYAAAAAAACsxeAWAAAAAAAArMXgFgAAAAAAAKzFgvIAAAAAAACecDhMF+AfmLkFAAAAAAAAazG4BQAAAAAAAGsxuOWG9StfU5eWtbV49ktp27q0rJ3u4501Sw2WZm7NqhVq06q56tSoqm6dO+rbHdtNJ7nNtvY3Vy5Sp+a1tGjmi2nb1ix+VU9076j72jbUQ3c11XOD++mX3T8ZrLwy2677v9ncT7sZNra/sXKROjavqYUzp0iSLlw4r6XzZmjAI110b9sGeqRzK82IHKPY6BOGS9N34cIFzZk5Xe3btFSjW6qrfdvbNH/uLKWmpppOc5uN980ltF97a9esUpeOd6lRvVpqVK+WHrq/qzZ/9aXprCt6Y8Ui3d3s7+81kvTNl59o7JD+eqh9c93drKb2/7rXYKFnFi14VTWr3qQpkyaYTnGbrff8JTb0//Dtdg0f+Jg6tmmmJnWq6KvPP3HZHxsTrcjnRqljm2Zq1ai2hjzRV38cOmio1j02XHcgOzC4dQW/7tmpjz9Yr5I3lnPZPm/tRpdHv8HPyOFwqG7j5oZKM7dxwweaPDFSvfv005p1b6lmzVrq37e3oo4cMZ12Rba1/7pnpza9d/k9U7R4CfV6cpimLlijF2YsVMHCRTRu6GOKPxlnqDRztl33f7O5n3YzbGzft2enNr33psv3m6Rz5/T7vj3q/GAvvTh3pYaOfVFH/jioyNEDzIVmYulrC/TG62s0ZMRorV3/vp4cOFjLlyzSmlXLTae5xcb75hLazShUqJCeGDBIK1av04rV63RL3Xoa+ORj+u3XfabTMrRvz0599N6bKvWvn22SziXqpirV9WCfJwyVZc3On3/Sm+vWqlz5CqZT3GbzPS/Z05+YmKiy5StowJCRl+1zOp0aNeQpHTnyh8a/+LIWLH9dhYoU1dOP9VJiYoKB2iuz5bpby+FzfT4sZW/5NXAuMUGvRI5R34GjlCdvPpd9ofkjXB7btnyhytVrq1DRGwzVZm7Zktd0d6dO6nhPZ91YpoyGjhilwkUKa+2aVabTrsim9sTEBE2fMFqPDhqtvPmCXfY1btFG1WrVVeGiN6hE6TLq0e9pJZw9q4O/e+cPszZd9/TY3E+7Gba1X/x+M0r9Bo1x+X6TJ28+PTdljho2baViJUqpQqWb1euJYfrtl906cSzKYHH6fvrhezVp2lyNbm2qosWKqcVtrVW3fkPt3vmz6TS32Hbf/BPtZjRp2lyNb22ikqVKq2Sp0nr8yYHKnTu3fvzxB9Np6UpMTNC08aPUf/AY5fnXzzZNW92prt37qFqtuobqPJeQcFajhg/WmGfHKTg4+MpP8BI23/OSPf31GjZWr35P6tbmt122749DB7Xrpx/09LAxqli5qkqUKq2Bw0YrMTFBn3z4gYHaK7PlugPZwejg1h9//KFRo0apWbNmqlixoipVqqRmzZpp1KhROnz4sMk0SdKClyepRt2GuvkK/2CfjIvRd/+3Wc1vb3+NyjxzPjlZu3ftVP0GjVy212/QUD98/52hKvfY1r5gxkTVqtvoij/knT9/Xpvee1O58+RVqTLlMj3WBNuu+7/Z3E+7GTa2z3fz+40kJZw9I4fDcdkvarxBtRq1tO1/W3XwwH5J0i979+iH775Vw8ZNDJddmY33zSW0e4eUlBRt3PC+EhMTdHO16qZz0jVv+kTVrufe9xobTBz/vBo1bqq69RuYTnGb7fe87f2XJJ9PliT5B/inbfP19VWuXH76yQu/juvlugPuymXqhTdv3qw2bdqoePHiatWqlVq1aiWn06njx4/rrbfe0iuvvKINGzaoYcOGmZ4nKSlJSUlJLtuSk5LlHxBwVX1ff/ah9u/bo8jZV15D64uP3lNg7jy6pXGzq3rNnBJ3Mk4pKSkKDw932R4eHqFoL12D5RKb2jd/+qF+37dHk+Ysy/CY7d98qWnjRiop6ZzC8kfo2SmzFRwSdg0r3WPTdU+Pzf20m2Fb+6XvN5Mz+X5zSXJykpbPf1mNW9yu3HnyXoM6z3R/uJfOnDmtzh3ukI+vr1JTUtTviQFq3eYO02lXZNt980+0m7Xvl73q/sC9Sk5OUlDu3Hpp+kyVKVPWdNZlvvrre82UuVf+XmODDze8rz27dmnZ6nWmUzxi+z1ve/8lJUuVVuEiRTVv1gwNHvGMAoNya+2KJYqNiVZMjPd9HdfLdQfcZWxwa+DAgerVq5emTZuW4f4BAwZo27ZtmZ4nMjJSY8eOddnWd8Bw9Xv68vdJuyv6+FEtnvWSRk2aKX//Kw+SfbbxHTVufrtbx5rkcDhc/ux0Oi/b5q28vT36+FEtmvWinpk8K9P7oEr1Onpx/iqdjj+pTe+v10vPD9fEWUsUEpb/Gta6z9uv+5XY3E+7GTa0Rx8/qoWzpuiZybOv+O/OhQvnNXXcCKWmOtXnqRHXqNAzmzZ+oA3vv6sXIqfoxrLl9Mue3Zo6JVIFChTUnXd1MJ3nFhvum4zQbkap0qW1et16nT59Sp9s+kjPjB6uBa8t86oBrujjR7Vw5hQ968b3GhscPRqlKRMnaPa8hQq4yl+Cm2LzPS/Z358rl5+enzRNk8c9oztbNJSvr69q1amnug0am07LlO3X3atxHb2KscGtn3/+WcuXZ7xYbN++fTV37twrnmfEiBF6+umnXbbtPZ58VW2/79uj+JOxGt7vwbRtqakp2v3Td9r41lqt3LBFPr6+kqTdP32nI4cPasDoyKt6zZwUFhomX19fRUdHu2yPjY1ReHiEoSr32NL+2y+7FR8XqyF9H0jblpqaol0/fqsNb63V6g+/ka+vrwKDglSkWHEVKVZc5StV1WMPdtAnG95Sx/seNlh/OVuue0Zs7qfdDJva//5+c3/atn9+v1nz4Vb5+vrqwoXzenHscB2L+lPPv/SqV87akqQZ015U94d7qdVfM7XKliuvqKgjWrxwntcPbtl03/wb7Wb5+fmrRImSkqTKlatq588/a9XypRr97POGy/526XvN4HS+13ywfq3WfnTxe40tdu/cqdjYGN3ftVPatpSUFH27Y7vWrlqhrTt+9Nqvx/Z73vb+f6pQsbIWrnxDZ86c1oXz5xUall+P9rhXFSpWNp12mevpugPuMDa4VaRIEW3ZskUVKqT/KSXffPONihQpcsXzBAQEXPbbF//401fVVrVGHb04f7XLtjlTnlfREiXVvmv3tIEtSfp0w9u6sXxFlSpT/qpeMyf5+furYqXK2rrla7Vo+ffiiFu3bFHT5i0Mll2ZLe0317xF0xaucdk2c/JYFSteSnff2z3jH5acTp1PPn8NCj1jy3XPiM39tJthU/vF7zdrXbbNnPycbiheSh3u7eEysBX15yE9P3We8oWEmol1Q9K5RPn4uC4B6uPrK2dqqqEi99l03/wb7d7GqeTkq/vlbHa7ueYtmr7oX99rJj2nYiVK6e6/vtfY5JZ69bT2zXdctj03ZqRKlb5RPR7u5dVfj+33vO396cn71xqWfxw6qL27d+qRRx83XHS56/G6A5kxNrg1ePBgPfroo9qxY4duu+02FSpUSA6HQ0ePHtWmTZu0YMECTZ8+3UhbUO48KlHadVp4QGCg8gWHumxPOHtGW7/8WA/2HXCNCz33YPeeGjV8qCpVqaJq1WrojdfXKCoqSp27djOddkU2tKd3zwQGBilfcIhKlC6rc4mJemPFQtVp0ESh+SN05tRJbXzndcWcOK76TVoaqs6cDdc9Mzb3026GLe1BufOoZDrfb/IGh6hk6bJKSbmgKc8N1e/79mjkhBlKTU1RXOzF39rmzRciPz8/E9kZatSkmV6b/6oKFy6iG8uU0949u7Ry2WLd1b6j6TS32HLfpId2M16ZMVUNG92qwoUL6+zZs/pw4wfavu1/mjVnvuk0F+l9rwn462ebS9tPn4pX9PGjiv1r/Z4/Dx2QJIXmD1dYfu+aGZInT16VLef6y+igoCCFhIZett0b2XzPS/b0JyQk6M/Dh9L+HHXkT+3bu0fBISEqVLiIPvv4Q4WGhalQoSL6/bd9euWliWrUpLnq1Mt8nWhTbLnuQHYwNrjVv39/hYeHa9q0aXr11VeVkpIi6eInTtSqVUtLly5Vly5dTOW5ZctnH8npdKpRs9tNp1zR7W3aKv5knObNma0TJ46rbLnymjV3nooWLWY67Ypsbr/Ex9dHfx46oM8/fE+nTp1UvuAQla1QWS/MWKASpcuYzkuX7dfd5n7azbC5/Z9iThzXti1fSJIG9Xb94fX5qfNUpXptE1kZGjJ8tObOmqFJE55XXGysIgoUVMd7uqhX3/6m09xi831DuxkxMTEaPXKook+cUN58+VSuXAXNmjNf9Rp45/84zsy2LV/olUnPpf35pXEX1/br2r2PuvV41FDV9cnme16yp3/v7p814NG/lwuZNW2yJOn2O9prxHPjFRN9QrOmTVZcbIzCIwqoddu79FAv773Xbbnu1nL4XPkYXDMOp9PpNB1x/vz5tPcCR0REXPVvlX84fHVvSzStQhHv+6j2/4Jfj50xnZBlZQt551o6AC7327GzphOuSvHwINMJWeafix9C4bnUVOM/KmfZgegE0wlZVjIit+mELPP1YZFpU04meN9yH+4Kze1dM6s9EWhsyoxZQfWGmU7IEYlbJ5lOyBKvuA39/PzcWl8LAAAAAAAA+Cd+hQkAAAAAAABrecXMLQAAAAAAAGs4eAuyN2HmFgAAAAAAAKzF4BYAAAAAAACsxeAWAAAAAAAArMXgFgAAAAAAAKzFgvIAAAAAAACecDBXyJvwtwEAAAAAAABrMbgFAAAAAAAAazG4BQAAAAAAAGux5hYAAAAAAIAnHA7TBfgHZm4BAAAAAADAWgxuAQAAAAAAwFoMbgEAAAAAAMBarLkFAAAAAADgCQdzhbwJfxsAAAAAAACwFoNbAAAAAAAAsBaDWwAAAAAAALAWa24BAAAAAAB4wuEwXYB/uC4HtyoUyWc64aqE3TrSdEKWxX05wXRClpUtlNd0AoD/gDKF8phOAOABHx97/8fLjQX5foP/ltDcfqYTABjC2xIBAAAAAABgLQa3AAAAAAAAYK3r8m2JAAAAAAAAOcbBXCFvwt8GAAAAAAAArMXgFgAAAAAAAKzF4BYAAAAAAACsxeAWAAAAAAAArMWC8gAAAAAAAJ5gQXmvwt8GAAAAAAAArMXgFgAAAAAAAKzF4BYAAAAAAACsxZpbAAAAAAAAnvBxmC7APzBzCwAAAAAAANZicAsAAAAAAADWYnALAAAAAAAA1mLNLQAAAAAAAE84mCvkTfjbAAAAAAAAgLUY3AIAAAAAAIC1GNwCAAAAAACAtRjc8sCaVSvUplVz1alRVd06d9S3O7abTtLgB5to88L+Or7pWR18f6TWTnxA5UpEpO3P5eujF/q31rZlTyr6k+f0+9vDtWDMPSoSkS/tmBKFQ5W4ZUK6j47Nqpj4si7jjdfeXbSbY3M/7WbQbo7N/bSbYXO7ZHc/7WbY3C7Z3U870uVwXJ8PSzG45aaNGz7Q5ImR6t2nn9ase0s1a9ZS/769FXXkiNGuxjVKa+4bW9Wkzxzd+dQi+fr66L3pPZU70E+SlDvQT9XLF9XE1z5T/Z4z1W3kCpUrHqHXJz2Ydo4/jser1J0TXB7Pz/9YZxKS9OHWX0x9aWm89dq7g3ZzbO6n3QzazbG5n3YzbG6X7O6n3Qyb2yW7+2kH7OBwOp1O0xHZ7dyF7D/n/d06q2KlShr9zNi0bR3atVGz5i311MBB2fpaYbeOzPJzI0Lz6PAHo9Sy/zx9/f2BdI+pVbGYNi98TOXvnqTDx+LTPeabxY/r+71H1C/yTY9eP+7LCZ4mX9G1vPbZjXZzbO6n3QzazbG5n3YzbG6X7O6n3Qyb2yW7+2m/ssBc2XYqqwS1yP7/7esNEj/J+niESczccsP55GTt3rVT9Rs0ctlev0FD/fD9d4aq0hecJ0CSFHcqMZNjApWamqqTp8+lu79GhaKqXr6olrxrfsqqTdf+32g3x+Z+2s2g3Ryb+2k3w+Z2ye5+2s2wuV2yu592wB4Mbrkh7mScUlJSFB4e7rI9PDxC0dEnDFWlb9KTd+jr7w9o1+/H0t0f4J9L4/q11ppNP+h0QlK6x3RvV1u79x/X1p8P5WSqW2y69v9Guzk299NuBu3m2NxPuxk2t0t299Nuhs3tkt39tAP28OrBrcOHD+vhhx/O9JikpCSdOnXK5ZGUlP6gzdVy/GtxNafTedk2k6YNuktVyxZW92dXp7s/l6+Plj3fTT4+Dj015Z10jwn0z6Wut1XTkvfMz9r6J2+/9pmh3Ryb+2k3g3ZzbO6n3Qyb2yW7+2k3w+Z2ye5+2pEuh8/1+bCUV5fHxsZqyZIlmR4TGRmpkJAQl8eUSZHZ2hEWGiZfX19FR0f/qy9G4eERGTzr2po6sJ3ubHSTWj++QH+eOHXZ/ly+Plrxwr0qWSRMdz61KMNZW3c3r6LcgX5ascE7pqracO0zQrs5NvfTbgbt5tjcT7sZNrdLdvfTbobN7ZLd/bQD9jA6uPXOO+9k+vjss8+ueI4RI0YoPj7e5TFk2Ihs7fTz91fFSpW1dcvXLtu3btmiatVrZOtrZcW0p9upfdNKuv2JhToYFXfZ/ksDW2WKR+iOpxYpNpP1uHrcWVvvb96j6JNnczLZbd5+7TNDuzk299NuBu3m2NxPuxk2t0t299Nuhs3tkt39tAP2MPq5Bh06dJDD4VBmH9h4pSmTAQEBCggIcNmWE5+W+GD3nho1fKgqVamiatVq6I3X1ygqKkqdu3bL/hfzwPTBd6nrbdXUedhynUlIUqH8eSVJ8WfO6VzyBfn6+mjlhPtUo3xRdRyyVL4+jrRjYk8l6vyFlLRz3VgsvxpVL6UOgzKfLXeteeu1dwft5tjcT7sZtJtjcz/tZtjcLtndT7sZNrdLdvfTDtjB6OBWkSJFNGvWLHXo0CHd/d9//71q1ap1baMycHubtoo/Gad5c2brxInjKluuvGbNnaeiRYsZ7erbsZ4kadPs3i7be7+wTss/+FbFCgSrXeNKkqT/LX3S5ZhWj83XV9/tT/tz9ztr68iJU/r4f7/mcLVnvPXau4N2c2zup90M2s2xuZ92M2xul+zup90Mm9slu/tpR4ZYu8yrOJyZTZvKYXfddZeqV6+u559/Pt39P/zwg2rUqKHU1FSPzpsTM7eupbBbR5pOyLK4LyeYTgAAAAAAXCOBRqfMmBN02yTTCTkicdMw0wlZYvQ2HDJkiM6ezXhtp7Jly7q17hYAAAAAAAD+m4wObjVu3DjT/Xny5FGTJk2uUQ0AAAAAAABs8x+dQAgAAAAAAJBFDh/TBfgH/jYAAAAAAABgLQa3AAAAAAAAYC0GtwAAAAAAAGAt1twCAAAAAADwhMNhugD/wMwtAAAAAAAAWIvBLQAAAAAAAFiLwS0AAAAAAABYizW3AAAAAAAAPOFgrpA34W8DAAAAAAAA1mJwCwAAAAAAANZicAsAAAAAAADWYnALAAAAAAAA1mJBeQAAAAAAAE84HKYL8A/M3AIAAAAAAIC1GNwCAAAAAACAtRjcAgAAAAAAgLVYcwsAAAAAAMATDuYKeRMGt7xQ3JcTTCdkWVidx00nZFnctpmmEwAAAAAAgIcYagQAAAAAAIC1GNwCAAAAAACAtXhbIgAAAAAAgCccDtMF+AdmbgEAAAAAAMBaDG4BAAAAAADAWgxuAQAAAAAAwFqsuQUAAAAAAOAJB3OFvAl/GwAAAAAAALAWg1sAAAAAAACwFoNbAAAAAAAAsBZrbgEAAAAAAHiCNbe8Cn8bAAAAAAAAsBaDWwAAAAAAALAWg1sAAAAAAACwFoNbAAAAAAAAsBYLygMAAAAAAHjC4TBdgH9g5hYAAAAAAACsxeAWAAAAAAAArMXgFgAAAAAAAKzFmlsAAAAAAACecDBXyJvwt+GBNatWqE2r5qpTo6q6de6ob3dsN53kEW/rH/xwK21ePkTHN7+og59Eau3U3ipXsqDLMaP6ttX3b45W9JaXdOSLyXp/7uOqU6XkZeeqe3NpbXj1CUVveUlRX07Wh/OfUmCA37X6UjLlbdfdEza3S3b3024G7ebY3E+7GTa3S3b3026Gze2S3f20A96PwS03bdzwgSZPjFTvPv20Zt1bqlmzlvr37a2oI0dMp7nFG/sb1yyruWu+VJOHXtSd/WbK19dX7815XLkD/dOO+fXgcQ2c9Lpqd56gFj2n6uCRWL07+3FFhOVNO6buzaX19sz++mTrHjV+YIoaPTBFc9d8odRUp4kvy4U3Xnd32dwu2d1Puxm0m2NzP+1m2Nwu2d1Puxk2t0t299MO2MHhdDrNjwBks3MXsv+c93frrIqVKmn0M2PTtnVo10bNmrfUUwMHZf8LZrNr1R9W5/EsPzciLK8OfzpRLR+Zpq+//S3dY/LlCdTxzS+qTd+X9fn/fpEkfbFkkD75vz16fvb7WX5tSYrbNvOqnp8em+8bm9slu/tpN4N2c2zup90Mm9slu/tpN8PmdsnuftqvLPA/uthRUPtXTSfkiMS3+5pOyBJmbrnhfHKydu/aqfoNGrlsr9+goX74/jtDVe6zpT84b6AkKS4+Id39frl89UjHhjp5OkE//fKnJKlAWF7dcnNpnYg9o88WP60DH0/QRwueUoPqN16z7ozYct3TY3O7ZHc/7WbQbo7N/bSbYXO7ZHc/7WbY3C7Z3U87MuVwXJ8PSxkf3EpMTNTmzZu1a9euy/adO3dOS5cuzfT5SUlJOnXqlMsjKSkpWxvjTsYpJSVF4eHhLtvDwyMUHX0iW18rJ9jSP2lQJ3397a/a9VuUy/Y2javoxNcv6eT/TdMTDzTTnY/OVMzJs5Kk0jdESLq4NteiN7eo/WOz9f3uw/rg1SdUpkSBa/41/JMt1z09NrdLdvfTbgbt5tjcT7sZNrdLdvfTbobN7ZLd/bQD9jA6uPXLL7+oYsWKuvXWW1W1alU1bdpUUVF/D2zEx8erZ8+emZ4jMjJSISEhLo8pkyJzpNfxr1FMp9N52TZv5s3904Z3UdVyRdV9xOLL9n2x7RfV7RapZj2m6qMtu7R88sMq8NeaWz4+F/sXvrFZy97Zqh/2/qGhL72pXw4cV/f29a/ll5Ahb77uV2Jzu2R3P+1m0G6Ozf20m2Fzu2R3P+1m2Nwu2d1PO+D9jA5uDRs2TFWrVtXx48e1d+9eBQcHq2HDhjp06JDb5xgxYoTi4+NdHkOGjcjWzrDQMPn6+io6Otple2xsjMLDI7L1tXKCt/dPHdZZdzapqta9X9afx09etj/hXLJ+Pxyt//10QP3GrtSFlFR1v7uBJCnqxClJ0u7fj7o8Z+/+oypeOCzH2zPj7dc9Mza3S3b3024G7ebY3E+7GTa3S3b3026Gze2S3f20A/YwOri1ZcsWTZgwQRERESpbtqzeeecdtWnTRo0bN9bvv//u1jkCAgIUHBzs8ggICMjWTj9/f1WsVFlbt3ztsn3rli2qVr1Gtr5WTvDm/mnDOqt982q6ve/LOngkxq3nOORQgN/FVQsPHonRkeMnVb5UQZdjypYsqENRsdne6wlvvu5XYnO7ZHc/7WbQbo7N/bSbYXO7ZHc/7WbY3C7Z3U87MuXwuT4fljL6uQaJiYnKlcs1YdasWfLx8VGTJk20cuVKQ2WXe7B7T40aPlSVqlRRtWo19MbraxQVFaXOXbuZTnOLN/ZPH9FFXdvUVueB83Tm7DkVCs8nSYo/c07nks4rd6C/hvVqrfe/+ElHo+OVPySP+nS5VcUKherNTd+mnWfako81+tE79NMvf+qHvX/ogXZ1VaFUId03ZKGpLy2NN153d9ncLtndT7sZtJtjcz/tZtjcLtndT7sZNrdLdvfTDtjB6ODWTTfdpO3bt6tixYou21955RU5nU7dddddhsoud3ubtoo/Gad5c2brxInjKluuvGbNnaeiRYuZTnOLN/b37XKrJGnTggEu23s/s0zL3/0/paSmqkKpQnqgXV2Fh+ZRbHyCtu88qJYPT3N5G+LMlZ8rMMBPkwd1UlhIbv30y5+6s99M7f/DdQquCd543d1lc7tkdz/tZtBujs39tJthc7tkdz/tZtjcLtndTztgB4fT6XSaevHIyEh99dVX+uCDD9Ld379/f82dO1epqakenffcheyoQ1aE1XncdEKWxW2baToBAAAAAKwSaHTKjDlBdy8wnZAjEtf3Mp2QJUYHt3IKg1vmMLgFAAAAAP8d/9nBrY7ml8HJCYlvPmI6IUvsXS0MAAAAAAAA/3kMbgEAAAAAAMBaDG4BAAAAAADAWgxuAQAAAAAAwFr/0aXfAAAAAAAAssbhcJhOwD8wcwsAAAAAAADWYnALAAAAAAAA1mJwCwAAAAAAANZizS0AAAAAAAAPsOaWd2HmFgAAAAAAAKzF4BYAAAAAAACsxeAWAAAAAAAArMWaWwAAAAAAAJ5gyS2vwswtAAAAAAAAWIvBLQAAAAAAAFiLwS0AAAAAAABYizW3AAAAAAAAPOBwsOiWN2HmFgAAAAAAAKzF4BYAAAAAAACsdV2+LTHmTLLphKsSntffdEKWxW2baTohy6Z/+ZvphCwbcGsZ0wkA3HT8VJLphKsSYfG/UTbz8eGtD6akpjpNJ2TZkZPnTCdkWdGwQNMJWebDW5WMSbH4v1dfvs8DV4WZWwAAAAAAALDWdTlzCwAAAAAAIKewoLx3YeYWAAAAAAAArMXgFgAAAAAAAKzF4BYAAAAAAACsxeAWAAAAAACABxwOx3X58ERkZKTq1KmjfPnyqWDBgurQoYP27t3rcozT6dRzzz2nokWLKigoSE2bNtXOnTtdjklKStITTzyhiIgI5cmTR3fddZf++OMPj1oY3AIAAAAAAIBHvvjiCz322GPaunWrNm3apAsXLqhVq1Y6e/Zs2jGTJ0/W1KlTNXPmTG3btk2FCxfWbbfdptOnT6cdM2DAAK1fv16rV6/W5s2bdebMGd15551KSUlxu8XhdDqd2frVeYE/TyabTrgq4Xn9TSf8J03/8jfTCVk24NYyphMAuOn4qSTTCVclgn+jjPDx4ROZTElNtfdH5SMnz5lOyLKiYYGmE7LMh09QMybF4v9efS3+Ph+Yy3SBGcHdlppOyBGnVj+U5eeeOHFCBQsW1BdffKFbb71VTqdTRYsW1YABAzRs2DBJF2dpFSpUSJMmTVLfvn0VHx+vAgUKaNmyZeratask6ciRIypevLg++OADtW7d2q3XZuYWAAAAAAAAlJSUpFOnTrk8kpLc++VsfHy8JCl//vySpP379+vo0aNq1apV2jEBAQFq0qSJtmzZIknasWOHzp8/73JM0aJFVaVKlbRj3MHgFgAAAAAAgAdMr42VU4/IyEiFhIS4PCIjI694PZxOp55++mk1atRIVapUkSQdPXpUklSoUCGXYwsVKpS27+jRo/L391dYWFiGx7jjPzqBEAAAAAAAAP80YsQIPf300y7bAgICrvi8xx9/XD/++KM2b9582b5/L1TvdDqvuHi9O8f8EzO3AAAAAAAAoICAAAUHB7s8rjS49cQTT+idd97RZ599phtuuCFte+HChSXpshlYx48fT5vNVbhwYSUnJysuLi7DY9zB4BYAAAAAAAA84nQ69fjjj+vNN9/Up59+qtKlS7vsL126tAoXLqxNmzalbUtOTtYXX3yhBg0aSJJq1aolPz8/l2OioqL0888/px3jDt6WCAAAAAAA4Al7P+Ay2zz22GNauXKl3n77beXLly9thlZISIiCgoLkcDg0YMAATZgwQeXKlVO5cuU0YcIE5c6dW/fdd1/asY888ogGDRqk8PBw5c+fX4MHD1bVqlXVsmVLt1sY3AIAAAAAAIBH5syZI0lq2rSpy/bXXntNPXr0kCQNHTpUiYmJ6t+/v+Li4lS3bl199NFHypcvX9rx06ZNU65cudSlSxclJiaqRYsWWrx4sXx9fd1ucTidTudVf0Ve5s+TyaYTrkp4Xn/TCf9J07/8zXRClg24tYzpBABuOn7KvY9S9lYR/BtlhI8Pvx42JTXV3h+Vj5w8Zzohy4qGBZpOyDIfDxZARvZKsfi/V1+Lv88H/kenzITct8x0Qo6IX/mg6YQsYc0tAAAAAAAAWOs/OsYKAAAAAACQNQ5maXoVZm4BAAAAAADAWgxupWPl4gXq16Ob7mhWVx1vb6IxQ57UoYP7XY5ZPH+2undpp7ZNbtFdLRto8OO9tPvnHw0Vu2fNqhVq06q56tSoqm6dO+rbHdtNJ7nNG9uP7vtJH89+TquHP6DX+rXVwe+3uOx3Op367r3lWj38AS19soM2TB2muCMH0/YnnT2trWvm6I1ne2vpk3dr7cju2rpmrpITz17rLyVD3njdPWFzP+1m2NC+askCPfbwvbqrRT11bttEzw57Sof/9W+UJB088LvGDHlC7Vs20F0t6umJXvfr+NEoA8WZW7tmlbp0vEuN6tVSo3q19ND9XbX5qy9NZ7nF5vZLbLjnM2Jru033zfvr1+qx7p11T+uGuqd1Qw169CFt37o53WNfmTJOdzSurrfWLr/Gle5ZOP9V3d/1HjW8paaa39pAA598TAf2/246yyO23vOX2N4vSYsWvKqaVW/SlEkTTKe47Xq47oA7GNxKxw/fbVf7e7pp5sIVmvLyPKWkpGjok32VmJiQdkzxEiX15OCRWrDyDc2Yt1SFixTT0Cf76mRcrMHyjG3c8IEmT4xU7z79tGbdW6pZs5b69+2tqCNHTKddkbe2X0g6p7BipVWva7909//00Trt/GS96nXtp3bDpisoOEwfvjxK589dvI8STsYo4WSM6nTqpQ5jZqvRQwP1567t2rxs+jX8KjLmrdfdXTb3026GLe0/frddd3XqppfnL9fEGfOUciFFwwc86vJv1JE/Dmtg3+4qUbK0Xpq1UK8uXacHevaVn7/3LQZfqFAhPTFgkFasXqcVq9fplrr1NPDJx/Tbr/tMp12Rze2SPfd8emxut+m+iShYSD0efVIz5q/UjPkrdXPNOho3YoAO7v/V5bhvvvxUe3f9pPCIAoZKr+zb7dvU9d77tHTlGs2Zt0gpFy6oX59eSkxIuPKTvYDN97xkf78k7fz5J725bq3Kla9gOsVt18N1B9zFpyW64WRcrDre3kTT5r6majVqp3vM2TNn1K5Ffb04c75q1ql3Va+XE5+WeH+3zqpYqZJGPzM2bVuHdm3UrHlLPTVwULa/Xna6Vu1X82mJr/Vrq+Z9R6tk9QaSLs7aWjP8AVVq3kE3t+4sSUo5f16rh92nWnf31E2N26Z7nv07vtKXi6fowenr5ePBx57mxKcl2nzPSHb3027GtWrP7k9LPBkXq85tm+ql2Yt081//Ro0fM1S+uXJp+LPZ/5vla/FpiU0a1tWAQUN0d8d7cvy1sltOtefEpyXy36t7rsWnJebUfZMTn5bYte2terj/QLW+825JUvSJY3q674Ma99JsPTf0CbXvfL86dHngql8npz8tMTY2Vi1ubaAFi5epVu062XrunPi0RJv/e5WuXX9OfVpiQsJZ3delo0aMelYL5s1R+Zsqasiwkdn6GjnxaYnX6rr/Vz8tMfR+75yperVOrrj67+EmMHPLDWfPnJEkBQeHpLv//Pnzeu+tdcqTN5/KlPO+kfzzycnavWun6jdo5LK9foOG+uH77wxVucfW9jPRR5V4Kk7FKtVM2+br56dC5arq+G+7M3ze+cSz8gvM7dHAVk6w9bpfYnM/7WbY3H7p36h8f/0blZqaqv/b8qVuKF5Swwc8qs5tm+iJR+7T1198ajLTLSkpKdq44X0lJibo5mrVTed4xLZ2m+95m9v/zab7JiUlRV98vFHnziWqYuWbJV38fvPSC6PV6d7uKlm6rOFCz5w5c1qSFBKS/s/33sT2e972fkmaOP55NWrcVHXrNzCd4rbr4bp7O4fDcV0+bGV8jHX37t3aunWr6tevr5tuukl79uzRjBkzlJSUpAceeEDNmzfP9PlJSUlKSkr61zaHAgICsqXP6XRq9owpqlqtpkqXKeey75vNX2jc6CFKOndO+SMKaMor8xQSGpYtr5ud4k7GKSUlReHh4S7bw8MjFB19wlCVe2xtTzgVJ0kKyhfqsj0oOFRnYo6n+5xzZ07p+w2rVKFRm5zOuyJbr/slNvfTboat7U6nU3NfnqIq1Wqk/Rt1Mi5WiQkJWrNsoXr0eUK9+g/Q9q1fa+yIgZoyc6Gq1Ux/BrJJ+37Zq+4P3Kvk5CQF5c6tl6bPVJkydvwPZVvbbb3nJbvbL7Hpvjnw2z4N6veQkpOTFRQUpNHjp6pE6YszxteteE2+vr666577DFd6xul06qXJE1WjZi2VLVfedM4V2X7P297/4Yb3tWfXLi1bvc50ikdsv+6Ap4wObm3cuFHt27dX3rx5lZCQoPXr1+uhhx5StWrV5HQ61bp1a3344YeZDnBFRkZq7NixLtsGDhutQcPHZEvjy1PG6/dff9HLry65bF/1WnU0f9k6xZ+M0/tvv6HnRw7WrEUrFJY/PJ0zmffvUVin02nNyKy17f9uzKA7OTFBH896VqGFS6jGnfdfo7grs/a6/8XmftrNsK39lRcnaP+v+zTt1cVp21JTUyVJ9Rs3U6d7H5QklS1/k3b+9L3ee2utVw5ulSpdWqvXrdfp06f0yaaP9Mzo4Vrw2jKv/R/7/2Rzu2TfPf9PNrfbdN8UK1FKryxao7NnTuvrzz/R1PHPaNIrC5SUnKS3163UywtXWXPdL5k4fpz2/bJXry1daTrFIzbf85Kd/UePRmnKxAmaPW9htk2euNZsvO5AVhh9W+Lzzz+vIUOGKCYmRq+99pruu+8+9e7dW5s2bdLHH3+soUOHauLEiZmeY8SIEYqPj3d5PD5waLb0vfziBG356nNNnb1QBQoVvmx/UFBuFSteQpWqVtOQ0c/L19dXG95Zny2vnZ3CQsPk6+ur6Ohol+2xsTEKD48wVOUeW9tzB1+cwZf41wyuSxJPxyvwX7O5zp9L0EczxyhXQJCaPzpGPr7GJ1Rae90vsbmfdjNsbJ/5UqS2bv5cU2YtUIGCf/8bFRIaJl/fXCpZ2nUtvhKlbtTxo0evdaZb/Pz8VaJESVWuXFVPDhik8uVv0qrlS01nucXWdhvv+Utsbr/EpvvGz89PRW8ooXI3VVaPR59U6bLl9fa6ldr5w7eKj4tVj3vaqF3TWmrXtJaOH43SwllT1bOz+VnoGZk4YZy++OxTzV+0VIUKX/7zvTey/Z63uX/3zp2KjY3R/V07qU71yqpTvbJ2bN+m1SuWqU71ykpJSTGdmCGbrzuQFUYHt3bu3KkePXpIkrp06aLTp0+rU6dOafvvvfde/fjjj5meIyAgQMHBwS6Pqx1VdzqdmjFlvL76/BO9NGuhihS9wb3nyank89m7mH128PP3V8VKlbV1y9cu27du2aJq1WsYqnKPre15IworKDhMR3Z/m7Yt5cJ5Hdv3kwqWqZi2LTkxQR++PFq+vrnUsv8zyuXnHZ9kZut1v8TmftrNsKnd6XTqlRcnaPPnn2jyzAWX/Rvl5+enChUr6/ChAy7b/zx0UIUKF7mGpVfDqeRk7/v31D12tNt0z/+bze0Zs+O+kSQ5nTqfnKzmre/UzMWv65VFa9Ie4REF1PHe7hr30hzTlZdxOp2aOP55ffrxJr26aLGK3eDez/fewPZ73ub+W+rV09o339Gq19enPSpVrqI2d7TTqtfXy9fwOrmZsfm628L02lisueXK/BSRv/j4+CgwMFChoaFp2/Lly6f4+Phr3jJjynh98uEHemHKDOXOk0exMRdHu/PkyauAwEAlJiZoxWvz1aBxU+WPKKBT8Sf1zhtrdOL4MTVp0eqa97rjwe49NWr4UFWqUkXVqtXQG6+vUVRUlDp37WY67Yq8tf38uUSdOvH3x+ieiTmmmMO/KSBPPuXNX1CVmnfQjxvXKrhgMQUXKKofN66Rr3+AytRp+tfzE/TRy6N04XySbu05RMmJCUpOvPhx1IH5QuTjY/YfS2+97u6yuZ92M2xpf+XF8fr0ow0aO2mGcue+/N8oSep8fw+NHzNEN1evqWo1b9G2rV/rm6+/0EuzFppMT9crM6aqYaNbVbhwYZ09e1YfbvxA27f9T7PmzDeddkU2t0v23PPpsbndpvtmyasvq1a9RipQsJASExL0xScb9dP32/X8i7MUHBKq4JBQl+N9c+VSWP5w3VCilJHezES+8Lw2fPCepr08S3ny5Elbcyhv3nwKDMzZT2bMDjbf85K9/Xny5L1sXbagoCCFhIZasV6brdcdyAqjg1ulSpXSr7/+qrJlL64v8M0336hEiRJp+w8fPqwiRa79b5nfeWONJGlgv4ddtg8dM06339lBvj6+OnRwvz784B2dOhmn4JBQVahYWTNeXaLSN3rfWgmSdHubtoo/Gad5c2brxInjKluuvGbNnaeiRYuZTrsib22PPrRPG6cNT/vz/9Zd/KG0bL2Watz9aVVtdY9Szifpm1WzlJxwRhGlK6j1Ey/ILzD3X8//VScO7JUkvfHMIy7nvueF15QvvNA1+krS563X3V0299Nuhi3t7765VpI0+DHXf6MGjx6n1ne0lyQ1atpCTw0do1VLF2rW1Em6oWQpPTthqqpUq3nZ+UyLiYnR6JFDFX3ihPLmy6dy5Spo1pz5qtegoem0K7K5XbLnnk+Pze023TdxcbF66YVRio2JVp48eVWqTHk9/+Is1ahT33Sax15fs0qS1LvnQy7bx74wQXd16GgiySM23/OS/f224rrjv8ThdDqdpl587ty5Kl68uO644450948aNUrHjh3TggULPDrvnyctmdadgfC83vHWtP+a6V/+ZjohywbcWubKBwHwCsdPJV35IC8Wwb9RRvj42Ps2Adulphr7UfmqHTl5znRClhUN8/7ZVBnxsfhtPbZLsfi/V1+Lv88Hes37wa6t/A/a9aEU7opdZtcn4F5i9DZ89NFHM90/fvz4a1QCAAAAAADgHpvXp7oeGV1QHgAAAAAAALgaDG4BAAAAAADAWgxuAQAAAAAAwFr/0aXfAAAAAAAAsoglt7wKM7cAAAAAAABgLQa3AAAAAAAAYC0GtwAAAAAAAGAt1twCAAAAAADwgMPBolvehJlbAAAAAAAAsBaDWwAAAAAAALAWg1sAAAAAAACwFoNbAAAAAAAAsBYLygMAAAAAAHiABeW9CzO3AAAAAAAAYC0GtwAAAAAAAGAtBrcAAAAAAABgLdbcAgAAAAAA8ABrbnkXZm4BAAAAAADAWgxuAQAAAAAAwFrX5dsSw/P6m06AhQbcWsZ0QpaF3T3bdEKWxa3vbzoBuKYKBgeYTgDgAR8fe992ckP+INMJwDXla/F/rwCuznU5uAUAAAAAAJBjGEv1KrwtEQAAAAAAANZicAsAAAAAAADWYnALAAAAAAAA1mLNLQAAAAAAAA84HCy65U2YuQUAAAAAAABrMbgFAAAAAAAAazG4BQAAAAAAAGux5hYAAAAAAIAHWHPLuzBzCwAAAAAAANZicAsAAAAAAADWYnALAAAAAAAA1mJwCwAAAAAAANZiQXkAAAAAAAAPsKC8d2HmFgAAAAAAAKzF4BYAAAAAAACsxeAWAAAAAAAArMWaWwAAAAAAAB5gzS3vwswtAAAAAAAAWIvBLQ+sWbVCbVo1V50aVdWtc0d9u2O76SSP2NxPe/YafE9NbZ56j46v6aWDy3po7ajbVa5YqMsx7evfqHfG3qnDK3oq8d3+url0+GXnebh1JX04ob2OremlxHf7KySP/zX6CtzjjdfeXbSbQbs5NvfTbobN7ZLd/bSbYXO7ZHc/7YD3Y3DLTRs3fKDJEyPVu08/rVn3lmrWrKX+fXsr6sgR02lusbmf9uzXuEpRzX3/JzUZ8obuHPOufH199N7z7ZQ74O93KucOzKVvdh/VmCVbMzxP7oBc2vTtIU15fce1yPaIt157d9BuBu3m2NxPuxk2t0t299Nuhs3tkt39tAN2cDidTqfpiOx27kL2n/P+bp1VsVIljX5mbNq2Du3aqFnzlnpq4KDsf8FsZnM/7VcWdvfsq3p+RHCgDq94WC2Hr9fXO6Nc9pUomE97Fz6ouk+u0Y/7Y9J9fuMqRfVRZAcV7rZA8WeTPXrtuPX9s9ydGe4bM2g3w+Z2ye5+2s2wuV2yu592M2xul+zup/3KAv+jK3kXffRN0wk54sjcjqYTssTrZm5541jb+eRk7d61U/UbNHLZXr9BQ/3w/XeGqtxncz/t10bwX28njDudZLgke9h07f+NdjNoN8fmftrNsLldsrufdjNsbpfs7qcdsIfXDW4FBARo9+7dpjNcxJ2MU0pKisLDXdccCg+PUHT0CUNV7rO5n/ZrY9IjDfX1ziPadSjWdEq2sOna/xvtZtBujs39tJthc7tkdz/tZtjcLtndTztgD2MTCJ9++ul0t6ekpGjixIlp/xFOnTo10/MkJSUpKcl1tonTN0ABAQHZE/oP//6oT6fTadXHf9rcT3vOmfZoY1UtFa4Ww9abTsl23n7tM0O7GbSbY3M/7WbY3C7Z3U+7GTa3S3b30w54P2ODW9OnT1e1atUUGhrqst3pdGr37t3KkyePW//RRUZGauzYsS7bRo15VqOfeS7bWsNCw+Tr66vo6GiX7bGxMQoPj8i218kpNvfTnrOm9mmkO28prZYj1uvPmLOmc7KNDdc+I7SbQbs5NvfTbobN7ZLd/bSbYXO7ZHc/7cgMg4TexdjbEsePH6/4+HiNGTNGn332WdrD19dXixcv1meffaZPP/30iucZMWKE4uPjXR5Dho3I1lY/f39VrFRZW7d87bJ965Ytqla9Rra+Vk6wuZ/2nDOtb2O1b3Cjbh/1tg4eO206J1t5+7XPDO1m0G6Ozf20m2Fzu2R3P+1m2Nwu2d1PO2APYzO3RowYoZYtW+qBBx5Qu3btFBkZKT8/P4/PExBw+VsQc+LTEh/s3lOjhg9VpSpVVK1aDb3x+hpFRUWpc9du2f9iOcDmftqz3/R+t6rrreXUefwGnUlMVqHQIElSfEKyziWnSJLC8gaoeIG8KpI/jySpfLEwSdKxuAQdO5koSSoUGqRCYblVpmiIJKlKyXCdTkzW4RNnFHfG7OL03nrt3UG7GbSbY3M/7WbY3C7Z3U+7GTa3S3b30w7YweiHdtapU0c7duzQY489ptq1a2v58uVeO7Xv9jZtFX8yTvPmzNaJE8dVtlx5zZo7T0WLFjOd5hab+2nPfn3bVpEkbYrs4LK99/RPtPyTvZKkO+qW0vwBLdL2LRvWSpL0wsptGr9qmySpV5sqGn1fnbRjPp5092XnMcVbr707aDeDdnNs7qfdDJvbJbv7aTfD5nbJ7n7aATs4nE6n03SEJK1evVoDBgzQiRMn9NNPP6lSpUpZPldOzNwCvFnY3bNNJ2RZ3Pr+phMAAAAAZFGg0Skz5hTrd/19IJck/TnnbtMJWeI1t2G3bt3UqFEj7dixQyVLljSdAwAAAAAAkC5vfdfZf5XXDG5J0g033KAbbrjBdAYAAAAAAAAsYezTEgEAAAAAAICrxeAWAAAAAAAArOVVb0sEAAAAAADwdqy55V2YuQUAAAAAAABrMbgFAAAAAAAAazG4BQAAAAAAAGux5hYAAAAAAIAnWHLLqzBzCwAAAAAAANZicAsAAAAAAADWYnALAAAAAAAA1mLNLQAAAAAAAA84HCy65U2YuQUAAAAAAABrMbgFAAAAAAAAazG4BQAAAAAAAGux5hYAAAAAAIAHWHPLuzBzCwAAAAAAANZicAsAAAAAAADWYnALAAAAAAAA1mLNLeA6ELe+v+mELAu7e7bphKti87UHAAAAgOsBg1sAAAAAAAAeYEF578LbEgEAAAAAAGAtBrcAAAAAAABgLQa3AAAAAAAAYC3W3AIAAAAAAPAAa255F2ZuAQAAAAAAwFoMbgEAAAAAAMBaDG4BAAAAAADAWqy5BQAAAAAA4AmW3PIqzNwCAAAAAACAtRjcAgAAAAAAgLUY3AIAAAAAAIC1WHMLAAAAAADAAw4Hi255E2ZuAQAAAAAAwFoMbgEAAAAAAMBaDG4BAAAAAADAWqy5BQAAAAAA4AHW3PIuzNwCAAAAAACAtRjcAgAAAAAAgLUY3AIAAAAAAIC1GNzywJpVK9SmVXPVqVFV3Tp31Lc7tptO8ojN/bSb4Y3tg++pqc1T79HxNb10cFkPrR11u8oVC3U5pn39G/XO2Dt1eEVPJb7bXzeXDr/sPA+3rqQPJ7TXsTW9lPhuf4Xk8b9GX4F7vPHau4t2M2xul+zup90Mm9slu/tpN8PmdsnuftoB78fglps2bvhAkydGqnefflqz7i3VrFlL/fv2VtSRI6bT3GJzP+1meGt74ypFNff9n9RkyBu6c8y78vX10XvPt1PugL8/HyN3YC59s/uoxizZmuF5cgfk0qZvD2nK6zuuRbZHvPXau4N2M2xul+zup90Mm9slu/tpN8PmdsnuftqREYfj+nzYyuF0Op2mI7LbuQvZf877u3VWxUqVNPqZsWnbOrRro2bNW+qpgYOy/wWzmc39tJtxrdrD7p59Vc+PCA7U4RUPq+Xw9fp6Z5TLvhIF82nvwgdV98k1+nF/TLrPb1ylqD6K7KDC3RYo/myyx68ft75/lrozw31jBu3m2NxPuxk2t0t299Nuhs3tkt39tF9ZYK4rH3M9Kjt4g+mEHPHri21MJ2QJM7fccD45Wbt37VT9Bo1cttdv0FA/fP+doSr32dxPuxk2tQf/9XbCuNNJhkuyh03X/t9oN8PmdsnuftrNsLldsrufdjNsbpfs7qcdsIdXjbHGxcVpyZIl2rdvn4oUKaLu3burePHimT4nKSlJSUmu/6PW6RuggICA7Os6GaeUlBSFh7uu2xMeHqHo6BPZ9jo5xeZ+2s2wqX3SIw319c4j2nUo1nRKtrDp2v8b7WbY3C7Z3U+7GTa3S3b3026Gze2S3f20A/YwOnOraNGiiom5+Fah/fv3q1KlSpo0aZL27dunV199VVWrVtWePXsyPUdkZKRCQkJcHlMmReZIr+Nfb0B1Op2XbfNmNvfTboa3t097tLGqlgpX9ymbTKdkO2+/9pmh3Qyb2yW7+2k3w+Z2ye5+2s2wuV2yu592pMfhcFyXD1sZnbl19OhRpaSkSJJGjhypm266Se+//75y586tpKQk3XPPPRozZoxef/31DM8xYsQIPf300y7bnL7ZN2tLksJCw+Tr66vo6GiX7bGxMQoPj8jW18oJNvfTboYN7VP7NNKdt5RWyxHr9WfMWdM52caGa58R2s2wuV2yu592M2xul+zup90Mm9slu/tpB+zhNWtu/d///Z/GjBmj3LlzS5ICAgI0evRobd2a8aedXTouODjY5ZGdb0mUJD9/f1WsVFlbt3ztsn3rli2qVr1Gtr5WTrC5n3YzvL19Wt/Gat/gRt0+6m0dPHbadE628vZrnxnazbC5XbK7n3YzbG6X7O6n3Qyb2yW7+2kH7GF8za1L096SkpJUqFAhl32FChXSiRPe8X7gB7v31KjhQ1WpShVVq1ZDb7y+RlFRUerctZvpNLfY3E+7Gd7aPr3frep6azl1Hr9BZxKTVSg0SJIUn5Csc8kXZ4KG5Q1Q8QJ5VSR/HklS+WJhkqRjcQk6djJRklQoNEiFwnKrTNEQSVKVkuE6nZiswyfOKO6M2cXpvfXau4N2M2xul+zup90Mm9slu/tpN8PmdsnuftoBOxgf3GrRooVy5cqlU6dO6ZdfflHlypXT9h06dEgREd4xZfL2Nm0VfzJO8+bM1okTx1W2XHnNmjtPRYsWM53mFpv7aTfDW9v7tq0iSdoU2cFle+/pn2j5J3slSXfULaX5A1qk7Vs2rJUk6YWV2zR+1TZJUq82VTT6vjppx3w86e7LzmOKt157d9Buhs3tkt39tJthc7tkdz/tZtjcLtndTzsyYvHyVNclh9PpdJp68bFjx7r8uV69emrdunXan4cMGaI//vhDq1at8ui85y5kSx6AayDs7tmmE65K3Pr+phMAAAAAYwKNT5kxo/zQjaYTcsQvk283nZAlRm/DZ599NtP9U6ZMuUYlAAAAAAAAsJHXLCgPAAAAAAAAeOo/OoEQAAAAAAAgaxwsuuVVmLkFAAAAAAAAazG4BQAAAAAAAGsxuAUAAAAAAABrseYWAAAAAACAB1hyy7swcwsAAAAAAADWYnALAAAAAAAA1mJwCwAAAAAAANZicAsAAAAAAADWYkF5AAAAAAAAD/j4sKK8N2HmFgAAAAAAAKzF4BYAAAAAAACsxeAWAAAAAAAArMWaWwAAAAAAAB5wsOSWV2HmFgAAAAAAAKzF4BYAAAAAAACsxeAWAAAAAAAArMWaWwAAAAAAAB5wsOiWV2FwC4BRcev7m064KmGdF5hOyLK413uZTgAAAACAq8bbEgEAAAAAAGAtBrcAAAAAAABgLd6WCAAAAAAA4AGW3PIuzNwCAAAAAACAtRjcAgAAAAAAgLUY3AIAAAAAAIC1GNwCAAAAAACAtVhQHgAAAAAAwAMOVpT3KszcAgAAAAAAgLUY3AIAAAAAAIC1GNwCAAAAAACAtVhzCwAAAAAAwAOsueVdmLkFAAAAAAAAazG4BQAAAAAAAGsxuAUAAAAAAABrseYWAAAAAACAB1hyy7swcwsAAAAAAADWYnALAAAAAAAA1mJwCwAAAAAAANZicMsDa1atUJtWzVWnRlV169xR3+7YbjrJIzb3026Gze2S9/UP7lhNmye31/GVD+ng4vu1dnhLlSsactlxo7rW1O8L71Xs6h76cNwdqlg8NG1fiQJ5lbi+V7qPjg1KX8OvJmPedt09Qbs5NvfTbobN7ZLd/bSbYXO7ZHc/7UiPw+G4Lh+2YnDLTRs3fKDJEyPVu08/rVn3lmrWrKX+fXsr6sgR02lusbmfdjNsbpe8s79x5cKau2GXmgx7R3c+t0G+vj5679nblTvg78/2GHT3zXryrioaOP8bNRr6to7FJej959oob6CfJOmPmLMq1XOFy+P5VTt0JvG8Pvz2sKkvLY03Xnd30W6Ozf20m2Fzu2R3P+1m2Nwu2d1PO2AHh9PpdJqOyG7nLmT/Oe/v1lkVK1XS6GfGpm3r0K6NmjVvqacGDsr+F8xmNvfTbobN7dK16w/rvCDLz40IDtThJQ+o5aj39PWuo5Kk3xfep1nv/ayX1v8oSfLP5aODi+/X6KXbtPCjPeme55uXOuj732PUb9ZXHr1+3Ou9styeEZvvG9rNsbmfdjNsbpfs7qfdDJvbJbv7ab+ywFxXPuZ6VGPsp6YTcsR3zzY3nZAlzNxyw/nkZO3etVP1GzRy2V6/QUP98P13hqrcZ3M/7WbY3C7Z0x+c21+SFHcmSZJUqlA+FcmfWx9//2faMckXUvXVzqOqd1PBdM9R48ZwVb8xQks+3pvzwVdgy3VPD+3m2NxPuxk2t0t299Nuhs3tkt39tAP2MDq49d1332n//v1pf16+fLkaNmyo4sWLq1GjRlq9evUVz5GUlKRTp065PJKSkrK1M+5knFJSUhQeHu6yPTw8QtHRJ7L1tXKCzf20m2Fzu2RP/6SedfX1rqPadShOklQ4NEiSdPxkostxx08mqlBo7nTP0b1lBe0+HKete4/nbKwbbLnu6aHdHJv7aTfD5nbJ7n7azbC5XbK7n3ZkxuG4Ph+2Mjq49cgjj+jAgQOSpAULFqhPnz6qXbu2Ro0apTp16qh3795atGhRpueIjIxUSEiIy2PKpMgc6f334mpOp9OqBdds7qfdDJvbJe/un9angaqWyq/uUy+fzuyU67vFHY6L7f8W6O+rrreW0ZKPf8mxzqzw5ut+JbSbY3M/7WbY3C7Z3U+7GTa3S3b30w54P6Pvjt27d6/KlCkjSZo9e7amT5+uPn36pO2vU6eOxo8fr4cffjjDc4wYMUJPP/20yzanb0C2doaFhsnX11fR0dEu22NjYxQeHpGtr5UTbO6n3Qyb2yXv75/aq77urFNCLUe9pz9jEtK2H/1rxlah0Nw6Gvf37K0CIUE6Hp942Xnurl9auf1zacXn+3I+2g3eft0zQ7s5NvfTbobN7ZLd/bSbYXO7ZHc/7YA9jM7cCgoK0okTF6dE/vnnn6pbt67L/rp167q8bTE9AQEBCg4OdnkEBGTv4Jafv78qVqqsrVu+dtm+dcsWVateI1tfKyfY3E+7GTa3S97dP613fbWvV0q3P/OBDh4/47LvwLHTiopNUItqxdK2+eXyUePKhbV1z+VvO+zRsoLe33ZI0afO5Xi3O7z5ul8J7ebY3E+7GTa3S3b3026Gze2S3f20A/YwOnOrTZs2mjNnjhYsWKAmTZpo3bp1qlatWtr+tWvXqmzZsgYL//Zg954aNXyoKlWpomrVauiN19coKipKnbt2M53mFpv7aTfD5nbJO/un92mgrreWUefITTqTeF6F/lpjKz4hWeeSUyRJs977WUPuqaZfo+L1a9QpDe1UTYlJF7Tmy99cznVj4WA1qlRYHV748Jp/HZnxxuvuLtrNsbmfdjNsbpfs7qfdDJvbJbv7aQfsYHRwa9KkSWrYsKGaNGmi2rVr66WXXtLnn3+uihUrau/evdq6davWr19vMjHN7W3aKv5knObNma0TJ46rbLnymjV3nooWLXblJ3sBm/tpN8Pmdsk7+/u2qSRJ2vTCnS7be7/8hZZ/dvGthS+t/1GB/rk0vU9DheX117Z9J3Tn2I06c+68y3O6tyivI7Fn9fH3f1ybeDd543V3F+3m2NxPuxk2t0t299Nuhs3tkt39tCMjrF3mXRzO9FYpvoZOnjypiRMn6t1339Xvv/+u1NRUFSlSRA0bNtTAgQNVu3Ztj8957kIOhAJAOsI6LzCdkGVxr/cynQAAAADLBRqdMmNOrXGfmU7IETvGNDOdkCXGb8PQ0FBNnDhREydONJ0CAAAAAAAAyxhdUB4AAAAAAAC4GsZnbgEAAAAAANiEJbe8CzO3AAAAAAAAYC0GtwAAAAAAAGAtBrcAAAAAAABgLdbcAgAAAAAA8ICDRbe8CjO3AAAAAAAAYC0GtwAAAAAAAGAtBrcAAAAAAABgLdbcAgAAAAAA8ABLbnkXZm4BAAAAAADAWgxuAQAAAAAAwFoMbgEAAAAAAMBarLkFAAAAAADgAQeLbnkVZm4BAAAAAADAWgxuAQAAAAAAwFoMbgEAAAAAAMBaDG4BAAAAAADAWiwoDwAAAAAA4AHWk/cuDG4BwFWIe72X6YQsi7hvsemELIte2cN0AgAAAAAvwdsSAQAAAAAAYC0GtwAAAAAAAGAt3pYIAAAAAADgAQeLbnkVZm4BAAAAAADAWgxuAQAAAAAAwFoMbgEAAAAAAMBarLkFAAAAAADgAZbc8i7M3AIAAAAAAIC1GNwCAAAAAACAtRjcAgAAAAAAgLVYcwsAAAAAAMADDhbd8irM3AIAAAAAAIC1GNwCAAAAAACAtRjcAgAAAAAAgLVYcwsAAAAAAMADLLnlXZi5BQAAAAAAAGsxuAUAAAAAAABrMbgFAAAAAAAAazG4BQAAAAAAAGsxuOWBNatWqE2r5qpTo6q6de6ob3dsN53kEZv7aTfD5nbJ7n5vax/Uoaq+mHCnopbcr/3zu2rVkOYqVyT4suNGdq6ufXO76MTyB7Th2dtV8YbQtH1hefz1Ys+6+nb63Tq+7AHtnn2PpvS8RcFBftfwK8mct113T9jcLtndT7sZNrdLdvfTbobN7ZLd/bQjPQ6H47p82IrBLTdt3PCBJk+MVO8+/bRm3VuqWbOW+vftragjR0ynucXmftrNsLldsrvfG9sbVSqseR/uUfNR76vdCx8pl49Db49updwBf3/o7sD2VfT4HZU0aNFWNRnxno6dTNQ7o1spb+DFY4rkz60i+YM0atk21R38th6dtVktqxXT7H4NTX1ZLrzxurvL5nbJ7n7azbC5XbK7n3YzbG6X7O6nHbCDw+l0Ok1HZLdzF7L/nPd366yKlSpp9DNj07Z1aNdGzZq31FMDB2X/C2Yzm/tpN8Pmdsnu/mvVHnHf4qw/N1+ADiy8V62f3aCvdx+TJP36ahfN+mCXpr39syTJP5ePfp/fTc+s2K5FH/+S7nnurldSC564VQUfXK6UVPf/OYpe2SPL7RnhnjHH5n7azbC5XbK7n3YzbG6X7O6n/coCc135mOtR45c2m07IEV8NamQ6IUuYueWG88nJ2r1rp+o3cP1Lrt+goX74/jtDVe6zuZ92M2xul+zut6U9OLe/JCnuTJIkqVTBvCoclluf/PD3bwKTL6Rq866jqluhYKbnOZ143qOBrZxgy3VPj83tkt39tJthc7tkdz/tZtjcLtndTztgj//oGKtn4k7GKSUlReHh4S7bw8MjFB19wlCV+2zup90Mm9slu/ttaY/sXkdbdh/TrsMnJUmFQoMkScfjE12OOxGfqOIRedM9R/68ARrWqZoWbdqbo63usOW6p8fmdsnuftrNsLldsrufdjNsbpfs7qcdmbF5farrkdGZW0888YS++uqrqzpHUlKSTp065fJISkrKpkJX/755nU6nVTe0zf20m2Fzu2R3vze3T32krqqUyK8eM764bN9lb3R3OJTenKx8QX5aN7yl9vxxUhPWfZ8TmVnizdf9Smxul+zup90Mm9slu/tpN8PmdsnuftoB72d0cGvWrFlq2rSpypcvr0mTJuno0aMenyMyMlIhISEujymTIrO1Myw0TL6+voqOjnbZHhsbo/DwiGx9rZxgcz/tZtjcLtnd7+3tL/asq7a1Sqjt2I06EpuQtv3YyYszti7N4LqkQHDgZbO58gbm0vqRt+nsufO698XPdCHF/NKP3n7dM2Nzu2R3P+1m2Nwu2d1Puxk2t0t299MO2MP4mlsfffSR2rZtqxdffFElSpRQ+/bt9d577yk1NdWt548YMULx8fEujyHDRmRro5+/vypWqqytW7522b51yxZVq14jW18rJ9jcT7sZNrdLdvd7c/tLD9fVXXVL6I7nN+rgiTMu+w4cP6OjcQlqfnPRtG1+vj5qVKmw/m/v8bRt+YL89PboVjp/IVVdJn+ipPMp16w/M9583a/E5nbJ7n7azbC5XbK7n3YzbG6X7O6nHbCH8TW3qlatqhYtWmjKlClav369Fi1apA4dOqhQoULq0aOHevbsqbJly2b4/ICAAAUEBLhsy4lPS3ywe0+NGj5UlapUUbVqNfTG62sUFRWlzl27Zf+L5QCb+2k3w+Z2ye5+b2yf9kg9dW50o7pN/kSnEy+oYMjFGVqnEpJ17q8Bqlkf7NLgu2/Wb1Gn9NvRUxp8981KTLqgtZt/l3Rxxtbbo1opd4Cver3ymfIF+SvfXxO9ok+dU6rhD+/1xuvuLpvbJbv7aTfD5nbJ7n7azbC5XbK7n3ZkhHd3ehfjg1uX+Pn5qUuXLurSpYsOHTqkRYsWafHixZo4caJSUsz/Zv/2Nm0VfzJO8+bM1okTx1W2XHnNmjtPRYsWM53mFpv7aTfD5nbJ7n5vbO/d+iZJ0saxbVy29521WSu++FWSNO3tnxXkn0vTetVTaJ4Abf/1hNqP/0hn/vqNQ40bI3RL+QKSpJ9e6eRynkqPrdOhf80Gu9a88bq7y+Z2ye5+2s2wuV2yu592M2xul+zupx2wg8PpNPerch8fHx09elQFC6b/MfFOp1Mff/yxbrvtNo/OmxMztwDgehNx32LTCVkWvbKH6QQAAABICvSaKTPXVpNpX1/5IAt9MbCh28d++eWXmjJlinbs2KGoqCitX79eHTp0SNvfo0cPLVmyxOU5devW1datW9P+nJSUpMGDB2vVqlVKTExUixYtNHv2bN1www0edRtdc6tkyZLy9fXNcL/D4fB4YAsAAAAAAAA56+zZs6pWrZpmzpyZ4TG33367oqKi0h4ffPCBy/4BAwZo/fr1Wr16tTZv3qwzZ87ozjvv9PgdfEbHWPfv32/y5QEAAAAAADzmYNEttWnTRm3atMn0mICAABUuXDjdffHx8Vq4cKGWLVumli1bSpKWL1+u4sWL6+OPP1br1q3dbjH+aYkAAAAAAAAwLykpSadOnXJ5JCUlZfl8n3/+uQoWLKjy5curd+/eOn78709S37Fjh86fP69WrVqlbStatKiqVKmiLVu2ePQ6DG4BAAAAAABAkZGRCgkJcXlERkZm6Vxt2rTRihUr9Omnn+qll17Stm3b1Lx587TBsqNHj8rf319hYWEuzytUqJCOHj3q0Wv9R5d+AwAAAAAAwD+NGDFCTz/9tMu2gICALJ2ra9euaf9/lSpVVLt2bZUsWVLvv/++OnbsmOHznE6nx2/7ZHALAAAAAAAACggIyPJg1pUUKVJEJUuW1L59+yRJhQsXVnJysuLi4lxmbx0/flwNGjTw6Ny8LREAAAAAAMADDsf1+chJMTExOnz4sIoUKSJJqlWrlvz8/LRp06a0Y6KiovTzzz97PLjFzC0AAAAAAAB45MyZM/r111/T/rx//359//33yp8/v/Lnz6/nnntOnTp1UpEiRXTgwAGNHDlSERERuvvuuyVJISEheuSRRzRo0CCFh4crf/78Gjx4sKpWrZr26YnuYnALAAAAAAAAHtm+fbuaNWuW9udLa3V1795dc+bM0U8//aSlS5fq5MmTKlKkiJo1a6Y1a9YoX758ac+ZNm2acuXKpS5duigxMVEtWrTQ4sWL5evr61GLw+l0OrPny/Ie5y6YLgAA7xdx32LTCVkWvbKH6QQAAABICvyPTplpNmOL6YQc8dlTnr0d0Fv8R29DAAAAAACArPH00/yQs1hQHgAAAAAAANZicAsAAAAAAADWYnALAAAAAAAA1mLNLQAAAAAAAA+w5JZ3YeYWAAAAAAAArMXgFgAAAAAAAKzF4BYAAAAAAACsxZpbAAAAAAAAHvBh0S2vwswtAAAAAAAAWIuZWwDwHxW9sofphCwr3nuN6YQsOzy/q+mEq+J0mi7IOpt/wZqSau+F9/Wx+MIDAAArMHMLAAAAAAAA1mLmFgAAAAAAgAdsnhF+PWLmFgAAAAAAAKzF4BYAAAAAAACsxeAWAAAAAAAArMXgFgAAAAAAAKzFgvIAAAAAAAAecLCivFdh5hYAAAAAAACsxeAWAAAAAAAArMXgFgAAAAAAAKzFmlsAAAAAAAAe8GHJLa/CzC0AAAAAAABYi8EtAAAAAAAAWIvBLQAAAAAAAFiLNbcAAAAAAAA84HCw6JY3YeYWAAAAAAAArMXgFgAAAAAAAKzF4BYAAAAAAACsxZpbAAAAAAAAHmDJLe/CzC0AAAAAAABYi8EtD6xZtUJtWjVXnRpV1a1zR327Y7vpJI/Y3E+7GTa3S3b305696pcvoOVPNdJPU+/Side6qk2NYi77CwQH6JVHbtFPU+/SwbmdtObpW3Vjobxp+0Pz+Cvy/pr6ZkIbHZzbSd+9eKcm3FdD+YL8rvWXkiFvvO7u2LF9m5587FHd1qyRqlepoE8/+dh0ksdsvfb/tGjBq6pZ9SZNmTTBdIrbbL/uNvfTbobN7ZLd/bQD3o/BLTdt3PCBJk+MVO8+/bRm3VuqWbOW+vftragjR0ynucXmftrNsLldsruf9uyXO8BXOw+f1PAVO9Ldv+SJRipZII8efGWzmj/3kQ7HJGjd4KbK7e8rSSocGqTCoYF6ds0PajJmo55Y+D81r1pEM3rWuZZfRoa89bq7IzExQeUrVNDwkc+YTskSm6/9JTt//klvrlurcuUrmE5xm+3X3eZ+2s2wuV2yu592wA4Mbrlp2ZLXdHenTup4T2fdWKaMho4YpcJFCmvtmlWm09xicz/tZtjcLtndT3v2++Sno4p882e9v+PPy/bdWCiv6pSN0JClO/T9/lj9dvS0hi7doTyBudSxXklJ0p4/49Vz1hZ99MMRHThxVpt3H9eEN35Uq+pF5etjfsEFb73u7mjUuIkef3KgWtzWynRKlth87SUpIeGsRg0frDHPjlNwcLDpHLfZft1t7qfdDJvbJbv7aUdGHNfp/9mKwS03nE9O1u5dO1W/QSOX7fUbNNQP339nqMp9NvfTbobN7ZLd/bRfewF+F2dnJZ1PSduW6nTq/IVU1S0XkeHzgnP76/S580pJdeZ4Y2Zsve7Xg+vh2k8c/7waNW6quvUbmE5xm+3X3eZ+2s2wuV2yu592wB7GB7deeeUVde/eXWvXrpUkLVu2TJUqVdJNN92kkSNH6sKFC5k+PykpSadOnXJ5JCUlZWtj3Mk4paSkKDw83GV7eHiEoqNPZOtr5QSb+2k3w+Z2ye5+2q+9fVGndCj6rEbfc7NCcvvJz9dHT7a9SYVCg1QoNDDd54Tl8dfT7Spp6ee/XePay9l63a8Htl/7Dze8rz27dumJAU+bTvGI7dfd5n7azbC5XbK7n3bAHkYHt8aNG6dRo0bp7NmzeuqppzRp0iQNHDhQ999/v7p3764FCxZo3LhxmZ4jMjJSISEhLo8pkyJzpNfxr8/6dDqdl23zZjb3026Gze2S3f20XzsXUpzqOfNrlSmcT7/O6qhDr3ZSw5sK6uMfj6Q7KytvYC6tHHirfjlySlPe3mmgOH22XffriY3X/ujRKE2ZOEEvTJyigIAA0zlZYuN1/yeb+2k3w+Z2ye5+2gHvl8vkiy9evFiLFy9Wx44d9cMPP6hWrVpasmSJ7r//fknSTTfdpKFDh2rs2LEZnmPEiBF6+mnX3zg6fbP3h7Sw0DD5+voqOjraZXtsbIzCwzN+y4q3sLmfdjNsbpfs7qfdjB8PxqnZsx8pX5Cf/HP5KOZ0kjaObqkfDsS6HJcnMJfWDGqis+fOq/srm3UhxexbEiW7r7vtbL72u3fuVGxsjO7v2iltW0pKir7dsV1rV63Q1h0/ytfX12Bhxmy+7pLd/bSbYXO7ZHc/7YA9jM7cioqKUu3atSVJ1apVk4+Pj6pXr562v2bNmjpyhU9yCAgIUHBwsMsju38D6efvr4qVKmvrlq9dtm/dskXVqtfI1tfKCTb3026Gze2S3f20m3U68bxiTifpxkJ5Vb10mDZ89/cC9HkDc+n1QU10/kKqHnx5s5IupBos/dv1cN1tZfO1v6VePa198x2ten192qNS5Spqc0c7rXp9vdcObEl2X3fJ7n7azbC5XbK7n3ZkxsdxfT5sZXTmVuHChbVr1y6VKFFC+/btU0pKinbt2qXKlStLknbu3KmCBQuaTEzzYPeeGjV8qCpVqaJq1WrojdfXKCoqSp27djOd5hab+2k3w+Z2ye5+2rNfnoBcKl0wb9qfSxTIoyrFQxV3Nll/xiborto3KPp0kv6MTVDFG0I0/r6a2vDtn/p857GLzw/MpdcHN1WQv6/6z9usfIF+yhfoJ0mKPp2kVKfZGVzeet3dkZBwVocOHUr7859//qE9e3YrJCRERYoUNVjmHluvfZ48eVW2XHmXbUFBQQoJDb1suzey9bpfYnM/7WbY3C7Z3U87YAejg1v33XefHnroIbVv316ffPKJhg0bpsGDBysmJkYOh0Pjx4/XPffcYzIxze1t2ir+ZJzmzZmtEyeOq2y58po1d56KFi1mOs0tNvfTbobN7ZLd/bRnv2qlwvT28OZpf37h3ou/sVy9eb+eWPg/FQoN0vP31lCB4AAdO3lOa7cc0Evv7Pr7+SXDVLvMxQVZt02+0+XcNQe/q8MxCdfgq8iYt153d+z8+Wf1fvihtD+/NPniupnt2t+tceMnmspym83X3ma2X3eb+2k3w+Z2ye5+2gE7OJxOc79uTklJ0cSJE7V161Y1atRIw4YN0+rVqzV06FAlJCSoXbt2mjlzpvLkyePRec9l/gGLAADLFe+9xnRClh2e39V0wlUxPEntqti8fm56H25gC1+b3+MAALiiQKNTZsy5a9420wk54p0+dUwnZInR29DX11ejRo1y2datWzd168Y0SQAAAAAA4J341EnvYnRBeQAAAAAAAOBqMLgFAAAAAAAAazG4BQAAAAAAAGv9R5d+AwAAAAAAyBqW3PIuzNwCAAAAAACAtRjcAgAAAAAAgLUY3AIAAAAAAIC1WHMLAAAAAADAAz4suuVVmLkFAAAAAAAAazG4BQAAAAAAAGsxuAUAAAAAAABrMbgFAAAAAAAAa7GgPAAAAAAAgAdYT967MHMLAAAAAAAA1mJwCwAAAAAAANZicAsAAAAAAADWYs0tAAAAAAAADzhYdMurMHMLAAAAAAAA1mJwCwAAAAAAANa6Lt+WuH1/nOmEq1K7dJjphP+k/SfOmk7IstIF8phOAK6pw/O7mk7Isj9jE00nXJXQPH6mE7IsT4C9P/b4+vDWB1NSU52mE7Ls9+P2/mxTIiK36YQs88/F/AFT4s4mm07IsrA8/qYTAKvZ+1MeAAAAAACAASy55V34tQIAAAAAAACsxeAWAAAAAAAArMXgFgAAAAAAAKzFmlsAAAAAAAAe8GHRLa/CzC0AAAAAAABYi8EtAAAAAAAAWIvBLQAAAAAAAFiLNbcAAAAAAAA8wIpb3oWZWwAAAAAAALAWg1sAAAAAAACwFoNbAAAAAAAAsBaDWwAAAAAAALAWC8oDAAAAAAB4wOFgSXlvwswtAAAAAAAAWIvBLQAAAAAAAFgrS29LPHDggL766isdOHBACQkJKlCggGrUqKH69esrMDAwuxsBAAAAAACAdHk0uLVy5Uq9/PLL+t///qeCBQuqWLFiCgoKUmxsrH777TcFBgbq/vvv17Bhw1SyZMmcagYAAAAAADDGhyW3vIrbg1s1a9aUj4+PevToobVr16pEiRIu+5OSkvTNN99o9erVql27tmbPnq3OnTtnezAAAAAAAABwidtrbo0bN07bt2/X448/ftnAliQFBASoadOmmjt3rnbv3q1SpUplZ+c1Fxd9XPNffFZP3ttK/To10XNPPKgDv+5J279jy2eaOuYpPXVfaz1yZz0d+v0Xg7XuWbNqhdq0aq46NaqqW+eO+nbHdtNJbrOhfcPbr+uph7vo3raNdW/bxhrWv7t2/N/XaftPxsZoRuSz6tmplbq0bqCxQx7TkT8OGSy+Mhuue2Zs7qfdDBva31+/Vv27d1an1g3VqXVDPf3oQ9q2dXPa/uWL5qjP/R1092311KVNY40c0Fd7dv5ksNjVdzu2a8hT/XVXq6ZqULOyvvjsE5f9C+bOUreOd6p5g9pq3aS+nnz0Ee386UdDte6x4b7JCO3X3to1q9Sl411qVK+WGtWrpYfu76rNX31pOuuK3ly5SJ1a1NKiWS+mu3/u1PHq1KKW3ntj5TUuc8+FCxc0Z+Z0tW/TUo1uqa72bW/T/LmzlJqaajrNbbbe85fY0L9i8QL17d5NbZrWVYfWTTRq8JM6dHC/yzFffvaxhjzRV3fd1lhNb6mqfb/syeBs3sGG6w5kB7cHt+644w63TxoREaE6depkKcgbnD1zSpFD+8g3Vy4NeG6axs1epS6PPKncefKmHZN07pzKVrpZnbr3N1jqvo0bPtDkiZHq3aef1qx7SzVr1lL/vr0VdeSI6bQrsqU9vEBBPdjnSb346nK9+OpyVa1ZR5GjBurQ/t/kdDoVOfppHYv6QyPHT9O0+StVoHARPTvoUZ1LTDSdni5brntGbO6n3Qxb2iMKFlLPR5/UjPkrNWP+SlWrWUfjRgzQwf2/SpKKFS+pfgOHa/aSdZoy+zUVLFxUowf1U3xcrOHyi86dS1TZ8hX09LBR6e4vUbKkBg0bpWVr12vOomUqUrSYBjzWW3Fe0v9vttw36aHdjEKFCumJAYO0YvU6rVi9TrfUraeBTz6m337dZzotQ7/u2alN769XyRvLpbv//zZ/pn17flb+8ALXuMx9S19boDdeX6MhI0Zr7fr39eTAwVq+ZJHWrFpuOs0tNt/zkj3933+7XR06d9PshSv04ivzlJKSoiFP9FViYkLaMecSE1WlWnX1eWyAuVA32XLdgezgcDqdTncPHjlypA4cOOD2ycuUKaNx48ZlpeuqbN4Xd1XPX7d4ln7d9aOGT371isdGHzuiYY901LMvL1WJG8tf1eteUrt0WLac55/u79ZZFStV0uhnxqZt69CujZo1b6mnBg7K9tfLTteqff+Js9l2rkseaNdU3R8doEo319BjD96tl197XSVKl5EkpaSkqMfdLfVQnyd12513X9XrlC6QJztyXdh8z0h299NuxrVq/zM2+we0u7S9VY/0H6jW6XwvSTh7Rvfc3kgTpr2q6rXrXvVrhebxu+pzXNKgZmVFvvSymjRrkeExZ8+c0W231tXLcxaqdt16V/V6eQKy9Dk6meKeN+Natqemuv2jcpY1aVhXAwYN0d0d78nW8/5+/Op/tklMTNCQvver91PD9caKhSpVprwefmxw2v6YE8c1/PHuGjNppiaMfEp3drpPd3a676pft0RE7qs+xz8NfPxR5Q8P15ix49O2DX36SQUGBur5CZOz9bX8c2X/B9Lb/N+rdO36484mZ9u5JOlkXKw6tG6iGXNfU7WatV32RR35U/d2uF3zl7+ucuVvuurXCsvjf9Xn+Ldrdd0Ds/+fVys8sPwH0wk5YvkD1UwnZIlHt+HGjRu1fv16t451Op3q0qVLpoNbUVFRmjNnjjZv3qyoqCj5+vqqdOnS6tChg3r06CFfX19P8rLN9//3larUrKfZkSP1y8/fKTS8gJq17agmt3cw0nO1zicna/eunXq4Vx+X7fUbNNQP339nqMo9tranpKRoy+cf69y5RN1U+WadP3/xH1o//7//0fL19VWuXH7a9dP3Vz24ld1sve6X2NxPuxm2tqekpGjzZ5t07lyiKla++bL958+f14Z33lCevHlVumz2/ALmWjp/Pllvv/m68ubNp7LlK5jOuYyt941Eu7dISUnRpo82KjExQTdXq246J10LZkxUrXqNVK1WXb2xYqHLvtTUVL08cYzad3lQJUqVMVTonmo1aunNdat18MB+lSxVWr/s3aMfvvtWTw8dYTrtimy/523uP3PmjCQpX0iI4RLP2XzdgazwaHDL6XR69CmImU0K2759u1q2bKnSpUsrKChIv/zyi+6//34lJydr8ODBWrhwoT788EPly5cv09dISkpSUlKSy7bk5CT5+we43flvJ44e0WcfvKlWHe7VHV26a/8vu7Rq3jT5+fmrQYu2WT6vKXEn45SSkqLw8HCX7eHhEYqOPmGoyj22tR/4fZ+G9++h5ORkBQYFafi4l1S81I26cOG8ChQqomXzZ6r/oFEKCAzSO2uXKy42WnGx3vd12Hbd/83mftrNsK19/2/7NKjfQ0pOTlZQUJDGjJ+aNitUkv7v6y81aewwJZ07p/zhERo/da5CQrN/VnBO+frLz/XMiME6d+6cwiMKaPqc+QoN875+2+6bf6LdrH2/7FX3B+5VcnKSgnLn1kvTZ6pMmbKmsy6z+dMP9fuvezRp9rJ097+1erF8fX11R8d7r3GZ57o/3EtnzpxW5w53yMfXV6kpKer3xAC1buP+0ium2H7P29rvdDo1e/oUVa1WUzeWSf8tud7M1usOZJVHc2YdDs8+6zKz4wcMGKCBAwfqu+++05YtW7RkyRL98ssvWr16tX7//XclJiZq9OjRV3yNyMhIhYSEuDyWz53mUee/OZ2pKlmmgjp176eSZSqoaZu7dWvru/TZB29e1XlN+/ffh9Pp9Pjv1BRb2osVL6VpC1Zp8uwlatO+s16OfEaHD/yuXLn8NOz5KTpy+KAeaNdUXVs30M/fb1fNug3l42NmhqI7bLnuGbG5n3YzbGm/oUQpzVy0RlPnLlXb9l300vhndGj/b2n7q9Wso5mL1uilOUtUq25DRT47VCe9dM2q9NSsc4uWrHpDr762QvUaNNKYYYMUGxtjOitDttw36aHdjFKlS2v1uvVasmK1OnfppmdGD9dvv/1qOstF9PGjWjTrRT014oV0f2n82y+79f6bq/X40LFWXPdNGz/Qhvff1QuRU7R89Rt6blykVixZpPfeect0mttsvucl+/pnTBmv3379RWNemGQ65arYdt2BrDL27thvv/1WS5cuTfvzfffdp4cffljHjh1ToUKFNHnyZPXo0UMzZszI9DwjRozQ008/7bJt++GEDI52T0hYhIqWKOWyrUjxUtrx9edXdV5TwkLD5Ovrq+joaJftsbExCg+PMFTlHtva/fz8VOSGi58mWvamStq3Z6fefWOl+g8arbIVKmn6wtU6e+a0Lly4oJDQMA3p95DKVqhouPpytl33f7O5n3YzbGv38/NT0b++15S/qbL27dmpt9et1BNDxkiSAoOCVPSGEip6QwndVPlm9bq3nT58b726PviIyWy3BQXl1g0lSuqGEiVV5eZq6tK+jd5760099HBv02kubLtv/ol2s/z8/FWixMV3Q1SuXFU7f/5Zq5Yv1ehnnzdc9rffftmt+JOxGvLoA2nbUlNTtOvHb7XhrbV6sPcTij8Zq7733uGyf8ncaXrvjZWau/I9E9kZmjHtRXV/uJda/TVTq2y58oqKOqLFC+fpzrs6mI27AtvveRv7Z0yZoK+//Fwvv7pYBQsVNp2TJTZed9swRuhdsn+1QzcVLFhQUVFRaX8+duyYLly4oODgYElSuXLlFBt75d8yBwQEKDg42OVxNW9JlKRylW7W0T8OuWw79udhhRe08xubn7+/KlaqrK1bvnbZvnXLFlWrXsNQlXtsbpckp5w6n3zeZVuevPkUEhqmI38c0m97d+mWhk3NxGXC9utucz/tZtjcLl38Lez55IwX0XU6lbb2n42cTqeSM/n6TLH5vqHd23jfPX5zzVs0bcEavTRvZdqjTIVKatyijV6at1LNWrfT1PmrXfbnDy+gu7o8qDGTZprOv0zSuUT5+Lj+Tx8fX185U1MNFbnP9nvepn6n06npU8brq88/0bTZC1Wk2A2mk7LMpusOZAeP19x6/nn3fqPkdDozXXOrQ4cOevTRRzVlyhQFBARo3LhxatKkiYKCgiRJe/fuVbFixTzJyza3te+myCG99f7axardqIX2/7JLX2x8S90fH552zJnT8Yo9cUwnYy6OhB/946AkKSQsXCFh4eme16QHu/fUqOFDValKFVWrVkNvvL5GUVFR6ty1m+m0K7Klfdn8V1SzbkNFFCisxMSz2vzph9r5/Q49M/niD3hff75JwSFhKlCosA7+/qsWvDJFtzRqqhp16hsuT58t1z0jNvfTboYt7YtffVm16zVSgYKFlJCQoC8/2aifvt+u51+cpXOJiVq9dL7qNWqqsPAInY6P13vr1yr6xDE1bnab6XRJUkLCWf1x+O9fIEX9+Yd+2btbwcEhCgkN1ZIF89SoSTOFRxTQqfiTevP11Tpx/Jia39baYHXGbLlv0kO7Ga/MmKqGjW5V4cKFdfbsWX248QNt3/Y/zZoz33Sai6DceVSitOs6YIGBQcoXHJK2PV9IqMt+31y5FJY/QsWKl7pGle5r1KSZXpv/qgoXLqIby5TT3j27tHLZYt3VvqPpNLfYfM9L9vRPnzxeH3/4gca/OENBufMo5q9ZT3nz5lVAYKAk6VR8vI4di1LMieOSpMMHD0iS8uePUHiEd82IsuW6A9nBo8Gt2bNn69SpU24f37p1xj+IvvDCC4qKilK7du2UkpKi+vXra/ny5Wn7HQ6HIiMjPcnLNqXLV9JjoybpjSVz9M6qRSpQqIi69R6ges1uTzvm+//7Sq9NfyHtz69OvvhWkLvufUTt7/eut01I0u1t2ir+ZJzmzZmtEyeOq2y58po1d56KFjUzgOgJW9pPxsVq+vgxiouNVp48eVXyxnJ6ZvJMVa998aPr42KitWjWVMXHxSgsPEJNW92pLg95371yiS3XPSM299Nuhi3tJ+Ni9eILoxQbc/F7Teky5fX8i7NUs059JScl6Y9DBzR+9CDFx59UcHCoylesrCkzF6lkae9YrHrPrp16vE/PtD+/PHWyJKltu/YaMvJZHTywXx+897biT8YpJCRUN1WuotkLl+pGL1xsW7LnvkkP7WbExMRo9Mihij5xQnnz5VO5chU0a8581WvQ0HTadW3I8NGaO2uGJk14XnGxsYooUFAd7+miXn37m05zi833vGRP/9tvrJEkDXj0YZftw54ZpzZ3dpAkff3VZ5r0/Ji0fc+PGiJJ6t6rn3r28a77yZbrDmQHhzOz6VX/snXrVo8Gt0JCQlS3bt1Mjzl37pwuXLigvHnzun3eK9m8Ly7bzmVC7dLe94lQ/wX7T5w1nZBlpQvkMZ0AwE1/xiaaTrgqoXn8TCdkWZ4AY0uNwmKpqW7/qOx1fj9u7882JSJym07IMv9cxlZ++c+LO+tdb+/1RFgef9MJWRb4H/3n9aGVP5pOyBFL77vZdEKWeHQb9u/fXx06dMj07Yb/9P777+t///tfpscE/jW9EwAAAAAAAPCUx2tuPfPMM24f/9573vUpKQAAAAAAALi+eDRn1uHhZ116ejwAAAAAAADgCd4QDgAAAAAAAGv9R5d+AwAAAAAAyBof3qjmVTxec+vw4cNuLSjvdDrdXngeAAAAAAAAyAqPBrduv/12DR061O3jW7du7XEQAAAAAAAA4C6PBrciIyNzqgMAAAAAAADwGGtuAQAAAAAAeMDhYNEtb8KnJQIAAAAAAMBaDG4BAAAAAADAWgxuAQAAAAAAwFoerbk1c+ZMHTlyxO3jb7jhBvXv39/jKAAAAAAAAG/FilvexaPBrUWLFmn69OlyOp1uHT9kyBAGtwAAAAAAAJBjPBrccjqduvXWWz06HgAAAAAAAMgpHq255elHXfLRmAAAAAAAAMhJHs3cAgAAAAAA+K/zYTKPV+HTEgEAAAAAAGAtj9fcWrp0qdvHsuYWAAAAAAAAcpJHg1ujR49WdHS028ePHDnS4yAAAAAAAADAXR4NbtWpU0fnzp1z+/igoCCPg7JD9RKhRl4XdisaauZ+BfDfkj+vv+mEq9Jq2lemE7Lsq2FNTSfAQqkWvxGhdME8phOyLOl8qumELPNnVWNjQnL7mU7AfwhLbnkXj771dujQQdWrV7/i2w0dDoecTqd27typ//3vf1cVCAAAAAAAAGTE4zW3Fi1a5PbxderU8TgIAAAAAAAAcJdHn5bo8HDenafHAwAAAAAAAJ7waHALAAAAAAAA8CYsdwgAAAAAAOAB3qnmXTyauXWlheSv9ngAAAAAAADAEx7N3Lr55v9n797jc64bP46/t9mBYdgwcxxDTmETOZNSKpES6S43pZOSQ+RQoWLIKTnkWJRT6KCDzjpKhVLOypk5zBy32fH6/eFndxezXdds+1wfXs/7cT0e977X6bVv312uffb5fq7r1aRJE7duDwAAAAAAAOQVtwa35s+fn1cdAAAAAAAAgNtcPi1x3759bj3wwYMH3Y4BAAAAAADwdF5eV+fFVi4Pbt1www3q1auXfv3118ve5tSpU5o9e7Zq166t9957L1cCAQAAAAAAgMtx+bTErVu3avTo0brtttvk6+urBg0aKCwsTAEBATpx4oS2bNmizZs3q0GDBnr11VfVrl27vOwGAAAAAAAAXJ+5VaJECY0fP16HDh3SjBkzVK1aNcXGxmrnzp2SpAceeEDr16/XTz/9xMAWAAAAAAAA8oVbC8pLUkBAgDp16qROnTrlRQ8AAAAAAIBH87Z5gaqrkMsztwAAAAAAAABPw+AWAAAAAAAArMXgFgAAAAAAAKzl9ppbAAAAAAAA1zKW3PIszNwCAAAAAACAtVyeubVy5UqXH/Suu+7KUQwAAAAAAADgDpcHtzp27OjS7by8vJSWlpbTHo81c/rrmvXGNKdtwcEh+mL1j4aK3Ld08UK99eZcxR47pioRVTVo8FBFRjUwneUSW9vj4+M1c9pr+nb1VzoRF6dq1WtowKChqlm7juk0l9i63y+wuZ92M2xp/339Or2zYJ62b9ms2NhjGjtxilq2vjnj+tVff6kPVryrbVs369TJk1qwZIWqVa9hpLV++SA92Li8rgstopJF/PXssk36bkes020qBRfS0zdVVmSFYvLyknbFJmjIe5t15HSSJMnXx0vPtKmiW2uVln8Bb/2254TGfrZTR88kmfiWLmHLcZMZ2vPf1fCe8oJ5c2Zq6muTdP9/HtLA54aaznHy+/p1WrhgnrZvPf86OWbC/14nU1NSNHP6FK356XsdOnBAhQsXVoNGjfVkn/4qWbKU4fLLs/WYv8DG/rmzZ+qbr77Unt275B8QoLr16uuZfgNUKbyy6TSX2bjfgZxw+bTE9PR0ly7uDmzFx8dr9uzZ6tGjh9q1a6fbb79dPXr00Jw5cxQfH+/2N5SXqlSpqs+/+SHjsnSF67PZTPts1acaNyZavR59QkuXf6DIyCg9+VgvxRw6ZDotWza3jxr5vH5Zu0YjXhmrRcs+VKPGTdX78Z46euSI6bRs2bzfJbv7aTfDpvbExARVrVZdAwY/n+n15xITdX3d+nry6f75XHapgn4+2nEkXq9+vjPT68sWC9Dsh+prz/EEPfbOH3pgzjrN/XGPklPTM27T/5YItapeUsPe36JHFvyugn4+mtSljrw9YK0Lm46bi9Fujs3vKS/YvOkvvbf8XVWtVt10SqbOnfv/18nnLn2dPHfunLZv26IejzyutxYtV/T4Kdq/d48G9e1toNQ1th/ztvZvWPebutzfTQsWLdWMWfOUlpqqJx59RIkJCabTXGLrfgdy4orX3Dp37lyO77tlyxZVq1ZNgwYN0okTJ1ShQgWVK1dOJ06c0MCBA1W9enVt2bLlShNzjU8BH4WElMy4FC9RwnSSy96e/6buvucedbq3sypXqaJBQ4YptEyo3l262HRatmxtP3funFZ//aWe7vusIqNuUPkKFfXoE08pLKycVizz7HbJ3v1+gc39tJthU3uTZi30eO9n1LrNLZle3+7Ou/TwY0/qhhsb53PZpdb8E6c3vtut1dtjM73+yVaVteaf43r9m13aceSsDp48p5/+jtOJhBRJUqC/jzrUK6PXvvpbv+45oR1HzurFD7eqSslANQwvnp/fSqZsOm4uRrs5Nr+nlKSEhHgNG/ysXhj+sooWLWo6J1ONm7bQY72fUatMXicLFymiKTPm6ua27VSxUrhqX19X/Z8bpm1bN+twjGf+0m/7MW9r/7SZc3RXx06qElFV1a+7TiNeidbhmEPasmWz6TSX2LrfbeHl5XVVXmyVo8GttLQ0vfzyyypbtqwKFy6sXbt2SZJeeOEFzZ071+XH6d27t1q0aKEjR47ogw8+0MyZMzVr1ix98MEHOnLkiFq0aKHevT3nLyj79u7VrW2aq/1tbTRkUH8dOLDfdJJLUpKTtXXLZjVu0sxpe+MmTbXxj98NVbnG5va0tDSlpaXJz9/fabt/gL82/r7BUJVrbN7vkt39tJthc7vNvCQ1jSihfXGJmtL1en3et4ne/G+kWlYLybhNjdAi8vXx1trdJzK2xZ5N1j/H4nV9uSAD1f9j83FDu1m2vqe8YMyol9SseSs1atzEdEquOXv2jLy8vFSkiOcN1tl+zNve/29nz56RJAUFmf33xxVX034HXJGjwa1Ro0bprbfe0rhx4+Tn55exvU6dOpozZ47Lj/PLL7/ohRdecHqMC/z8/DR06FD98ssvOUnMdbXr1NVLo8Zo6ow5en7Eyzoee0w9H7xfJ0+eyP7Ohp04eUJpaWkKDg522h4cHKLY2GOGqlxjc3tgYKDqXF9P82bN0LGjR5WWlqZVn6zU5r/+9Ph2m/e7ZHc/7WbY3G6zEoF+CvQvoO6NK+jnXXF6evGf+nZ7rMbdW0uRFc7/4hBc2E/Jqek6cy7V6b5x8ckKDrz0/UN+svm4od0cm99TStLnqz7Rti1b9HRf86c955akpCTNmDJJbW+7Q4GFC5vOuYTtx7zt/Rc4HA5NGDdG9SOjFFG1mumcbF0t+x1wlcsLyv/bggULNGvWLLVp00aPP/54xvbrr79e27Ztc/lxihcvrp07d6pmzZqZXv/333+rePGsTzlISkpSUpLzgrIp8pP/RbNlrlTT5i2cvr7++nrqcEdbfbzyA/3noR65+lx55eIphg6Hw5pph7a2jxw1Vi+PGKY72raUj4+Pql9XU7e2u1Pbt3nO6bZZsXW/X2BzP+1m2Nxuowu79rsdsVr86wFJ0o4jZ3V9uaLqFBmmDftOXf6+khz50OgKm48b2vOfze8pDx+O0atjRmv6rLm5/l7blNSUFL04ZIDSHekaOORF0zlZsvWYv8D2/jGjXtbOHdv15oJFplPcYvt+B1yVo5lbBw8eVERExCXb09PTlZKS4vLj9OrVS927d9f48eO1ceNGHT58WEeOHNHGjRs1fvx49ezZU4899liWjxEdHa2goCCny4Rx0W5/T+4qWKiQIqpW0769e/P8ua5U8WLF5ePjo9hY5/VO4uKOKzg45DL38gw2t0tSufIVNHPu2/ru5/X66LNv9NbCd5WamqKwsLKm07Jk+363uZ92M2xut9nJhBSlpqVrd6zzwry7YxMUWjRAknT8bLL8CnirSIDz3+OKB/opLj4531ozY/NxQ7vnsOk95dbNmxUXd1wPdLlHN9SrpRvq1dL6db9pycK3dUO9WtZ9YnpqSoqGDe6vQwcPasr0uR45a0uy/5i3vV+Sxox+Wd+t/kaz5y1Q6dBQ0zkuuRr2u6fzvkovtspRe61atfTDDz9csn3ZsmWqX7++y48zYsQIDRkyRBMnTlT9+vVVtmxZhYWFqX79+po4caIGDx6sF1/M+i8oQ4YM0alTp5wuAwYNcft7cldycrJ27/pHISVL5vlzXSlfPz/VqFlLa9f85LR97Zo1qlvP9f9eJtjc/m8FCxZSSMlSOn36lNau+UktWrUxnZQl2/e7zf20m2Fzu81S0x3aEnNGFYMLOm2vEFxQMafOf2DN1sNnlJKWrkb/Wjw+uLCfqpQM1J8HLj+zKz/YfNzQ7jlsek/Z8MYb9e57K7V42fsZl5q1aqvdHe21eNn78vHxMZ3osgsDWwf27dWUN+YqqFgx00mXZfsxb3O/w+HQmFEv6ZuvvtTMeW+pbLlyppNcZvN+B3IiR6clDh8+XA8++KAOHjyo9PR0vffee9q+fbsWLFigjz/+2K3Heu655/Tcc89p9+7dOnz4sCQpNDRU4eHhLt3f39//kmnRZ5Ny/0SFSePHqkWr1goNDVNc3HHNnTVD8fFn1f6ujrn+XHnhwe49NGzwINWsXVt169bXimVLFRMTo85duppOy5bN7T+v+VFyOFShUvj5N0+TxqtipXC173C36bRs2bzfJbv7aTfDpvaEhHgd2L8v4+tDBw9qx/atKlo0SKFlwnTq1EkdORyj2KNHJUl79+yRdH6djeCQ/P0FuqCvj8qX+N/gVVixAFUrXVinElN05HSS3l67X6Pvrqnf953Sur0n1bhKCTWvGqLH3/5DkhSflKYP/4hR35ur6FRiik4lpqpvmyr651i8ft1tfo0im46bi9Fuhs3vKQMDC1+y1lDBggUVVKyYx61BlNXrZEjJUho6qK+2b9uq8a9NV3pamo7//xpERYOC5Otrdj2/zNh8zEv29ke/8pJWffqxJk2ZpsDAwIy1qgoXLqKAgADDddmzdb8DOZGjwa327dtr6dKlGj16tLy8vPTiiy8qMjJSH330kW65JfOPJc9OeHj4JQNa+/fv1/DhwzVv3rwcPWZuOnr0iIY+N0AnT5xU8RLFVadOXb31zlKV8fDTyy64rd3tOnXyhGbNmK5jx44qomo1TXtjlsefHifZ3X72zBlNf32Sjh45rKJBQbqpTVs98VRfFfD1NZ2WLZv3u2R3P+1m2NS+dctm9e7134yvX5swVpJ0e/uOevGl0frhu9V6ZfiwjOtfGDxAkvTwY0+q1+NP5WtrjTJFNPPBehlf97/l/LIGH288rJEfb9O322MVvWqH/tukgga0jdC+uEQ9t2KTNv5rVtakL/9RWrpDo++upQBfb/2254RGLt2mdA9YdMum4+ZitJth+3tKW2zbslm9H/1vxtdTJv7vdfKRx3rrh+9WS5Ie6trJ6X7TZr2lyAYN863TVTYf85K9/cuWLpYk9erxkNP2ka+M1l0dO2V2F49i634HcsLL4XB4wFvDzG3cuFGRkZFun7+fFzO38lMBHxb4MyEpJd10Qo75+9p8djRwbUlMtmtNmou1nXTpsgS2+OG5VqYTYKHUNHvfV9q8ZrTN78sK+dtzeubVJt1zf7XNlrfFP7ABOZoyY78+H7j+YXo2mdLxOtMJOXJFh+G6deu0detWeXl5qUaNGoqKinLr/itXrszy+l27dl1JHgAAAAAAAK5yORrcOnDggO6//3799NNPKvb/iy+ePHlSTZo00eLFi1W+fHmXHqdjx47y8vJSVpPH+JhSAAAAAAAAXE6OzmXq2bOnUlJStHXrVsXFxSkuLk5bt26Vw+HQww8/7PLjlClTRitWrFB6enqmlw0bNuQkDwAAAAAAANeIHM3c+uGHH7RmzRpVr149Y1v16tX1+uuvq2nTpi4/TlRUlDZs2KCOHTtmen12s7oAAAAAAADymzcnmXmUHA1uVahQQSkpKZdsT01NVdmyrn/ywsCBAxUfH3/Z6yMiIrR69eqcJAIAAAAAAOAakKPTEseNG6enn35a69aty5hZtW7dOj3zzDMaP368y4/TvHlz3XbbbZe9PjAwUC1btsxJIgAAAAAAAK4BLs/cKl68uNPi7vHx8WrUqJEKFDj/EKmpqSpQoIB69ux52dMMAQAAAAAAgNzk8uDW5MmT8zADAAAAAADADqy55VlcHtzq3r17XnYAAAAAAAAAbsvRgvL/lpiYeMni8kWLFr3ShwUAAAAAAACylaMF5ePj4/XUU0+pVKlSKly4sIoXL+50AQAAAAAAAPJDjga3Bg0apG+++UbTp0+Xv7+/5syZo5EjRyosLEwLFizI7UYAAAAAAAAgUzk6LfGjjz7SggUL1KpVK/Xs2VPNmzdXRESEKlasqIULF+qBBx7I7U4AAAAAAACP4OXFivKeJEczt+Li4hQeHi7p/PpacXFxkqRmzZrp+++/z706AAAAAAAAIAs5GtyqXLmy9uzZI0mqWbOm3n33XUnnZ3QVK1Yst9oAAAAAAACALOVocKtHjx7auHGjJGnIkCEZa2/169dPAwcOzNVAAAAAAAAA4HJytOZWv379Mv5/69attW3bNq1bt05VqlRR3bp1cy0OAAAAAADA03iz5JZHydHMrYtVqFBBnTp1UokSJdSzZ8/ceEgAAAAAAAAgW7kyuHVBXFyc5s+fn5sPCQAAAAAAAFxWrg5uAQAAAAAAAPkpR2tueTrOfUVO+BWwd6w33eEwnZBj3l78wOLa4u9r72uNJP3wXCvTCTlW67lVphNybPPYdqYTrlmJKWmmE3KssL+9b/UL+fuYToCFeF+J/MTh5lnsfocNAAAAAACAa5pbf87p1KlTltefPHnySloAAAAAAAAAt7g1uBUUFJTt9Q899NAVBQEAAAAAAACucmtw680338yrDgAAAAAAACuwxptnYc0tAAAAAAAAWIvBLQAAAAAAAFiLwS0AAAAAAABYy601twAAAAAAAK51zBTyLPz3AAAAAAAAgLVyPLj19ttvq2nTpgoLC9PevXslSZMnT9aHH36Ya3EAAAAAAABAVnI0uDVjxgz1799ft99+u06ePKm0tDRJUrFixTR58uTc7AMAAAAAAAAuK0eDW6+//rpmz56tYcOGycfHJ2N7gwYN9Ndff+VaHAAAAAAAAJCVHC0ov3v3btWvX/+S7f7+/oqPj7/iKAAAAAAAAE/l5WW6AP+Wo5lb4eHh+uOPPy7ZvmrVKtWsWfNKmwAAAAAAAACX5Gjm1sCBA9W7d2+dO3dODodDv/76qxYvXqzo6GjNmTMntxsBAAAAAACATOVocKtHjx5KTU3VoEGDlJCQoG7duqls2bJ67bXX1LVr19xuBAAAAAAAADKVo8EtSerVq5d69eql2NhYpaenq1SpUrnZBQAAAAAA4JG8WXTLo+R4cOuCkJCQ3OgAAAAAAAAA3Jajwa3w8HB5ZTFKuWvXrhwHAQAAAAAAAK7K0eBW3759nb5OSUnR77//rs8++0wDBw7MjS4AAAAAAAAgWzka3HrmmWcy3T5t2jStW7fuioI81btLF2v50sU6dOigJKlylQg9+nhvNWvewnCZ65YuXqi33pyr2GPHVCWiqgYNHqrIqAams1xia/v6db9p/ptztXXLJh07dkwTX5umm9rcbDorW3Nnz9Q3X32pPbt3yT8gQHXr1dcz/QaoUnhl02lusfW4kWg3xdb2q+Fn1hP3/Q2Vi6tXq8qqXa6oSgcF6PE31+vLTUczrv9nQrtM7zfmo22a/e3ujK/rVyymAe2qqW6FIKWmO7Tl4Gn1nL1OSanpef49ZMcT97urbGn/Y8M6LVowT9u3btHx2GMaPX6KWrRuk3H9qOFDterjD53uU7P29Zo1f3F+p7rE1vc2F9hy3GTG5nbJ7n7akRmW3PIs3rn5YO3atdOKFSty7fGOHDmil156Kdce70qULl1aT/cdoIVLlmvhkuVq2OhG9evTW//8vdN0mks+W/Wpxo2JVq9Hn9DS5R8oMjJKTz7WSzGHDplOy5bN7YmJCapWvboGD33RdIpbNqz7TV3u76YFi5Zqxqx5SktN1ROPPqLEhATTaS6z+bih3Qyb223/mfXUfV/Iz0fbDp3WiPe3ZHp9oxFfO10GLflT6ekOffbn4Yzb1K9YTG/2aqAfdsSq02s/6+7Ja/T2T/vkcOTXd3F5nrrfXWFTe2JioiKqVVf/54Zd9jaNmjTTh59/m3EZP2VGPha6x9b3NpJdx83FbG6X7O6nHbCDl8ORe2+vxo0bp+nTp2vPnj258ngbN25UZGSk0tLS3LpfQnL+vGNs2bSR+g4YqLs73Zurj+vtnftDwA907awaNWvq+RdHZmzr2L6dWt90s57pNyDXny835Vd7Xv+iUa929Tz766ZDeRsfFxenNi2aaM5bbyuqwQ25+th59SkjHPNm0J699HwY1bDtZza/9n2t51bl+L7/TGh3ycyti73RI1KB/j568I3fMrYt79NYP+2I1aTPruyPYZvHZj5L7Erw8+qaM+dSc+2xmkXVynTm1tkzZxQ98fVce54LCvtf8WdHZSkv39vkxdsDjnlzbO6nPXsBeftS47Fe/NyOiS7ueunWqqYTciRHh2H9+vWdFpR3OBw6fPiwjh07punTp7v8OH/++WeW12/fvj0neXkuLS1NX37xmRITE3R93Xqmc7KVkpysrVs2q+cjjzptb9ykqTb+8buhKtfY3H41OXv2jCQpKCjIcIlrbD5uaDfD5vbM2PQze7Xs++DCfmpVo6QGLv7TaVv9isW0csMhLXv6RlUILqR/jsZrwqodWr/7hMFau/e7ze2X8/v633Tnzc1VuEgR1Y9soEd7P6PiJYJNZ11VbD5ubG6X7O6nHbBHjga3Onbs6PS1t7e3SpYsqVatWum6665z+XHq1asnLy8vZTZ57ML2rD6VUZKSkpKUlJTktC3Ny0/+/v4ud7hq547t6v6f+5WcnKSChQppwuSpqlIlItefJ7edOHlCaWlpCg52fpMUHByi2NhjhqpcY3P71cLhcGjCuDGqHxmliKrVTOe4xObjhnYzbG6/mG0/s1fLvr/nhrKKT0rV538dydhWvkQhSVKfthGK/mibth46o7ujwvT24w11+6s/aE+sudNGbd7vNrdn5samzdX65lsVWiZMhw4d0JwZr6vP4z01951l8vPzM5131bD5uLG5XbK7n3ZkJQ9OuMIVcHtwKzU1VZUqVdKtt96q0NDQK3ry4OBgjR07Vm3atMn0+s2bN6t9+/ZZPkZ0dLRGjhzptG3o8y9q2AsjrqgtM5XCw7Vk+fs6c+a0vv7yC734/GDNefNtKwa4JF0yUOjK4KGnsLnddmNGvaydO7brzQWLTKe4zebjhnYzbG6/wNafWdv3/b0Ny2nlhkNK/tci8d7/v7Lp4p/3a8Vv5z+QZsvB02pSNVj3Niyn8Z/uMJHqxOb9bnP7v7Vp+79TTitHVNV1NWrr3jtv1s8/fqeWN91isOzqZPNxY3O7ZHc/7YDnc3twq0CBAnriiSe0devWK37yqKgoHTp0SBUrVsz0+pMnT2Y6q+vfhgwZov79+zttS/PKm79y+fr6qUKF8621atXR5k2btPidBXp+uGcsen85xYsVl4+Pj2JjY522x8UdV3BwiKEq19jcfjUYM/plfbf6G82d/45KX+Fgdn6y+bih3Qyb2//Nxp/Zq2HfNwgvriqlCqvPgj+cth89fX5m+d9Hzjpt/+dovMKKF8yvvEzZvN9tbndFSMmSCi0Tpv379ppOuarYfNzY3C7Z3U87YI8cfVpio0aN9PvvV36e7mOPPaZKlSpd9voKFSrozTffzPIx/P39VbRoUadLXpySmDmHkpOT8+m5cs7Xz081atbS2jU/OW1fu2aN6tarb6jKNTa328zhcGjMqJf0zVdfaua8t1S2XDnTSW6x+bih3Qyb2yW7f2Zt3/eSdF+jcvpr/yltiznjtP1AXKIOnzqnyqUCnbZXKhmog3GJ+Zl4CZv3u83trjh18qSOHjms4JCSplOuKjYfNza3S3b30w7YI0drbj355JMaMGCADhw4oKioKAUGOr9pu/766116nLvvvjvL64sXL67u3bvnJDHXvf7aRDVt1kKhoaGKj4/X5599qnW//appM2abTnPJg917aNjgQapZu7bq1q2vFcuWKiYmRp27dDWdli2b2xMS4rVv376Mrw8ePKBt27YqKChIZcqEGSzLWvQrL2nVpx9r0pRpCgwMzDgvv3DhIgoICDBc5xqbjxvazbC53fafWU/d94X8fFQxpFDG1+VKFFKNsCI6mZCimJPnJJ3/NLp214dq9EfbMn2M2at3q++tEdp66Iy2HjytTjeUVZVSgXpqvvnFfD11v7vCpvaEhHgd3P+/9wIxhw5o5/atKlI0SEWDgjRv5nS1anOLgkNKKubQQc2a9pqCihVXy9a5/wmEucHW9zaSXcfNxWxul+zupx2Xk1ef+o6ccWtwq2fPnpo8ebK6dOkiSerTp0/Gdf9eAD4tLS1X4vbv36/hw4dr3rx5ufJ4V+L48eN6fuggxR47psJFiqhq1eqaNmO2bmzS1HSaS25rd7tOnTyhWTOm69ixo4qoWk3T3pilsLCyptOyZXP75k2b1KvnQxlfTxgXLUlq3+FuvTxqjKmsbC1buliS1KvHQ07bR74yWnd17GQiyW02Hze0m2Fzu+0/s5667+uUD9KiJxtlfP18hxqSpBW/HdCgJX9Jku6sX0ZeXl766PeYTB/jrR/2yN/XW893uE5BBX21LeaMHpr5m/YdN7eY/AWeut9dYVP7ti2b1eexHhlfvz5xnCSp3Z0d9OyQF7Xr7x367JOVOnvmtIJDSiqyQUONjB6vQhf98dhT2PreRrLruLmYze2S3f20A3bwcmS3qNW/+Pj4KCYmRomJWU+lv9waWu7auHGjIiMj3R4sS0h2+VvySN587IIRrv8keB6H7I3nLx641qTb/GIju39maz23ynRCjm0e2y77GyFPnDmXajohxwr75+gkDY9g8UsNcM0JsPel5oq89OXfphPyxIu32PGBeRdz6zC8MA6WW4NXK1euzPL6Xbt25crzAAAAAAAA4Ork9hhrbn5saMeOHTNOZ8yP5wMAAAAAAMDVxe3BrWrVqmU74BQXF+fSY5UpU0bTpk1Tx44dM73+jz/+UFRUlLuJAAAAAAAAeYZ5OJ7F7cGtkSNHKigoKFeePCoqShs2bLjs4FZ2s7oAAAAAAABwbXN7cKtr164qVapUrjz5wIEDFR8ff9nrIyIitHr16lx5LgAAAAAAAFx93Brcyu31r5o3b57l9YGBgWrZsmWuPicAAAAAAACuHjn6tEQAAAAAAIBrlTdrbnkUtwa30tPT86oDAAAAAAAAcJu36QAAAAAAAAAgpxjcAgAAAAAAgLXc/rREAAAAAACAa5mXWHTLkzBzCwAAAAAAANZicAsAAAAAAADWYnALAAAAAAAA1mLNLQAAAAAAADd4s+SWR2HmFgAAAAAAAKzF4BYAAAAAAACsxeAWAAAAAAAArMXgFgAAAAAAAKzFgvIAAAAAAABuYEF5z3JVDm55c5QhB7wsPmy8ZHE8kAMOh+mCnPO2+cVGUkpauumEHNs8tp3phByr8ewnphNybPO4200nXJEiAVfl22WPZ/PrvOUv8zqXkmY6IccCfH1MJwAwhNMSAQAAAAAA4Jbvv/9e7du3V1hYmLy8vPTBBx84Xe9wODRixAiFhYWpYMGCatWqlTZv3ux0m6SkJD399NMKCQlRYGCg7rrrLh04cMDtFga3AAAAAAAA4Jb4+HjVrVtXU6dOzfT6cePGaeLEiZo6dap+++03hYaG6pZbbtGZM2cybtO3b1+9//77WrJkiX788UedPXtWd955p9LS3JtFyjxrAAAAAAAAN3jZfg5yLmjXrp3atct82QeHw6HJkydr2LBh6tSpkyRp/vz5Kl26tBYtWqTHHntMp06d0ty5c/X222/r5ptvliS98847Kl++vL766ivdeuutLrcwcwsAAAAAAABKSkrS6dOnnS5JSUluP87u3bt1+PBhtW3bNmObv7+/WrZsqTVr1kiS1q9fr5SUFKfbhIWFqXbt2hm3cRWDWwAAAAAAAFB0dLSCgoKcLtHR0W4/zuHDhyVJpUuXdtpeunTpjOsOHz4sPz8/FS9e/LK3cRWnJQIAAAAAAEBDhgxR//79nbb5+/vn+PEuPn3T4XBke0qnK7e5GINbAAAAAAAAbvC+Spfc8vf3v6LBrAtCQ0MlnZ+dVaZMmYztR48ezZjNFRoaquTkZJ04ccJp9tbRo0fVpEkTt56P0xIBAAAAAACQa8LDwxUaGqovv/wyY1tycrK+++67jIGrqKgo+fr6Ot0mJiZGmzZtcntwi5lbAAAAAAAAcMvZs2f1999/Z3y9e/du/fHHHypRooQqVKigvn37avTo0apataqqVq2q0aNHq1ChQurWrZskKSgoSA8//LAGDBig4OBglShRQs8++6zq1KmT8emJrmJwCwAAAAAAAG5Zt26dWrdunfH1hbW6unfvrrfeekuDBg1SYmKinnzySZ04cUKNGjXSF198oSJFimTcZ9KkSSpQoIDuu+8+JSYmqk2bNnrrrbfk4+PjVouXw+Fw5M635TnOpZouAADkJZv/5XJzbUyPk5KWbjohx3x97F2Nocazn5hOyLHN4243nXBFvK/WRVU8HK/z5pxLSTOdkGMBvu79MozcEXCNTpmZ+P0u0wl5on+LyqYTcsTed3kAAAAAAAC45jG4BQAAAAAAAGsxuAUAAAAAAABrXaNnxwIAAAAAAOSMt+0L7F1lmLkFAAAAAAAAazG4BQAAAAAAAGsxuAUAAAAAAABrMbgFAAAAAAAAazG45YalixeqXdubdEP9OurauZM2rF9nOsktNvfTbobN7ZLd/bTnv/XrflOf3o/rltbNVK92dX3z9Vemk9xi636XpKNHjuiFIYPUpvmNatqwvrp1vltbt2w2neUyT9z3DSuX0JxHGmjtyDbaPfkO3VKntNP1uyffkenl0daVM24TUsRfEx+oq19faqPNY2/VRwOaqV3d0Pz+VjL17tLFuq/TXWp2Y5Sa3Rilhx7ooh9/+N50lls88bhxla3tvM7nj9/Xr9OAPk/qjltaqlG9mvruG+f97HA4NHvGVN1xS0u1aFRfTzzcXbv+3mmo1jW27PvM2Nzu6by9rs6LrTxicOvAgQM6e/bsJdtTUlL0/fee8Ubls1WfatyYaPV69AktXf6BIiOj9ORjvRRz6JDpNJfY3E+7GTa3S3b3025GYmKCqlWvrsFDXzSd4jab9/vp06f0cPduKlCggF6bPkvL3v9YfQcMUpEiRUynucRT931Bfx9tPXRaw1dkPkh4wwtfOV0GLtqo9HSHVv0Zk3GbSf+pq8qlCqvXnHW6bdz3+vzPw3q9e6Rqli2aX9/GZZUuXVpP9x2ghUuWa+GS5WrY6Eb169Nb/3j4L8gXeOpx4wqb23mdzx+JiQmqWq26nh38fKbXv/3WXC16Z76eHfy83lz4rkqEhOjpJx5RfHx8Ppe6xqZ9fzGb2wF3GR3ciomJUcOGDVWxYkUVK1ZM3bt3dxrkiouLU+vWrQ0W/s/b89/U3ffco073dlblKlU0aMgwhZYJ1btLF5tOc4nN/bSbYXO7ZHc/7WY0a95ST/Xppza3tDWd4jab9/v8eXNUunQZDX95tGrXuV5hZcuq4Y2NVa58BdNpLvHUff/d1mOa8OkOff7n4Uyvjz2T5HS5pU5p/fz3ce0/nphxm/qVimv+D3u0cd8p7T+eqKlf/q3TiSmqXS4ov76Ny2rZ6iY1b9FSFSuFq2KlcD3Vp58KFSqkP//caDrNJZ563LjC5nZe5/NHk2Yt9PhTz6h1m1suuc7hcGjJwgXq8chjat3mFlWJqKrhL0frXOI5fb7qYwO12bNp31/M5nbAXUYHtwYPHiwfHx/98ssv+uyzz7Rlyxa1atVKJ06cyLiNw+EwWHheSnKytm7ZrMZNmjltb9ykqTb+8buhKtfZ3E+7GTa3S3b30w532b7fv/92tWrUqqXnBvTVLS2bqtt9nfT+8ndNZ7nE9n1/QUhhP7WuWUrvrt3vtH3drjjdUb+Mggr5ystLurN+GfkV8Nbav48bKs1cWlqaPlv1iRITE3R93Xqmc7Jl83Fjc7vNrqb9fujgAR2PjVWjxk0ytvn5+al+gwb6648/zIVdhs373uZ2ICcKmHzyr776Su+//74aNGggSWrevLm6dOmim266SV9//bUkycsr65M+k5KSlJSU5LTN4eMvf3//XOs8cfKE0tLSFBwc7LQ9ODhEsbHHcu158orN/bSbYXO7ZHc/7XCX7fv94IH9WvHuEj3w4H/V45FHtXnTXxo/drR8/fx0510dTedlyfZ9f8E9Dcsp/lyqPrtoltfT83/X693r64/RbZWSlq7E5DQ9Pne99h1PMFTqbOeO7er+n/uVnJykgoUKacLkqapSJcJ0VrZsPm5sbrfZ1bTfj8fGSpJKlAhx2l6iRIgOx3jeqXI273ub222RzVAF8pnRmVunTp1S8eLFM7729/fX8uXLValSJbVu3VpHjx7N9jGio6MVFBTkdHl1bHSe9F480OZwOLIdfPMkNvfTbobN7ZLd/bTDXbbu9/R0h66rUVO9n+mn62rU1D2du6jjPZ214t0lptNcZuu+v6Bzo/L6cP0hJaemO20fcHt1BRXy1QPT1qrDhB8199vdmtYjUtXLeMZ6aJXCw7Vk+fuav3CJOt/XVS8+P1j//PO36SyX2Xzc2Nxus6tpv1/S7eHfi8373uZ2wB1GB7cqV66sP//802lbgQIFtGzZMlWuXFl33nlnto8xZMgQnTp1yuky8LkhudpZvFhx+fj4KPb//9JwQVzccQUHh1zmXp7D5n7azbC5XbK7n3a4y/b9HlIyROGVqzhtCw+vrMOHYy5zD89h+76XpBsqF1eV0oW1dO0+p+0Vggupe4tKGrT4T63ZeVxbD53RlM936s99p/Rgs4qGap35+vqpQoWKqlWrjvr0HaBq1a7T4ncWmM7Kls3Hjc3tNrua9ntwyPne48edZw7FnTiuEiWCM7uLUTbve5vbgZwwOrjVrl07zZo165LtFwa46tWrl+2aW/7+/ipatKjTJTdPSZQkXz8/1ahZS2vX/OS0fe2aNapbr36uPldesLmfdjNsbpfs7qcd7rJ9v9etF6m9e/Y4bdu7d4/KlAkzE+QG2/e9JN13Y3n9ue+kth4647S9oJ+PJCn9ordh6Q6HvD32L/4OJScnm47Ils3Hjc3tNrua9ntY2XIKDgnRrz//nLEtJSVZv69bpzr16pkLuwyb973N7UBOGF1za9SoUUpIyHzdhgIFCui9997TgQMH8rkqcw9276FhgwepZu3aqlu3vlYsW6qYmBh17tLVdJpLbO6n3Qyb2yW7+2k3IyEhXvv2/W/2ysGDB7Rt21YFBQV5/ECLzfu924Pd1fOhbpo3e6ZuufU2bf7rL72/fJmGDR9pOs0lnrrvC/n5qGLJwIyvy5copBpli+pUfLIOnTwnSSrsX0C31y2jUR9uveT+/xw5q93H4jX6vtoa/eFWnYhPUds6pdWsWogenv1bvn0fl/P6axPVtFkLhYaGKj4+Xp9/9qnW/farps2YbTrNJZ563LjC5nZe5/NHQkK8DvxrPx86eFA7tm1V0aAghZYJU9cHHtJbc2epfMWKKl+hot6aM0sBBQN0a7vsz9oxwaZ9fzGb223gLU/9Y8+1yejgVoECBVS0aNHLXn/o0CGNHDlS8+bNy8eqzN3W7nadOnlCs2ZM17FjRxVRtZqmvTFLYWFlTae5xOZ+2s2wuV2yu592MzZv2qRePR/K+HrCuPPrN7bvcLdeHjXGVJZLbN7vtWrX0fhJUzT1tUmaM3O6wsqW04BBg9Xujvam01ziqfu+ToUgLXmqccbXL9xdU5K0/Nf9Grjo/JIQ7SPLyMvLSx9tuHQR59R0h3rO/FWD2l+nOb1uUCE/H+2NTdCzizbq263mFyI+fvy4nh86SLHHjqlwkSKqWrW6ps2YrRubNDWd5hJPPW5cYXM7r/P5Y+vmzXqy138zvp48Yawk6Y72HfXiy6P14H8fVtK5cxo3+iWdOX1atepcrykz5igwMPAyj2iWTfv+Yja3A+7ycmR33p9BGzduVGRkpNLS0ty637nUPAoCAHgEz/2XK3see0aXi1LS0rO/kYfy9TG6GsMVqfHsJ6YTcmzzuNtNJ1wRb2/Lf2gtxeu8OedS3Pvdy5ME+PqYTrgmBRidMmPOtJ/2mE7IE72bVjKdkCNGD8OVK1dmef2uXbvyqQQAAAAAAAA2Mjq41bFjR3l5eWW5aDwfUwoAAAAAADwJQxWexej8/DJlymjFihVKT0/P9LJhwwaTeQAAAAAAAPBwRge3oqKishzAym5WFwAAAAAAAK5tRk9LHDhwoOLj4y97fUREhFavXp2PRQAAAAAAALCJ0cGt5s2bZ3l9YGCgWrZsmU81AAAAAAAA2ePDdD2LvZ+JDQAAAAAAgGseg1sAAAAAAACwFoNbAAAAAAAAsBaDWwAAAAAAALCW0QXlAQAAAAAAbOPtxYrynoSZWwAAAAAAALAWg1sAAAAAAACwFoNbAAAAAAAAsBZrbgEAAAAAALiBJbc8CzO3AAAAAAAAYC0GtwAAAAAAAGAtBrcAAAAAAABgLdbcAgAAAAAAcIM3i255FGZuAQAAAAAAwFrM3AIAWCc5Nd10Qo75+9r9dyVfH7v7bbVxTDvTCTlW7uHFphOuyKE3u5lOuCYxIcIc/wI+phMAwG28QwUAAAAAAIC1mLkFAAAAAADgBmaYehZmbgEAAAAAAMBaDG4BAAAAAADAWgxuAQAAAAAAwFqsuQUAAAAAAOAGZgp5Fv57AAAAAAAAwFoMbgEAAAAAAMBaDG4BAAAAAADAWgxuAQAAAAAAwFosKA8AAAAAAOAGLy8v0wn4F2ZuAQAAAAAAwFoMbgEAAAAAAMBaDG4BAAAAAADAWqy5BQAAAAAA4AZW3PIszNwCAAAAAACAtRjcAgAAAAAAgLUY3AIAAAAAAIC1WHMLAAAAAADADd5erLrlSZi55YalixeqXdubdEP9OurauZM2rF9nOsktNvfTbobN7ZLd/bTnv/j4eE0cN1p3tbtJzRvV08MP3a8tm/4yneUyW/f7BTb329iempqqGVMnq0O7m9WsYT11uP0WzX5jmtLT002nqXH1klrUv6U2T+mouLe76faock7XlywaoKmP3qjNUzrqwJz7tGxgK1UuXeSSx7khIkQfDLlJ++fcp91v3KuVQ9sowNcnv76NbNl43FxAuxm2tq9f95v69H5ct7Rupnq1q+ubr78yneQ2W/e9ZHc74A7jg1vHjx/X6tWrFRcXJ0mKjY3V2LFj9dJLL2nr1q2G6/7ns1WfatyYaPV69AktXf6BIiOj9ORjvRRz6JDpNJfY3E+7GTa3S3b3027GqJHP65e1azTilbFatOxDNWrcVL0f76mjR46YTsuWzftdsrvf1vYFb87RimVLNXDI83r3/U/Up9+zemf+PC1d/I7pNAX6F9CmfSf03ILMfwF7p28LVSpZWP+Z9L1aPb9K+2Pj9f7gm1TI/38DVzdEhGjZwFZa/ddh3TL8c7UZ/rnmfLlD6Q5Hfn0bWbL1uJFoN8Xm9sTEBFWrXl2Dh75oOiVHbN73NrcD7vJyOMz9K//rr7+qbdu2On36tIoVK6Yvv/xSnTt3VoECBeRwOHTw4EH9+OOPioyMdOtxz6XmfusDXTurRs2aev7FkRnbOrZvp9Y33axn+g3I/SfMZTb3026Gze2S3f20Zy8pJXdnl5w7d06tmzbQq5OmqlmLVhnbH7jvbjVr0VJPPNU3157L3zf3/65k8zEj2d2fX+3Jqbl7zPd76nGVCA7WCyNHZWwb1L+PAgIC9NLocbn6XJV6LcnxfePe7qb/TP5en64/IEmqElpEv73aXk0Gf6JtB09JOn9ayI5pnTRy6R96+7t/JElfDG+rbzcd1ugVf15x/6E3u13xY1yMY94M2l2Tl78d1qtdXRNfm6ab2tycJ4+fF2eJcdxkL+AaXezonf//t+lq85+LZkzbwujMrWHDhqlz5846deqUhg4dqo4dO6pNmzbasWOHdu7cqW7duunll182mShJSklO1tYtm9W4STOn7Y2bNNXGP343VOU6m/tpN8PmdsnuftrNSEtLU1pamvz8/Z22+wf4a+PvGwxVucbm/S7Z3W9ze936Ufrt17Xau2e3JGnH9m3a+PsGNW3e0nBZ1vwKnH/rei4lLWNbusOh5LR0NapeUpIUUtRfDSJCdOz0OX324i3aNvVufTSsjRpVK2mk+WI2Hze0m2Fzu+1s3vc2t9vC6yq92Mro4Nb69evVv39/FSlSRM8884wOHTqkXr16ZVzfu3dv/fbbbwYLzztx8oTS0tIUHBzstD04OESxsccMVbnO5n7azbC5XbK7n3YzAgMDVef6epo3a4aOHT2qtLQ0rfpkpTb/9afHt9u83yW7+21u797zEbW97Q517niHboyqo/906aSu/3lIt7a7w3RalnbGnNa+Y2f14n11FVTIV74+3nrmzpoKLVZQoUEFJUmVShaWJD13dx0tWP2POr/6rf7cc0IfDL4p07W58pvNxw3tZtjcbjub973N7UBOGJ1AmJycrIIFz78R8fX1VaFChRQSEpJxfXBwsI4fP57lYyQlJSkpKclpm8PHX/4X/fU9N3hdNM/V4XBcss2T2dxPuxk2t0t299Oe/0aOGquXRwzTHW1bysfHR9Wvq6lb292p7du2mE5zia37/QKb+21s//KzT7Xqk4/0SvSrqhxRVTu2bdXEV6NVsmQp3XlXR9N5l5Wa5lD3KT9oyiM3avfMzkpNS9d3mw/ry43/Wz/G2/v8vn9r9d9a9MMuSdJfe0+oRc3SeqBlZb387kYj7Rez8bi5gHYzbG63nc373uZ2wB1GB7fKly+vXbt2qVKlSpKkJUuWqEyZMhnXx8TEOA12ZSY6OlojR4502jbsheF6/sURudZZvFhx+fj4KDY21ml7XNxxBQdn3ecJbO6n3Qyb2yW7+2k3p1z5Cpo5920lJiYo/uxZhZQspaGD+iksrKzptCzZvt9t7re5/bVJ48/P3vr/mVoRVaspJuaQ3po7y6MHtyRp454Tavn8KhUp6Cu/At46fiZJX45oq993n/9wosMnEyVJ2/9/Ta4Ldhw6rXLBgfneezGbjxvazbC53XY273ub24GcMHpaYteuXXX06NGMr++4446MmVyStHLlSjVs2DDLxxgyZIhOnTrldBn43JBc7fT181ONmrW0ds1PTtvXrlmjuvXq5+pz5QWb+2k3w+Z2ye5+2s0rWLCQQkqW0unTp7R2zU9q0aqN6aQs2b7fbe63uT3pXKK8vZ3fBnr7+MiRnrsL1+elM4kpOn4mSZVLF1G98BJa9f8L++47Fq9DcQmqWqao0+2rhBbR/th4E6lObD5uaDfD5nbb2bzvbW4HcsLozK3hw4dnef2wYcPk4+OT5W38/S89BTEvPi3xwe49NGzwINWsXVt169bXimVLFRMTo85duub+k+UBm/tpN8PmdsnuftrN+HnNj5LDoQqVwnVg315NmTReFSuFq32Hu02nZcvm/S7Z3W9re7OWrfXm7JkKDS2jylWqavu2LVr09lu6q0Mn02kK9C+g8NKFM76uWDJQtSsU04n4ZB08nqAODcsr9nSSDhyPV83yxRT9nyh9uv6AVm86nHGfqZ9u1eBOdbRp3wn9tfeE7m9eWVXDiuq/r/9o4lu6hK3HjUS7KTa3JyTEa9++fRlfHzx4QNu2bVVQUJDKlAkzWOYam/e9ze024OxOz+LRH9p5/PhxDR8+XPPmzTOdotva3a5TJ09o1ozpOnbsqCKqVtO0N2Z5/OkqF9jcT7sZNrdLdvfTbsbZM2c0/fVJOnrksIoGBemmNm31xFN9VcDX13Ratmze75Ld/ba2Dxz8vN6Y9prGjn5JJ+LiFFKylDrde58eeexJ02mqF15CHw27OePrUQ9ESZIW/bBLT81aq9LFCuqVbpEqGRSgIyfPaemPu/XqB5ucHuONz7fL39dHox6IVLHC/tq874Q6jV2tPUfP5uv3cjm2HjcS7abY3L550yb16vlQxtcTxkVLktp3uFsvjxpjKstlNu97m9sBd3k5HA6H6YjL2bhxoyIjI5WWlpb9jf8lL2ZuAQA8R1KKPadOXczf1+iKALBUcqq9x3ylXktMJ1yRQ292M50A5CvP/e0we8ykMSPAo6fM5J1FGw6YTsgT3SLLmU7IEaOH4cqVK7O8fteuXflUAgAAAAAAABsZHdzq2LGjvLy8lNXkMT6mFAAAAAAAeBLGKjyL0XMjypQpoxUrVig9PT3Ty4YNG0zmAQAAAAAAwMMZHdyKiorKcgAru1ldAAAAAAAAuLYZPS1x4MCBio+Pv+z1ERERWr16dT4WAQAAAAAAwCZGB7eaN2+e5fWBgYFq2bJlPtUAAAAAAABkj8+/9iz89wAAAAAAAIC1GNwCAAAAAACAtRjcAgAAAAAAgLWMrrkFAAAAAABgGy8vL9MJ+BdmbgEAAAAAAMBaDG4BAAAAAADAWgxuAQAAAAAAwFqsuQUAAAAAAOAGVtzyLMzcAgAAAAAAgLUY3AIAAAAAAIC1GNwCAAAAAACAtRjcAgAAAAAAgLVYUB4AAAAAAMANXl4sKe9JmLkFAAAAAAAAa12VM7dS0tJNJ1wRXx/GHE04fOqc6YQcCw0KMJ0A5Ct/X3tfJ5NT7f43qoC3vX+lTElzmE7IMZuP+UNvdjOdcEXqPf+56YQcW/9SW9MJOeZj8WuN7WyejJKWbu/rPMc8cGXsfacEAAAAAACAa95VOXMLAAAAAAAgrzBTyLPw3wMAAAAAAADWYnALAAAAAAAA1mJwCwAAAAAAANZizS0AAAAAAAA3eNn80aJXIWZuAQAAAAAAwFoMbgEAAAAAAMBaDG4BAAAAAADAWqy5BQAAAAAA4AZW3PIszNwCAAAAAACAtRjcAgAAAAAAgLUY3AIAAAAAAIC1WHMLAAAAAADADV4suuVRmLkFAAAAAAAAazG4BQAAAAAAAGsxuAUAAAAAAABrMbgFAAAAAAAAa7GgPAAAAAAAgBu8xYrynoSZWwAAAAAAALAWg1suOnrkiF4YMkhtmt+opg3rq1vnu7V1y2bTWW5Zunih2rW9STfUr6OunTtpw/p1ppNcZkP7kgVz9XTPbup4c2Pdd3srjXiur/bv3eN0m8SEBE2dMFoPdLhF7Vs11CP3d9RH771rJtgFNuz3rNjcT7sZtranpqZqxtTJ6tDuZjVrWE8dbr9Fs9+YpvT0dNNp2Xp36WLd1+kuNbsxSs1ujNJDD3TRjz98bzrLZfHx8Zo4brTuaneTmjeqp4cful9bNv1lOstlth7zkme2Nwgvrhnd6+v7oS21bcytalOzlNP128bcmumlZ4tKGbe5r2E5LXj0Bq0b0UbbxtyqIgGee6LFvDkzFVnnOr06drTpFJd54nHjKpvbJfv7JY55wJN55OBW5cqVtXPnTtMZGU6fPqWHu3dTgQIF9Nr0WVr2/sfqO2CQihQpYjrNZZ+t+lTjxkSr16NPaOnyDxQZGaUnH+ulmEOHTKdly5b2P39fp/b3dNHkWW8r+rWZSktL1dC+j+tcYkLGbd547VWtW7tGg4aP1uzF76tTl/9o+qQxWvP9aoPlmbNlv1+Ozf20m2Fz+4I352jFsqUaOOR5vfv+J+rT71m9M3+eli5+x3RatkqXLq2n+w7QwiXLtXDJcjVsdKP69emtf/72nPcBWRk18nn9snaNRrwyVouWfahGjZuq9+M9dfTIEdNp2bL5mPfU9oK+PtoWc0Yvf7g10+ubvbLa6TJ02V9KT3foi03/O14CfH30w/ZYzVy9K7+yc2Tzpr/03vJ3VbVaddMpLvPU48YVNrdL9vdLHPOAp/NyOBwOU08+ZcqUTLf3799fgwYNUmhoqCSpT58+bj3umaTc/Uv165MnaOPvv2vO/Pz5JcHXJ/fHHB/o2lk1atbU8y+OzNjWsX07tb7pZj3Tb0CuP19uyq/2w6fO5dpjSdLJE3HqckdrjZ82T3XqR0mSHn2gk1refKse6PFYxu169+iqho2bqfujT+X4uUKDAq6492I2HzOS3f20m5Ff7cmpuT+bqt9Tj6tEcLBeGDkqY9ug/n0UEBCgl0aPy9XnKuCd9+tLtGzaSH0HDNTdne7N1cdNScvdtzznzp1T66YN9OqkqWrWolXG9gfuu1vNWrTUE0/1zbXn8vflvcG/5Wd7vec/z9H9to25Vb0X/K6vtxy97G2mPlhPgf4F1GPOpTMpGlYurgWPNtQNI77WmXOpOWpY/1LbHN0vOwkJ8ep2XycNGTZcc2bNULXramjgc0Nz9Tl88uC1hmPenPzqT0vPm19tOeaz5sETTPPUx5s8/w9ZOXFn7dKmE3LE6Mytvn376tVXX9WkSZOcLunp6VqwYIEmTZqkyZMnm0yUJH3/7WrVqFVLzw3oq1taNlW3+zrp/eWeeyrZxVKSk7V1y2Y1btLMaXvjJk218Y/fDVW5xub2+PizkqQiRYtmbKtVt77W/vCdYo8dkcPh0B/rf9XB/XsV1aiJqcxM2bzfJbv7aTfD5nZJqls/Sr/9ulZ79+yWJO3Yvk0bf9+gps1bGi5zT1pamj5b9YkSExN0fd16pnOylZaWprS0NPn5+ztt9w/w18bfNxiqco3Nx7zN7f8WXNhPLa8rqRW/HTSd4rYxo15Ss+at1KixZ71/yYrNx43N7ZL9/RLHPGADo2OsvXr10q+//qpFixapRo0aGdt9fX31xRdfqGbNmtk+RlJSkpKSkpy2JctX/he90bwSBw/s14p3l+iBB/+rHo88qs2b/tL4saPl6+enO+/qmGvPk1dOnDyhtLQ0BQcHO20PDg5RbOwxQ1WusbXd4XBo1pTxqlW3vipVqZqx/cl+gzV5zEg90KGtfHwKyNvbS30HD1ftupEGay9l636/wOZ+2s2wuV2Suvd8RGfPnlHnjnfI28dH6WlpeuLpvrq13R2m01yyc8d2df/P/UpOTlLBQoU0YfJUVakSYTorW4GBgapzfT3NmzVD4eFVVCI4WF989ok2//WnyleoaDovSzYf8za3/1vHyDDFJ6Xpi812/eX/81WfaNuWLXp7yXLTKW6x+bixuV2yv59jHrCD0ZlbM2fO1PDhw3Xrrbdq6tSpOXqM6OhoBQUFOV0mjBuTq53p6Q5dV6Omej/TT9fVqKl7OndRx3s6a8W7S3L1efKal5fzVFeHw3HJNk9lW/u0CdHa/fdODRk51mn7B8sWadvmPzVy3Gua+uZi9Xp6gKZOGK0Nv601VJo12/b7xWzup90MW9u//OxTrfrkI70S/areWbJCI16O1sL58/Txyg9Mp7mkUni4lix/X/MXLlHn+7rqxecH659//jad5ZKRo8bKIYfuaNtSzRrW1dJF7+jWdnfKx8fHdJpLbD3mJbvbJemeBmX18R+H8uRU5bxy+HCMXh0zWq+MeTVX/5Ccn2w+bmxul+zs55gH7GH87NiOHTvqhhtu0EMPPaRPPvlEb775plv3HzJkiPr37++0LVm+uZmokJIhCq9cxWlbeHhlffPVF7n6PHmleLHi8vHxUWxsrNP2uLjjCg4OMVTlGhvbp02M1s8/fqsJ0+epZKn/na+clHROb70xRS9GT1Kjpi0kSZUjqmnXzu1avmi+Im+40VTyJWzc7/9mcz/tZtjcLkmvTRqv7j0fUdv/n6kVUbWaYmIO6a25s6yYYezr66cK/z/TqVatOtq8aZMWv7NAzw9/yXBZ9sqVr6CZc99WYmKC4s+eVUjJUho6qJ/CwsqaTsuSzce8ze0XRFUqpsqlCqvf4j9Np7hl6+bNios7rge63JOxLS0tTRvWr9O7ixdq7fo/PXZg1+bjxuZ2ye5+jnlkxUsMEnoSj/i0xLJly+qrr75SixYtVL9+fbmzxr2/v7+KFi3qdMntUfW69SK1d88ep2179+5RmTJhufo8ecXXz081atbS2jU/OW1fu2aN6tarb6jKNTa1OxwOTZ0wWj99+7XGvT5boWHlnK5PTU1VamqqvL2df+y8vb3lSPesv9ratN8zY3M/7WbY3C5JSecSL31t8fHxuNcW1zmUnJxsOsItBQsWUkjJUjp9+pTWrvlJLVq1MZ2UJZuPeZvbL7j3hnLadOCUtsecMZ3iloY33qh331upxcvez7jUrFVb7e5or8XL3vfYX/Ilu48bm9slu/s55gF7GJ+5dYGXl5eGDBmitm3b6scff1SZMmVMJ2Xo9mB39Xyom+bNnqlbbr1Nm//6S+8vX6Zhw0dmf2cP8WD3Hho2eJBq1q6tunXra8WypYqJiVHnLl1Np2XLlvap40dr9ZerNGLsZBUsFKi44+f/ShJYuLD8/QMUGFhY19dvoNlTJ8rP31+lQ8voz9/X66tVH+vRPs8arr+ULfv9cmzup90Mm9ubtWytN2fPVGhoGVWuUlXbt23Rorff0l0dOplOy9brr01U02YtFBoaqvj4eH3+2ada99uvmjZjtuk0l/y85kfJ4VCFSuE6sG+vpkwar4qVwtW+w92m07Jl8zHvqe2F/HxUIbhQxtflShTUdWWK6FRCimL+/1OZA/19dGud0hr7yfZMHyOksJ9CivhnPE610MKKT0pTzMlzOpWYkvffRBYCAwsromo1p20FCxZUULFil2z3RJ563LjC5nbJ3n6OecAeHjO4dUFUVJSioqIkSfv379fw4cM1b948o021atfR+ElTNPW1SZozc7rCypbTgEGD1e6O9ka73HFbu9t16uQJzZoxXceOHVVE1Wqa9sYsjz9tQrKn/eP3z3+C5sDeDzttHzDsJbW9o4MkachLYzVvxmsaO2KIzpw+rVKhZfTfx57SnXd3zvfe7Niy3y/H5n7azbC5feDg5/XGtNc0dvRLOhEXp5CSpdTp3vv0yGNPmk7L1vHjx/X80EGKPXZMhYsUUdWq1TVtxmzd2KSp6TSXnD1zRtNfn6SjRw6raFCQbmrTVk881VcFfHN3iYS8YPMx76nttcsV1YJHG2Z8PeTO6yRJ768/qCHLNkmS7qhbRl7y0id/HM70MbreWF5P3fy/D1RY+Hij84+17C+9v/5QXqVfEzz1uHGFze2S/f22Yr/jWuLlcOccwHy2ceNGRUZGKi0tza37nUmy9TSM83x9POJs0WvO4f//i6qNQoMCTCcAcJFNi0dnpoC3vetLpKR57FuebPn78t7AlHrPf246IcfWv9TWdEKO+Vj8WgNz0tLtfZ23+ZgP8LgpM/nj081HTSfkidtrlTKdkCNGD8OVK1dmef2uXbvyqQQAAAAAAAA2Mjq41bFjR3l5eWW5gDwfUwoAAAAAAIDLMTrHvUyZMlqxYoXS09MzvWzYsMFkHgAAAAAAADyc0cGtqKioLAewspvVBQAAAAAAkN+85XVVXmxl9LTEgQMHKj4+/rLXR0REaPXq1flYBAAAAAAAAJsYHdxq3rx5ltcHBgaqZcuW+VQDAAAAAAAA2/C50gAAAAAAALAWg1sAAAAAAACwltHTEgEAAAAAAGzjZe/a61clZm4BAAAAAADAWgxuAQAAAAAAwFoMbgEAAAAAAMBarLkFAAAAAADgBtbc8izM3AIAAAAAAIC1GNwCAAAAAACAtRjcAgAAAAAAgLVYcwsAAAAAAMANXmLRLU/CzC0AAAAAAABYi8EtAAAAAAAAWIvBLQAAAAAAAFjrqlxza/fRBNMJV6RamcKmE65JiclpphMAXAMOxiWaTrgi5YILmk7IMb8C/E0P7tvwclvTCTm2L9be15uyxQNMJ+SYL681xqSmOUwn5JiPN+s32Yb/ZJ6FV14AAAAAAABYi8EtAAAAAAAAWIvBLQAAAAAAAFiLwS0AAAAAAABY66pcUB4AAAAAACCveIkV5T0JM7cAAAAAAABgLQa3AAAAAAAAYC0GtwAAAAAAAGAt1twCAAAAAABwgxdLbnkUZm4BAAAAAADAWgxuAQAAAAAAwFoMbgEAAAAAAMBarLkFAAAAAADgBi+x6JYnYeYWAAAAAAAArMXgFgAAAAAAAKzF4BYAAAAAAACsxZpbAAAAAAAAbvBmyS2PwswtAAAAAAAAWIvBLQAAAAAAAFiLwS0XvL9onjrfHKU3p4932n5g726NeaGfHrqrhR5s31xDn+quY0diDFVmb+nihWrX9ibdUL+OunbupA3r15lOcpkN7as+XKZnet6n+29vrvtvb67nnuyu9b/8lHH9ybjjei16uHrc01b33dpEIwf21qED+wwWZ8+G/Z4Vm/tpN8PG9uUL56lj60jNmfpqxjaHw6HFb72hHve21X23Ntawvr20b/c/BiuzdvTIEb0wZJDaNL9RTRvWV7fOd2vrls2ms1yyft1v6tP7cd3Supnq1a6ub77+ynSSW2w85i+wtX3u7Jl6oMu9atowUje1aKJ+fXprz+5dprMyterDd9Wn533qenszdb29mQY9+ZDW//JjxvWJCQmaOXmMet57qzq3vVG9H+qkVR++a7D48tq3a6MGdWtcchk7+iXTaS6z9Zi/wNb++Ph4TRw3Wne1u0nNG9XTww/dry2b/jKd5TJb9zvgLo8a3EpJSdEHH3ygV199Ve+8847i4+NNJ+nvbZv15afvq2Llqk7bDx/arxf6Pqyy5Stp5IRZGj9zse75zyPy8/M3VJq1z1Z9qnFjotXr0Se0dPkHioyM0pOP9VLMoUOm07JlS3twyVJ68NE+Gj/zHY2f+Y7qRN6g6GH9tG/3P3I4HIp+vr+OxBzQ0FGTNGn2IpUMLaPhAx7XucRE0+mZsmW/X47N/bSbYWP7zm2b9cXH76nSRf9Gvb9kvlYuW6hH+zynV994W8VLBGv4wCeUmGD+39WLnT59Sg9376YCBQrotemztOz9j9V3wCAVKVLEdJpLEhMTVK16dQ0e+qLpFLfZeMxfYHP7hnW/qcv93bRg0VLNmDVPaampeuLRR5SYkGA67RLBJUvroUef1oSZCzVh5kLViWyo0f//3kaS5k4brw2/rlG/YaM0df57uqvzA5r12jj98uNqw+WXWrBwmT77+vuMy7SZcyVJbW65zXCZa2w+5iW7+0eNfF6/rF2jEa+M1aJlH6pR46bq/XhPHT1yxHRatmze7zbwukr/Zyujg1tNmjTRyZMnJUnHjh1TVFSUunTpotmzZ6tXr16qWbOmDh48aKwvMTFBU6Kf1+P9nldg4aJO1y2eN131GzXVg48+o/Cq16l0WDlF3dhcQcVLGKrN2tvz39Td99yjTvd2VuUqVTRoyDCFlgnVu0sXm07Lli3tDZu0VIMbm6ls+YoqW76i/vPIUwooWEjbt/ylQwf2afuWv/R4v6Gqel0tla1QSY/1HaJziYn64evPTKdnypb9fjk299Nuhm3tiYkJmjRqmHo/+4ICi/zv3yiHw6GPli9S5/88rMYt2qhieISeGfySks6d0/dfrTJYnLn58+aodOkyGv7yaNWuc73CypZVwxsbq1z5CqbTXNKseUs91aef2tzS1nSK22w75v/N5vZpM+foro6dVCWiqqpfd51GvBKtwzGHtMUDZyuef2/TPOO9zYMZ723+lCRt3/ynbrrtTtWp30Cly4Tp1vb3KDyimv7evsVw+aWKlyihkJCSGZcfv/9W5cpXUFSDG0ynucTmY16yt//cuXNa/fWXerrvs4qMukHlK1TUo088pbCwclqxzLPbJXv3O5ATRge31q5dq+TkZEnSsGHD5OPjo71792rHjh06cOCAypUrpxdfNPeX0LlTxiiyUTNdH9XIaXt6ero2/PKjwspV0CvP9dbD996sIU89pF9/8ry/UklSSnKytm7ZrMZNmjltb9ykqTb+8buhKtfY2p6WlqYfvv5c584l6rpa1ysl5fxx7uvnl3EbHx8fFSjgqy1//WGo8vJs3e8X2NxPuxk2ts+aPEZRNzZT3Yv+jToSc1An4mJVr8GNGdt8/fxUu26Utm3+M78zs/X9t6tVo1YtPTegr25p2VTd7uuk95d75mlNVxMbj/kLbG7PzNmzZyRJQUFBhkuylpaWpu+//kznziWqeq3rJUk16tTTrz99p+PHjsrhcOjP33/Twf17Vf+GJoZrs5aSkqxPP/lId3XsJC8vz5+lYPsxb3N/Wlqa0tLS5OfvfHaOf4C/Nv6+wVCVa2ze70BOFDAdcMF3332niRMnKjQ0VJIUHBysUaNGqUePHlneLykpSUlJSU7bkpNSLnkBctdPqz/Xrp3bNGb625dcd+pknM4lJuiDJW+p63+f1AO9+uiP39Zo/IiBGj5+pmrVjbqi585tJ06eUFpamoKDg522BweHKDb2mKEq19jWvmfXTg1+8r9KTk5WQMGCGvzyBJWvVFmpqSkqWbqM3p49VU8OGCb/gIJa+e47OhEXqxNxnvd92LbfL2ZzP+1m2Nb+wzef65+d2zT+jUv/jToZd1ySVKy48/cSVLyER64LefDAfq14d4keePC/6vHIo9q86S+NHztavn5+uvOujqbzrlq2HfP/ZnP7xRwOhyaMG6P6kVGKqFrNdE6m9uzaqeee7K7k5GQVLFhQQ16eoAqVqkiSevV5TtPGv6SenW+Vj08BeXl76amBL6rm9fUNV2ft22++1tkzZ9T+rrtNp7jE9mPe5v7AwEDVub6e5s2aofDwKioRHKwvPvtEm//6U+UrVDSdlyWb9zuQE8bX3Lrw15KTJ08qPDzc6brw8HDFxGT9Rjw6OlpBQUFOl7nTJlxRU+zRw3pz2nj1GfJKpmtoOdIdkqQGjVvqznsfUHhEdd19fw9F3thcX3684oqeOy9d/Jcph8NhxV+rJHvay5avpElzFmvc9Plq16GzpkS/qP17dqlAAV8999KrOrR/r/7TvpW63NpEm/5Yp8hGTeXt7WM6+7Js2e+XY3M/7WbY0H7s6GHNmfqq+g3N/N+oDJlke9r3Iknp6Q5dV6Omej/TT9fVqKl7OndRx3s6a8W7S0ynXRNsOOYvx+b2C8aMelk7d2xX9Lgre++al8qWr6TJc5Zo3PT5uq1DZ70W/aL27Tm/5tbHKxZr+5a/NGz0ZE2ctVA9n+ivNyZF6491aw1XZ+3D91eoSdPmKlmqlOkUt9h+zNvaP3LUWDnk0B1tW6pZw7pauugd3druTvn4eO57+H+zdb8D7jI+c+u///2v/P39lZKSor1796pmzZoZ18XExKhYsWJZ3n/IkCHq37+/07YdR1OuqGnXzq06dTJOzz3xn4xt6elp2vrXBn32wbt65+Mf5ePjo/IVKzvdr1yFcG3b9McVPXdeKF6suHx8fBQbG+u0PS7uuIKDQwxVuca2dl9fX5Upd36dmIjramrnts36aMUiPTngeUVUr6nJc5co/uwZpaamKqhYcQ184iFFVK9huPpStu33i9ncT7sZNrX/s2OrTp2I04DHHsjYlp6epi1/btCn77+raQvek3R+BleJ4JIZtzl1Iu6S2VyeIKRkiMIrV3HaFh5eWd989YWhomuDTcf8xWxu/7cxo1/Wd6u/0dz576j0/5+54In+/d6m6nW1tHPbZn28YrEefupZvTPndQ15eaIaNG4uSapUpZp2/b1dHyx92+nUaE8Sc+igfv3lZ42bOMV0istsP+Zt7y9XvoJmzn1biYkJij97ViElS2nooH4KCytrOi1Ltu93GzBG6FmMztzq3r27SpUqpaCgIHXo0EFnz551un7FihWqV69elo/h7++vokWLOl2u9JTEOvUbasLspXp15qKMS5VqNdWsTTu9OnORfP38VKV6LR08sNfpfocO7FVIKc97c+Lr56caNWtp7ZqfnLavXbNGdet59rRxm9slySGHUpKdB1sDCxdRULHiOnRgn/7ZvkUNm7YyE5cF2/e7zf20m2FTe93Ihnpt3ruaNGdxxiWiek21uLmdJs1ZrNCwcipeIsRp5kRKSoo2bVyv6/5/nRxPUrdepPbu2eO0be/ePSpTJsxM0DXCpmP+Yja3S+dnTYwZ9ZK++epLzZz3lsqWK2c6yW0pyclKS01VamqqvLydf7vz8fGRw5FuqCx7Kz98X8VLlFCz5i1Np7jM9mPe9v4LChYspJCSpXT69CmtXfOTWrRqYzopS1fLfgdcZXTm1ptvvpnl9SNGjDAy3bNgoUBVCI9w2uYfUFBFigZlbL/rvgc16ZUhqlmnvmrVu0F//LZG63/+QSMmzMz3Xlc82L2Hhg0epJq1a6tu3fpasWypYmJi1LlLV9Np2bKl/e3ZryuyUVOFlAxVYmK8fvzmc23+Y71eHDdVkvTTt1+qaFBxlSwdqr27/tac119Vw2atVP+GxobLM2fLfr8cm/tpN8OW9oKFAlXxMv9GXdje/t5uWr5wnsLKVVCZchW0/J158g8IUIub25lIzlK3B7ur50PdNG/2TN1y623a/Ndfen/5Mg0bPtJ0mksSEuK1b9++jK8PHjygbdu2KigoyOMH6Gw55jNjc3v0Ky9p1acfa9KUaQoMDMxY+6Zw4SIKCAgwXOfs4vc2P3zzuTb9sU7Dx01TocDCql03Sm/NmCw/vwCVCi2jTX+s1+rPP1bP3v2zf3AD0tPT9dGH7+nO9h1VoIDxE1jcYvMxL9nd//OaHyWHQxUqhevAvr2aMmm8KlYKV/sOnr9mm837HXCXR7+qx8XFafjw4Zo3b57plEs0anaTHn1mqN5f8qbmTRuvsPIV9ezwcapRxzNHwW9rd7tOnTyhWTOm69ixo4qoWk3T3pjl8dNpJXvaT56I0+RRL+hEXKwCAwurYuWqenHc1Ixp+SeOx2retIk6deK4igeHqFXbO3XfQ70MV1+eLfv9cmzup90Mm9svdnfX7kpKOqeZk8fo7JnTqlajtka8Ol0FCwWaTrtErdp1NH7SFE19bZLmzJyusLLlNGDQYLW7o73pNJds3rRJvXo+lPH1hHHRkqT2He7Wy6PGmMpyic3HvM3ty5YuliT16vGQ0/aRr4zWXR07mUi6rJMnjmvyqOcV96/3NsPHTct4b/Psi2O0YPbrmjhqqM6ePq2SpcvoP4/01m13dTZcnrlf1/6swzExHrefXWHzMS/Z3X/2zBlNf32Sjh45rKJBQbqpTVs98VRfFfD1NZ2WLZv3O+AuL4fD4TAdcTkbN25UZGSk0tLS3Lrfn/vPZn8jD1atTGHTCdek3cfiTSfkWHhJz/uFFUDmdh+197VGksoFFzSdkGMFvI1/jk6Osa6HOeme+1Y5W/tiE00n5FjZ4p41i80dvgXsfa2xXVKK554Smx1/X3uPmwCPnjKTd37aecJ0Qp5oWrW46YQcMXoYrly5Msvrd+3alU8lAAAAAAAAsJHRwa2OHTvKy8tLWU0e42NKAQAAAAAAcDlG5z6WKVNGK1asUHp6eqaXDRs2mMwDAAAAAACAhzM6uBUVFZXlAFZ2s7oAAAAAAADym7eX11V5sZXR0xIHDhyo+PjLL6wbERGh1atX52MRAAAAAAAAbGJ0cKt58+ZZXh8YGKiWLVvmUw0AAAAAAABsY+/njQIAAAAAAOCaZ3TmFgAAAAAAgG3sXZ3q6sTMLQAAAAAAAFiLwS0AAAAAAABYi8EtAAAAAAAAWIs1twAAAAAAANzBolsehZlbAAAAAAAAsBaDWwAAAAAAALAWg1sAAAAAAACwFoNbAAAAAAAAsBYLygMAAAAAALjBixXlPQoztwAAAAAAAGAtBrcAAAAAAABgLQa3AAAAAAAAYK2rcs2t0GL+phNgoeDCfqYTAFwDwooXNJ1wRQ7GnTOdkGMVQuzd96zrYZDDdEDOlSxq73ubN9buMZ2QY083q2w64ZrlzUsl8pEXx5tHYeYWAAAAAAAArMXgFgAAAAAAAKzF4BYAAAAAAACsdVWuuQUAAAAAAJBXWHLLszBzCwAAAAAAANZicAsAAAAAAADWYnALAAAAAAAA1mLNLQAAAAAAAHew6JZHYeYWAAAAAAAArMXgFgAAAAAAAKzF4BYAAAAAAACsxZpbAAAAAAAAbvBi0S2PwswtAAAAAAAAWIvBLQAAAAAAAFiLwS0AAAAAAABYi8EtAAAAAAAAWIsF5QEAAAAAANzgxXryHoWZWwAAAAAAALAWg1sAAAAAAACwFqclXsYfG9Zp8dtvavvWLToee0yjxr+mFq3aZFzfvEHtTO/3RJ/+6vZQz/zKdMvSxQv11ptzFXvsmKpEVNWgwUMVGdXAdJZLbGjnmPE8NvfTboat7fHx8Zo57TV9u/ornYiLU7XqNTRg0FDVrF3HdJqTVR++q1UfLtfRw4ckSRUqVVaX7o8qqlEzSVJiQoIWzJqiX35crTOnT6lUaJjuvKer2nW4z2T2Zc2dPVPffPWl9uzeJf+AANWtV1/P9BugSuGVTae5zNZjXrK3/d2li7V86WIdOnRQklS5SoQefby3mjVvYbjsUr+vX6dFC+Zp+9Ytio09pugJU9Sy9f/e28x5Y5q++mKVjh4+LF9fX1WvUVOP9X5Gtepcn++tMTv+0p9fLNfxfX8r4VScbn7iBVWq1yTjeofDoQ0fL9T2H1YpKeGsSoZXV9P7e6t4WMWM26SlJOuX5XP0z2/fKS0lSWHX1VPTbr0VWLxkvn8/mbH1mL/Axv727doo5tChS7Z37nK/nhv6ooEi99m434GcMDpz68CBA4qNjc34+ocfftADDzyg5s2b6z//+Y9+/vlnY23nEhMVUbW6+g0amun1H3z2rdNl8Isvy8vLS61uuiWfS13z2apPNW5MtHo9+oSWLv9AkZFRevKxXpm+WHsaW9o5ZjyLzf20m2Fz+6iRz+uXtWs04pWxWrTsQzVq3FS9H++po0eOmE5zElyytB569GlNmLlQE2YuVJ3Ihho9rJ/27f5HkjR32nht+HWN+g0bpanz39NdnR/QrNfG6ZcfVxsuz9yGdb+py/3dtGDRUs2YNU9pqal64tFHlJiQYDrNJTYf8za3ly5dWk/3HaCFS5Zr4ZLlatjoRvXr01v//L3TdNolzp1LVES16ur/3LBMr69QsaIGPDdMb7/7vmbMe1tlwsqqb+9eOnEiLp9LpdTkcwouV1mNuz6Z6fV/fr5Mm756T427PqkOQ15ToaLFtWryUCWf+9/P68/vztSeP9bopl6DdefA8UpJOqfPp45Qenpafn0bl2XzMS/Z279g4TJ99vX3GZdpM+dKktrccpvhMtfYut9t4XWVXmxldHDrvvvu02+//SZJ+vDDD9WqVSudPXtWTZs2VUJCglq2bKmPP/7YSNuNTZur15N91PIyAw/BISFOlx+/W636DRoqrFz5fC51zdvz39Td99yjTvd2VuUqVTRoyDCFlgnVu0sXm07Lli3tHDOexeZ+2s2wtf3cuXNa/fWXerrvs4qMukHlK1TUo088pbCwclqxzLPaGzZpqQY3NlfZ8hVVtnxFPfjIUwooWEjbt/wpSdq++U/ddNudqlO/gUqXCdOt7e9ReEQ1/b19i+HyzE2bOUd3deykKhFVVf266zTilWgdjjmkLVs2m05zia3HvGR3e8tWN6l5i5aqWClcFSuF66k+/VSoUCH9+edG02mXaNy0uR7r/Yxatcn8vU3bdnfqhkaNVbZceVWuEqE+/Qcp/uxZ/bNjRz6XSuVr36AGHbsrPLLpJdc5HA5t+voD1WvXVeGRTVWibCW1/O8ApSYn6Z9fv5UkJSfGa8dPX6jRvb1UtkZ9hVSIUKueA3Xi4B4d2vpH/n4zmbD5mJfs7S9eooRCQkpmXH78/luVK19BUQ1uMJ3mElv3O5ATRge3Nm3apBo1akiSoqOjNXr0aH344YcaM2aM3nvvPU2cOFEvvuj50z3jjsfq5x+/150dOplOyVRKcrK2btmsxk2aOW1v3KSpNv7xu6Eq19jcnhWOmbxlcz/tZtjcnpaWprS0NPn5+ztt9w/w18bfNxiqyl5aWpq+//oznTuXqOq1zp/CVKNOPf3603c6fuyoHA6H/vz9Nx3cv1f1b2iSzaN5hrNnz0iSgoKCDJdkz+Zj3ub2i6WlpemzVZ8oMTFB19etZzrniqSkJOvD95apcOEiiqhW3XSOkzOxh5V4+oTK1ozM2Obj66fQanV09J/zg+exe3cqPS1V5f51m8BiwSpetqKO/GN2gN32Y972/gtSUpL16Scf6a6OneRlwcfkXS37HXCV0cEtb29vnT59WpK0e/dutWvXzun6du3aafv27SbS3LLq45UqFFhILVrfbDolUydOnlBaWpqCg4OdtgcHhyg29pihKtfY3J4Vjpm8ZXM/7WbY3B4YGKg619fTvFkzdOzoUaWlpWnVJyu1+a8/PbJ9z66d6nJbE917SyO9MXGUhrw8QRUqVZEk9erznMpXqqyenW/VPTc31MhBvfV4vyGqeX19w9XZczgcmjBujOpHRimiajXTOdmy+Zi3uf2CnTu2q0nDSDWKul6jXh6hCZOnqkqVCNNZOfLT99+qTdMGanVjpJYsXKDJM2arWPHiprOcJJ4+IUkqWNS5q2CRYkr4/+sSTp+Qd4EC8g8scsltLtzfFNuPedv7L/j2m6919swZtb/rbtMpLrla9js824gRI+Tl5eV0CQ0Nzbje4XBoxIgRCgsLU8GCBdWqVStt3pw3M9yNDm61bNlSixefnxJZv359ffvtt07Xr169WmXLls3yMZKSknT69GmnS1JSUl4lZ+rTle/rltvulP9FfzX3NBf/hcHhcFjxVwfJ7vbMcMzkD5v7aTfD1vaRo8bKIYfuaNtSzRrW1dJF7+jWdnfKx8fHdNolypavpMlzlmjc9Pm6rUNnvRb9ovbtOb/m1scrFmv7lr80bPRkTZy1UD2f6K83JkXrj3VrDVdnb8yol7Vzx3ZFj5tgOsUtth7zkt3tlcLDtWT5+5q/cIk639dVLz4/WP/887fprByJvKGh5i9eoZlvLtSNTZrphecGKC7uuOmsTGV2fHhls8KM4/wd8ybITTYf85L9/R++v0JNmjZXyVKlTKe4xfb97tFML47lIYtu1apVSzExMRmXv/76K+O6cePGaeLEiZo6dap+++03hYaG6pZbbtGZM2fcf6JsGB3cGjNmjGbPnq3u3burWbNmGjZsmB588EGNHj1a3bt311NPPaWhQzNfnPuC6OhoBQUFOV2mTBibT9+BtPH39dq3d7fad/TM08skqXix4vLx8XFavF+S4uKOKzg4xFCVa2xuvxyOmbxncz/tZtjcLknlylfQzLlv67uf1+ujz77RWwvfVWpqisLCsv4DkQm+vr4qU66Cql5XSw892keVqlTTxysWKynpnN6Z87oefnKAGjZpqUpVqumOTl3VrHVbfbD0bdPZWRoz+mV9t/obzZ63QKX/9ddKT2bzMW9z+wW+vn6qUKGiatWqoz59B6hateu0+J0FprNypGDBQipXoaJqX19XQ4e/LB8fH338wXums5xcmLGVcMp5ofvEMydVsGgxSVKhosWVnpqqpHjnX7jOnTmpgkWK5UfmZdl+zNveL0kxhw7q119+VodO95pOcdnVsN9hhwIFCig0NDTjUrLk+U+YdTgcmjx5soYNG6ZOnTqpdu3amj9/vhISErRo0aJc7zA6uFWjRg398ssvSk5O1rhx4xQfH6+FCxdqxIgR+vvvv7VkyRL997//zfIxhgwZolOnTjld+gx4Ln++AUkff/ieqteoqYhq1+Xbc7rL189PNWrW0to1PzltX7tmjerW8+xTPWxuvxyOmbxncz/tZtjc/m8FCxZSSMlSOn36lNau+UktWrUxneSSlORkpaWmKjU1VV7ezn8y9PHxkcORbqgsaw6HQ2NGvaRvvvpSM+e9pbLlyplOcpnNx7zN7ZfnUHJysumIXOFweN73UiQkVAWLFtfBrf9bZygtNUWHd/ylUlVqSpJCKlaVt08Bp9sknIrTiYN7Vfr/b2OK7ce87f2StPLD91W8RAk1a97SdIrLrob9DjPcPTtu586dCgsLU3h4uLp27apdu3ZJOr/01OHDh9W2bduM2/r7+6tly5Zas2ZNrncXyPVHdFOVKlW0ePFiORwOHT16VOnp6QoJCZGvr69L9/f397/k1K5zZ1KuuCshIUEH9+/L+Drm4EHt3L5NRYOCVDq0jCQp/uxZffvVF+rd99krfr689mD3Hho2eJBq1q6tunXra8WypYqJiVHnLl1Np2XLlnaOGc9icz/tZtjc/vOaHyWHQxUqhevAvr2aMmm8KlYKV/sOnrUuyNuzX1dko6YKKRmqxMR4/fDN59r0xzoNHzdNhQILq3bdKL01Y7L8/AJUKrSMNv2xXqs//1g9e/c3nZ6p6Fde0qpPP9akKdMUGBiYsYZJ4cJFFBAQYLguezYf8za3v/7aRDVt1kKhoaGKj4/X5599qnW//appM2abTrtEQkK8Dji9tzmgHdu3qmjRIAUVK6b5c2apWcvWCg4pqdOnTuq9ZUt07OgR3XTLrfnemnIuUaePHcr4+kzsER3f/4/8A4uocIlSqt2mozauWqqgUmEqWqqsNq5aqgJ+/qrSsJUkya9goKo1batfls+Wf2AR+QcW0a/L56h42UoKq1Ev37+fi9l8zEt296enp+ujD9/Tne07qkAB478+u8Xm/Q5zoqOjNXLkSKdtw4cP14gRIy65baNGjbRgwQJVq1ZNR44c0SuvvKImTZpo8+bNOnz4sCSpdOnSTvcpXbq09u7dm+vdHvPT6eXldck3vX//fg0fPlzz5s3L957tWzapz+M9M76eOmmcJOm2Ozto2IhRkqSvv1glh8Ohm2+7Pd/73HVbu9t16uQJzZoxXceOHVVE1Wqa9sYsjzxl5WK2tHPMeBab+2k3w+b2s2fOaPrrk3T0yGEVDQrSTW3a6omn+qqAi38oyi8nTxzX5FHPKy4uVoGBhVWxclUNHzdN9RrcKEl69sUxWjD7dU0cNVRnT59WydJl9J9Heuu2uzobLs/csv//KPVePR5y2j7yldG6y4NPPb/A5mPe5vbjx4/r+aGDFHvsmAoXKaKqVatr2ozZurFJU9Npl9i2ZbOeerRHxtdTJp5/b3N7+w4aOHS49u7ZrU8//lCnTp5QUFAxXVertqbPXaDKBhbHP7Z3pz6d+L+zN35ZNkuSVLXxzWr53wG6/tbOSk1J1k+Lpik54axKhlfXbc+Mkl9AoYz73HjfY/L29tE3s6OVmpyssOvqqu1/B8jb2/z6hTYf85Ld/b+u/VmHY2KseF2/mM373QbZrdlnqyFDhqh/f+c/LF5ureh/fyhgnTp11LhxY1WpUkXz58/XjTeef3+XX+u+eTkcDkeuP2ou2bhxoyIjI5WWlubW/Y7mwswtk4oW9KxfRq4VpxPtPW44ZgB7JKV45il2roo5ec50Qo5VCCloOiHHvFn815j0dI99q5ytxBT33kN7knm/7cv+Rh7q6WaVTSdcs1JS7f031reA0RWDrkiAx0yZyV+/7839RdE9Qf2KRbK/URZuueUWRUREaODAgapSpYo2bNig+vX/dypshw4dVKxYMc2fP/9KU50YPQxXrlyZ5fUXztUEAAAAAACA50pKStLWrVvVvHlzhYeHKzQ0VF9++WXG4FZycrK+++47jR2b+x8CaHRwq2PHjvLy8lJWk8f4mFIAAAAAAADP8uyzz6p9+/aqUKGCjh49qldeeUWnT59W9+7d5eXlpb59+2r06NGqWrWqqlatqtGjR6tQoULq1q1brrcYnftYpkwZrVixQunp6ZleNmzYYDIPAAAAAAAAmThw4IDuv/9+Va9eXZ06dZKfn5/Wrl2rihUrSpIGDRqkvn376sknn1SDBg108OBBffHFFypS5MpOfcyM0ZlbUVFR2rBhgzp27Jjp9dnN6gIAAAAAAMhvnGQmLVmyJMvrvby8NGLEiEw/aTG3GR3cGjhwoOLj4y97fUREhFavXp2PRQAAAAAAALCJ0cGt5s2bZ3l9YGCgWrZsmU81AAAAAAAAsI29nzcKAAAAAACAa57RmVsAAAAAAAC2Ycktz8LMLQAAAAAAAFiLwS0AAAAAAABYi8EtAAAAAAAAWIs1twAAAAAAANzBolsehZlbAAAAAAAAsBaDWwAAAAAAALAWg1sAAAAAAACwFmtuAQAAAAAAuMGLRbc8CjO3AAAAAAAAYC0GtwAAAAAAAGAtBrcAAAAAAABgLdbcAgAAAAAAcIMXS255FC+Hw+EwHZHbzqWaLgAAAABwLar81HumE67IrqmdTCfAMgHX6JSZvw6cNZ2QJ+qUK2w6IUc4LREAAAAAAADWYnALAAAAAAAA1mJwCwAAAAAAANa6Rs+OBQAAAAAAyBnWk/cszNwCAAAAAACAtRjcAgAAAAAAgLUY3AIAAAAAAIC1WHMLAAAAAADAHSy65VGYuQUAAAAAAABrMbgFAAAAAAAAazG4BQAAAAAAAGux5hYAAAAAAIAbvFh0y6MwcwsAAAAAAADWYnALAAAAAAAA1mJwCwAAAAAAANZizS0AAAAAAAA3eLHklkdh5hYAAAAAAACsxeAWAAAAAAAArMXgFgAAAAAAAKzFmlsAAAAAAABuYMktz8LMLTcsXbxQ7drepBvq11HXzp20Yf0600lusbmfdjNsbpfs7qfdDNrNsbmfdjNsbpfs7qc9dzWKCNb8Jxtrw5h2OvRGJ91Wt4zT9YX8fTSqa12ti26nf6Z00HfDb9ZDLcIveZyo8BJ6t28z/f3aXdo68U4t799cAb6e8+ueJ+57V9EOeD6jr3YTJkzQ3r17TSa47LNVn2rcmGj1evQJLV3+gSIjo/TkY70Uc+iQ6TSX2NxPuxk2t0t299NuBu3m2NxPuxk2t0t299Oe+wr5F9DmA6c0bMnGTK8f2fl6tapZWk+/+ZtajvxSs77+W690qatb/zUIFhVeQgv7NNX3W4/q9jGrdfuY1Xrz23+U7siv7yJrnrrvXUE7YAcvh8Nh7CXP29tb3t7eat26tR555BHdfffd8vPzu+LHPZeaC3EXeaBrZ9WoWVPPvzgyY1vH9u3U+qab9Uy/Abn/hLnM5n7azbC5XbK7n3YzaDfH5n7azbC5XbK7n/bsVX7qvRzf99AbndRzxs/6bGNMxrZvXmijlesPavKn2zK2fTaktb7edESvfrRFkvTRoFb6fuvRjK+vxK6pna74MS7GcWNGfrUHXKOLHW09FG86IU/UCAs0nZAjxuepzpkzR4GBgXrwwQcVFhamvn37atOmTaaznKQkJ2vrls1q3KSZ0/bGTZpq4x+/G6pync39tJthc7tkdz/tZtBujs39tJthc7tkdz/tZvz6z3G1vb6MQosFSJKaVAtR5dKF9d2WI5Kk4CL+iqpcQsfPnNPKgS21cdztWtG/uRpWCTaZncHmfU87YA/jg1u33367PvjgAx04cECDBg3S559/rrp166phw4aaPXu2zpw5k+X9k5KSdPr0aadLUlJSrjaeOHlCaWlpCg52/gciODhEsbHHcvW58oLN/bSbYXO7ZHc/7WbQbo7N/bSbYXO7ZHc/7Wa8sHSjdsSc1oYxt2vvtI5a+HRTDVn8h37957gkqWJIIUlS/ztraOGPe/TA6z/pr/0ntbRvM4WXMj8Dw+Z9Tzuy5HWVXixlfHDrglKlSmnQoEHaunWrvv32W9WsWVP9+vVTmTJlsrxfdHS0goKCnC6vjo3Ok0YvL+f/0g6H45JtnszmftrNsLldsrufdjNoN8fmftrNsLldsruf9vz18E0Rigovoe7T1ui20d/opRV/Kfr+emp+XUlJkvf/97/zwx4t/XmvNu0/pRHL/tI/R86qa5NKBsud2bjvL6Ad8HxGz4693A9V8+bN1bx5c02ZMkVLly7N8jGGDBmi/v37O21z+PjnWqMkFS9WXD4+PoqNjXXaHhd3XMHBIbn6XHnB5n7azbC5XbK7n3YzaDfH5n7azbC5XbK7n/b8F+DrrcEdaunhN9bq602HJUlbD55WrXLF9Pgt1fTDtmM6cuqcJGlHzGmn+/59+IzKliiY780Xs3XfS7QDNjE6cyu7teyLFi2qXr16ZXkbf39/FS1a1Oni75+7g1u+fn6qUbOW1q75yWn72jVrVLde/Vx9rrxgcz/tZtjcLtndT7sZtJtjcz/tZtjcLtndT3v+K+DjLb8C3kq/6PemtHSHvP9/nsD+4wmKOZmoKqWLON2mcqnCOnA8Mb9SL8vWfS/RDtjE6Myt9PR0k0/vlge799CwwYNUs3Zt1a1bXyuWLVVMTIw6d+lqOs0lNvfTbobN7ZLd/bSbQbs5NvfTbobN7ZLd/bTnvkL+PgovWTjj6/IhgapVLkgn45N18ESi1uw4phc61da5lDQdOJ6gxtVCdO+NFTRy+Z8Z95nxxQ49276mthw8qc37T6nzjRVVJbSIes36xcS3dAlP3feuoB2X42XzAlVXIY/+0M79+/dr+PDhmjdvnukU3dbudp06eUKzZkzXsWNHFVG1mqa9MUthYWVNp7nE5n7azbC5XbK7n3YzaDfH5n7azbC5XbK7n/bcV7dica3o3yLj65Gdr5ckLf15r/rNX68n5vyqoR1ra2rPG1SskJ8OxiVo7IebteD73Rn3mfPNPwrw9dHIe69XsUA/bTlwSve/9qP2xsbn+/eTGU/d966gHbCDlyO7cwMN2rhxoyIjI5WWlubW/c6l5lEQAAAAAGSh8lPvmU64IrumdjKdAMsEePSUmbyzLSbBdEKeuK5MIdMJOWL0MFy5cmWW1+/atSufSgAAAAAAAGAjo4NbHTt2lJeXV5YLy/MxpQAAAAAAwJMwVOFZjH5aYpkyZbRixQqlp6dnetmwYYPJPAAAAAAAAHg4o4NbUVFRWQ5gZTerCwAAAAAAANc2o6clDhw4UPHxl/8Ej4iICK1evTofiwAAAAAAAGATo4NbzZs3z/L6wMBAtWzZMp9qAAAAAAAAsseSW57F6GmJAAAAAAAAwJVgcAsAAAAAAADWYnALAAAAAAAA1jK65hYAAAAAAIB1WHTLozBzCwAAAAAAANZicAsAAAAAAADWYnALAAAAAAAA1mJwCwAAAAAAANZiQXkAAAAAAAA3eLGivEdh5hYAAAAAAACsxeAWAAAAAAAArMXgFgAAAAAAAKzFmlsAAAAAAABu8GLJLY/CzC0AAAAAAABYy8vhcDhMR+S2c6mmCwAAAHJPusVv17z50zZglfK9lppOyLH9s7uYTrgmBVyj54P9fTTRdEKeiChV0HRCjjBzCwAAAAAAANa6RsdYAQAAAAAAcoZ5yZ6FmVsAAAAAAACwFoNbAAAAAAAAsBaDWwAAAAAAALAWa24BAAAAAAC4g0W3PAoztwAAAAAAAGAtBrcAAAAAAABgLQa3AAAAAAAAYC0GtwAAAAAAAGAtFpQHAAAAAABwgxcrynsUZm4BAAAAAADAWgxuAQAAAAAAwFoMbgEAAAAAAMBarLkFAAAAAADgBi+W3PIozNwCAAAAAACAtRjcAgAAAAAAgLUY3AIAAAAAAIC1WHMLAAAAAADADSy55VmYueWGpYsXql3bm3RD/Trq2rmTNqxfZzrJLTb3026Gze2S3f20m0G7OTb329g+d/ZMPdDlXjVtGKmbWjRRvz69tWf3LtNZbrFxv/+bzf20m+GJ7Y2rldQ7zzTTXxPv0rE3u6hd/bJO15cs6q/XH26ovybepb1v3KOl/VuocunCGdcXC/RT9AOR+nl0O+194x79Pv5Oje5WX0UK+ub3t5IlT9z3rrK5HXCH8cGtjz76SMOHD9fPP/8sSfrmm290++2367bbbtOsWbMM1/3PZ6s+1bgx0er16BNauvwDRUZG6cnHeinm0CHTaS6xuZ92M2xul+zup90M2s2xud/W9g3rflOX+7tpwaKlmjFrntJSU/XEo48oMSHBdJpLbN3vF9jcT7sZntpeyN9Hm/ef1OCF6zO9fv7TzVSxZKAefP1H3TTiC+0/nqDlz7ZSIT8fSVJosYIKLRag4Us3quULn+npub/qpjpl9FqPG/Lz28iSp+57V9jcDrjLy+FwOEw9+RtvvKGnn35adevW1c6dOzV9+nQ98cQT6tKli3x8fLRgwQJFR0frmWeecetxz6XmfusDXTurRs2aev7FkRnbOrZvp9Y33axn+g3I/SfMZTb3026Gze2S3f20m0G7OTb351d7eh6/XYuLi1ObFk005623FdUgd3+p9M6Dz0q3+ZiR7O6n3Yz8bC/fa2mO7nfszS56aMqPWvX7QUlS5dKF9cuYO9Rs2CptP3Ra0vnXg61TOujlZX/qne8zny16V4Nymv7ojar4+Aqlpbv32rd/dpcctWeF4yZ7AdfoYkd7Ys+ZTsgTlUICTCfkiNGZW1OmTNH06dO1bt06ffDBB3rkkUc0ZswYzZ49W2+88YamT5+umTNnmkyUJKUkJ2vrls1q3KSZ0/bGTZpq4x+/G6pync39tJthc7tkdz/tZtBujs39Nrdf7OzZM5KkoKAgwyXZs32/29xPuxm2tvv7np+dlZSSlrEt3eFQSmq6GlUNuez9ihby05lzKW4PbOUFW/e9ZHe7Nbyu0ouljA5u7dmzR7feeqskqXXr1kpLS1OLFi0yrm/VqpX27t2b5WMkJSXp9OnTTpekpKRc7Txx8oTS0tIUHBzstD04OESxscdy9bnygs39tJthc7tkdz/tZtBujs39Nrf/m8Ph0IRxY1Q/MkoRVauZzsmW7fvd5n7azbC1fWfMae2Ljdfz916voEK+8vXxVp/br1PpYgVVuljmM0OKB/qpf/uaWvDtP/lcmzlb971kdzuQE0YHt4KDgzMGrw4dOqTU1FTt27cv4/q9e/eqRIkSWT5GdHS0goKCnC6vjo3Ok16vi6bVOxyOS7Z5Mpv7aTfD5nbJ7n7azaDdHJv7bW6XpDGjXtbOHdsVPW6C6RS32L7fbe6n3Qzb2lPTHOox9SdVCS2iv6d10r6Z96jpdaX01Z+HMp2VVTiggBb1a6Edh07r1Q83Gyi+PNv2/b/Z3A64w+jZsR06dNDDDz+s7t27a+XKlXrooYc0YMAAeXt7y8vLSwMHDlTbtm2zfIwhQ4aof//+TtscPv652lm8WHH5+PgoNjbWaXtc3HEFB19+Sq2nsLmfdjNsbpfs7qfdDNrNsbnf5vYLxox+Wd+t/kZz57+j0qGhpnNcYvt+t7mfdjNsbv9z7wm1Hv6FihT0lV8Bbx0/k6TPnr9ZG/fEOd0uMKCAlg5oqfhzKer++o9KTTN/SqJk9763uR3ICaMzt8aOHauWLVtqyZIlioyM1OzZs/Xwww+rQ4cOateunYKDgxUdnfUsLH9/fxUtWtTp4u+fu4Nbvn5+qlGzltau+clp+9o1a1S3Xv1cfa68YHM/7WbY3C7Z3U+7GbSbY3O/ze0Oh0NjRr2kb776UjPnvaWy5cqZTnKZzftdsrufdjNsbr/gTGKKjp9JUuXShVUvvHjGovPS+Rlbywa0VEpquh6c8qOSUtMNljqzed/b3G4Lr6v0f7YyOnMrMDBQs2fPdtr27LPP6qmnnlJKSoqKFCliqOxSD3bvoWGDB6lm7dqqW7e+VixbqpiYGHXu0tV0mkts7qfdDJvbJbv7aTeDdnNs7re1PfqVl7Tq0481aco0BQYGZqy/UrhwEQUEeP6nJNm63y+wuZ92Mzy1PdC/gMJLFc74ukLJQNUuX0wn4pN1MC5BdzUop9gzSToYl6Aa5YI0qlukVm04qG83Hzl//4ACWvZsKxX089GTs35UkQBfFQnwlSTFnknK80+KdYWn7ntX2NwOuMsjP7QzICBAAQEB2r9/v4YPH6558+aZTtJt7W7XqZMnNGvGdB07dlQRVatp2huzFBZW1nSaS2zup90Mm9slu/tpN4N2c2zut7V92dLFkqRePR5y2j7yldG6q2MnE0lusXW/X2BzP+1meGp73UrF9eHgmzK+fuX+8zOClvy4W0/P/VWlixXUS/fXV8mi/jpy8pzeXbNHE1Zu+d/9KxZXgyrnFzz/bdydTo8d+exH2n88IR++i6x56r53hc3tgLu8HA4PGA6/jI0bNyoyMlJpaWnZ3/hfzqXmURAAAIABnjB7Iae8WbgYsNCbBNwAADroSURBVEr5XktNJ+TY/tldTCdckwI8cspM3tt7PMl0Qp6oGJy7yzzlF6OH4cqVK7O8fteuXflUAgAAAAAAABsZHdzq2LGjvLy8lNXkMT6mFAAAAAAAeBKGKjyL0U9LLFOmjFasWKH09PRMLxs2bDCZBwAAAAAAAA9ndHArKioqywGs7GZ1AQAAAAAA4Npm9LTEgQMHKj4+/rLXR0REaPXq1flYBAAAAAAAAJsYHdxq3rx5ltcHBgaqZcuW+VQDAAAAAACQPZbc8ixGT0sEAAAAAAAArgSDWwAAAAAAALAWg1sAAAAAAACwltE1twAAAAAAAGzjxaJbHoWZWwAAAAAAALAWg1sAAAAAAACwFoNbAAAAAAAAsBZrbgEAAAAAALiFRbc8CTO3AAAAAAAAYC0GtwAAAAAAAGAtBrcAAAAAAABgLdbcAgAAAAAAcIMXS255FGZuAQAAAAAAwFoMbgEAAAAAAMBaXg6Hw2E6IrftOnbOdMIVCSseYDrhmnT8bLLphBwLLuxnOgGAi46cSjKdcEVsfr3xtvhPet6c+2BMSmq66YQcO30u1XRCjhUtaO/qKb4+Fr/YWG7lpkOmE3Ksfa0w0wk5VtDXdIEZB0/a+/tjVsoWs/O9Hq+8AAAAAAAAsJa9fxIBAAAAAAAwgDnVnoWZWwAAAAAAALAWg1sAAAAAAACwFoNbAAAAAAAAsBZrbgEAAAAAALiBDzL2LMzcAgAAAAAAgLUY3AIAAAAAAIC1GNwCAAAAAACAtVhzCwAAAAAAwA1eYtEtT8LMLQAAAAAAAFiLwS0AAAAAAABYi8EtAAAAAAAAWIs1twAAAAAAANzBklsehZlbAAAAAAAAsBaDWwAAAAAAALAWg1sAAAAAAACwFoNbAAAAAAAAsBYLygMAAAAAALiB9eQ9C4Nbmfj4/Xf1yQfv6kjMIUlSxfAq6vbfx3RD42aSpJ+++0qffrhcf2/fqtOnTmrqm0tVpep1JpNdsnTxQr315lzFHjumKhFVNWjwUEVGNTCd5RIb2he9NUc/fPuV9u3dLX//ANWqU1e9nuqnChXDM27z1uzpWv3lKh07ckQFfAuo2nU19fDjfVSj9vUGyy/Phv2eFZv7aTfDhvbFC+bop2+/1v59u+Xn56+aderpkSf7qvy/XmvaNsn8NeWR3v103wM98ivVJTOnv65Zb0xz2hYcHKIvVv9oqMh1c2fP1Ddffak9u3fJPyBAdevV1zP9BqhSeGXTaS6z4Zi/HFvb27dro5hDhy7Z3rnL/Xpu6IsGii7vantvc/TIEb0+eYLW/Pi9ziUlqWLFSnph5CuqUbOW6TSX2HrMX2BDf1pamlYve0sbf/xKZ0/GqUjxYNVveatadnpQ3t7nT3ra/Mv3WvfVRzq0e4cSzpz+v/buPD6mc/Hj+HeyL5KQRCQRiRBrRERCxVKKhlBLldpKiKV6KaFFbY1aYt/3fVeU2pXaS9Uebiy1EyRECLInMuf3h5/cjkRmIsszj37f9zWvW2dmznwyxpzJM+c8B/+ZtAROpT0El2fv3NkzWLViGa5euYQnT55g+qx5aNiosegsogIh/LDE5ORkLF++HMHBwQgMDMRnn32Gb7/9FgcPHhTWZF/cAd37DMDspesxe+l6eFeviTHDBuDe7ZsAgJTkZFT2qobufQYIa8ytvb/tweSJE9Cr9zfYuHkbqlf3xX++7pXthyt9I0v7xfCzaNW2A+YuW4cpsxcjIyMDQ/p/jeTkpMzblHJ1Q//vh2Pp+i2YtXg1HJ1KYkj/r/E87pnA8uzJ8ry/i8z9bBdDlvaI8LNo+UUHzFq8FhNnLYY6IwPDQvpovNds2HlI4/Ld8DFQqVSo1+BTgeXvVrZsOew7dCzzsnHLDtFJOjl/9gzad+yE1es3YsHi5ch49Qrf9O6J5KQk7XfWA7K85rMjc/vqdb9g78E/Mi/zFi0DADT6tKngsqw+pM82L1++QI+gTjAyMsKs+Yvxy9ZdCPluCKysrESn6UTm1zwgT/+x7T/jzIEd+Cy4P/pPX4WAzl/j+M6NOLX318zbpKemwLVCFXzasbfAUt0kJyehfIUK+EHPBs6JCoJKURRF1IPfvHkTjRs3RkJCAkxMTPDo0SM0a9YMsbGxOHv2LNq0aYP169fDyCh3O5jdfpKS763tAuuhZ9+BaPJZm8xlj6Mfolu7Zvm+55ZzMbN8W9cbnTu0Q6XKlTHyx58yl7VuEYhPGjbGgIHf5fvj5afCan+akJZv6wKA53HP0KZpfcxYuALePtl/K5WYkIAWjfwxde4SVK9R670fy66IyXvf911kfs0AcvezXYzCan/8IjXf1gW8fq/5snkDTJ23HFXf8V4TOnQAkpMSMXnO0jw/Xn6/3yyaPwdHDh/Ez79sy9f1ZseggL/Se/bsGRp9XBtLV66Br1+NfF23gSr/D37gv1fdpL9S5+v63jZtchiO/XEUW3fuhSqf/55fprzK1/UV5mcba/P8PcBkzsxpuBgejqWr1ubrerNjbJj/bzYy/3sFCq9/x6W8DZatmTQMRWyK4fM+QzKX/TztRxibmqFtv+Eat42LeYTp33bMtz23Wng653kdOalWpUKB7bllbpzvq5TC45fpohMKRAlrOf9Che651b9/fzRt2hQxMTGIiopCWFgY1Go1Tp48iatXr+LMmTMYN26cyERkZGTgyIHfkJKSjIqe3kJb3ld6WhquXrkM/9p1NZb7166DixfCBVXpRub2xIQEAIC1tU2216enp2PXts2wLGKFsuUqFGaaVjI/74Dc/WwXQ+b2xMTX7zVW73iviXv2FKdPHEPTFp8XZlauRN67hyaN6qFF00YYNmQQHjy4LzrpvSQkxAMAbGyy/7vQJzK/5mVuf1t6ehr27N6Jlq3b5PvAVkGQ+bPNH0cOo5KnJ4Z+F4JP69dBpy/bYOvmTaKzdCL7a16mfrcKXrh96Txio15vh6Lv3sS9a5dQ3ucjwWWkj1SqD/MiK6Fzbh09ehQXLlzIPH550KBB+PHHH/H06VOUK1cOM2fOREhICEaPHl3obXdu3cCgPl2QlpYGc3MLjAqbATf3soXekR/inschIyMDdnZ2Gsvt7OwRG/tEUJVuZG1XFAXzZ02Bl3d1uJctp3HdX8ePYuzIwUhNSYGtfXFMmbMYNkWLCSrNnqzP+xsy97NdDFnbFUXBotlTUMXbJ8t7zRv792yHhYUF6tbXzzk2qnh5Y8z4iXB1K41nz55i2eIFCO7SEZu27kRRPXtvzImiKJg2eSJ8qvvCo1x50TlayfqaB+Ruf9uRQweREB+PFi31d/D5Ddk/2zx8cB9bNm1A5y7d0L1nb1y+FIGpk8JgbGKCz1q2Fp2XI9lf8zL112vVESlJiZg9KAgqAwMoajUate+BqnUaiU4jIi2EDm4VLVoU8fHxmX9OSkrCq1evYGLy+pCHqlWrIjo6Osd1pKamIjU19a1lCkxNTfPU5uJaGvNWbEJCQjz+PHIA08aPwuQ5y6Qd4AKQ5RtBRVGk+JYQkK999pTxuH3zOmYvWpXlumq+NbBkzWa8eB6H3du3YMzw7zFv+ToUs7XLZk1iyfa8v03mfraLIVv73GlhuHPzBqYvXPnO2+zdtQ0NmzSHSR63iwWlTr2PNf5ctWo1tGoegF07tuGrrvo1+X1OJo4fixvXr2HF6vWiU3JFttf8P8nc/sb2rVtQu049FHdwEJ2ileyfbdRqBZU9PdF3wEAAQMVKlXH71k1s2bRB7we33pD9NS9Df8SJw7h4fD/afjsSDqVK49Hdm9izah6sbe3gU1//5sUjov8Reljip59+ikGDBuHvv//GnTt30KdPH1SrVi1zYsfIyEg4aNnYT5gwATY2NhqXhbOm5LnN2NgYzi6uKF/RE937DECZsuWx/Zd1eV6vCMWKFoOhoSFiY2M1lj979hR2dvaCqnQjY/vsqWE4cewIps9fhuIlHLNcb25ugZKlXFHZyxuDR46BoaEhftuxtfBDcyDj8/5PMvezXQwZ2+dNn4C/jh/B5LlLUdwh63sNAERcOIcHkXfRtEWbbK/XR+YWFvAoVx6R9+6JTtHZxLCxOHr4EJYsX40Sjtn/XegbGV/zb8jc/k/RUQ9x+tRfaNWmregUrT6Ezzb2xe3hXkbzS2p39zJ49CjnL9L1geyveZn6961biI9bdUTVOg3h6FoG1T4OQO1mbfHHNrm+uCD6NxI6uDV58mSkpqaicuXK8PDwwKlTp7Bs2bLM6588eYLBgwfnuI5hw4bhxYsXGpc+A3K+z/tQoCA9Xc4J44xNTFCpsidOnvhTY/nJEyfgXc1HUJVuZGpXFAWzpozHsSMHMW3eMjg5u+h2PyhIS8/fyezzSqbnPTsy97NdDJnaFUXB3GlhOH7kIKbMWZrje83eXVtRrmJlvZv7JidpaWm4c/sW7IsXF52ilaIomDh+DA4d2I9Fy1eipItu7/v6QKbX/Ntkbv+nHdu3opitLerWqy865Z0+pM823tWq497duxrL7t27Cyengp3EOz/I/pqXqT89NRUqleavyCoDAwg8BxvpMdUH+j9ZCT0s0cHBAX/99Rdu3LiB1NRUVKxYUePMiG3bav8my9TUNMshiLGpeTtb4spFs+FXqy6KO5RAUlISjh7Yi4jwsxg7bT4AIP7lC8Q8jsbT/z9G/EHkXQBAMVt72OrZtw9vdAnqjhE/DEHlKlXg7e2DLb9sRHR0NNq17yA6TStZ2mdNGY+D+/Zg3JRZsLC0xLOnr7+dsrQsAlMzMyQnJ2HdiiWoXa8BbO2L4+WL59ixZSOexDxG/UYBguuzkuV5fxeZ+9kuhiztc6aOx+H9v+GnSbNgbvGP95oiRWBq+r+z7SYmJuCPQ7/j62+/F5WqkxlTJ+HjBp/A0dE5c86txMQEtJDgMKEJ48bgtz27MGP2PFhaWmbOHVOkiBXMzPL/zMf5TZbXfHZkbgcAtVqNndt/xWctWuf6rOCF6UP6bNOpSxCCu3bC8iWL8GmTprgcEYGtm3/BiNCftN9ZD8j+mpelv6KvP45uXQsbewc4uLgj+u4NnNj9C6p/Eph5m6SEl3gRG4P4uNf/HmKjIgEARYrawqqorZDud0lKSkRkZGTmnx8+fIC//74KGxsbKQZ2iXJDpejxMPT9+/cRGhqK5cuX5+p+t5/kbXBrxoRQXDh3Gs+ePoGlZRG4ly2Pdl91R/Ua/gBeT847PezHLPfr3L0PvurxTZ4eGwCcixXMB+KNP6/DyuXL8ORJDDzKlcfgocPy/VTlBaUw2p8m5O0bxoYfeWW7fMiosWj6WWukpaZi3I9DcfVyBF4+j4O1TVFUqOSJr4K/RsXKVfL02HZFTPJ0/3eR+TUDyN3PdjEKo/3xi1TtN8pBQO2q2S7/fsRYBDRvlfnn3ds2Y+Gsydiw8yAsi1jl6TH/Kb/fb4YNGYTz587gedxzFLMtBi8vb3zTbwDKlM37adXfZpDP+6v7VKmY7fKfxoWhZev8PRTUoIDmpeG/V+3SX6nzfZ0nT/yJft/0xJbte+BW2j3f1//Gy5RXebq/yM821ub5P+h37OhhzJ01A/cj78G5pAs6dwnC522/zPfHMTYsmINjZP73ChRO/45LUXm6f2pyEg5uXI4rZ44j8UUcrGztUbV2QzRo2xVGRsYAgPNH9mLrgklZ7vtJ2yA0bNftvR+7hWf+DzadOX0KvYK7Zn2sVp9j7PiJ+fY45sb5tiqpPInP23usvipupb9fuuRErwe3Ll68iOrVqyMjIyNX98vr4JZoBTW4RTnL6+CWSAU1uEVE+S+vg1uiyfx+k9+DW4WpoAa3SLuCGNwqLHkd3BKpIAa3CktBDW6Rdnkd3BKpIAa3CgsHtz4ssg5uCa3esWNHjtffvn27kEqIiIiIiIiIiHTE7530itDBrdatW0OlUuU4QZ++nR6WiIiIiIiIiIj0h9B9Zp2cnLBlyxao1epsL+fPnxeZR0REREREREREek7o4Javr2+OA1ja9uoiIiIiIiIiIqJ/N6GHJQ4ePBiJiYnvvN7DwwOHDx8uxCIiIiIiIiIiopxxAiX9InRwq169ejleb2lpifr16xdSDRERERERERERyYbnqSUiIiIiIiIiImlxcIuIiIiIiIiIiKTFwS0iIiIiIiIiIpKW0Dm3iIiIiIiIiIhko+KM8nqFe24REREREREREZG0OLhFRERERERERETS4uAWERERERERERFJi3NuERERERERERHlggqcdEufcM8tIiIiIiIiIiKSFge3iIiIiIiIiIhIWhzcIiIiIiIiIiIiaXHOLSIiIiIiIiKiXFBxyi29wj23iIiIiIiIiIhIWhzcIiIiIiIiIiIiaakURVFER+S3pHS5fyQD7t8ohFot7+vGwICvGSIiIiIiWVUYtFN0wnu7N7uF6AQh4pIyRCcUiGIWhqIT3gv33CIiIiIiIiIiImlxcIuIiIiIiIiIiKTFwS0iIiIiIiIiIpIWB7eIiIiIiIiIiEhaRqIDiIiIiIiIiIhkwvPA6RfuuUVERERERERERNLi4BYREREREREREUmLg1tERERERERERCQtDm4REREREREREZG0OKE8EREREREREVEuqMAZ5fUJ99wiIiIiIiIiIiJpcXCLiIiIiIiIiIikxcEtIiIiIiIiIiKSFufcIiIiIiIiIiLKBRWn3NIr3HOLiIiIiIiIiIikxcEtIiIiIiIiIiKSFge3iIiIiIiIiIhIWpxzi4iIiIiIiIgoFzjlln7hnltERERERERERCQtDm4REREREREREZG0hA9uJSYmYsmSJejevTsCAwPRrFkzdO/eHUuXLkViYqLoPADAsiWL0Ll9W9SpWR0NP66Ngf374u6d26Kzcm3jz+sQGNAQNXy80KFdG5w/d1Z0ks5kbN+08Wd82aYl6tbyRd1avujauT2OH/tDdFauyPi8/5PM/WwXg+3iyNzPdjFkbgfk7me7GDK3A3L362N7zbK2WNa7Bk6P/RT3ZrdAgJejxvUWJoYY07YKTo5pjGtTm+Hg8Ab4qq6bxm1MjAzw0xdVEB7WBFenBGJprxpwLGpWmD8GUb4ROrh15coVlC9fHkOGDEFcXBxcXV3h4uKCuLg4DB48GBUqVMCVK1dEJgIAzp89g/YdO2H1+o1YsHg5Ml69wje9eyI5KUl0ms72/rYHkydOQK/e32Dj5m2oXt0X//m6F6KjokSnaSVre4kSJfBtyHdYt2Ez1m3YjJof1cLA/n1x6+YN0Wk6kfV5f0PmfraLwXZxZO5nuxgytwNy97NdDJnbAbn79bXdwsQIVx++xI+/RGR7/Y9tPFG/kgNCVoejUdhhLD1yGz99UQWfepXQuE0Tb0f0W3kObWf9CUtTIyzvXRMGnExKN6oP9CIplaIoiqgH/+STT+Do6IhVq1bBxMRE47q0tDR069YN0dHROHz4cK7Wm5ResD/Ss2fP0Ojj2li6cg18/Wrk+/oNVPn/iurcoR0qVa6MkT/+lLmsdYtAfNKwMQYM/C7fHy8/FVa7Wl3w/xTq1/kIId8Nxudt2ubreg0KYAsk82sGkLuf7WKwXRyZ+9kuhsztgNz9bBdD5nZA7v7Caq8waOd73/fe7BboteQMfo94lLns9x/qY1d4FGbv+98X67sG18PhyzGYtucarMyMcD6sCQauCceu8NcDdQ7Wpjg55lN0W3gKf/z9JFeP/28Un6oWnVAgrEyFH+D3XoRWnzp1CqNGjcoysAUAJiYmGD58OE6dOiWgLGcJCfEAABsbG8EluklPS8PVK5fhX7uuxnL/2nVw8UK4oCrdyNz+TxkZGdj7224kJyehqnc10Tlayf68y9zPdjHYLo7M/WwXQ+Z2QO5+toshczsgd7/M7WduP0PjKo4oYfP6MEP/cnZwL14ER/9/0MqrlA1MjAw0BrFiXqbiWvRL+LoXE9JMlBdGIh+8WLFiuHHjBipXrpzt9Tdv3kSxYjn/w0pNTUVqaqrGsgwDE5iamuZb5z8pioJpkyfCp7ovPMqVL5DHyG9xz+OQkZEBOzs7jeV2dvaIjdV9RF4EmdsB4Mb1awj6qiPS0lJhbmGBaTPnomxZD9FZWsn+vMvcz3Yx2C6OzP1sF0PmdkDufraLIXM7IHe/zO2jt1zCxA7eOD32U6RnqKFWFAz9+b84e/sZAKC4tRlSX2XgZXK6xv1i49NQ3JrzbpF8hO651atXLwQFBWHq1Km4ePEiHj16hMePH+PixYuYOnUqgoOD8fXXX+e4jgkTJsDGxkbjMnXShAJrnjh+LG5cv4YJk6cV2GMUFNVbhzsqipJlmb6Stb20uzs2bN6KVes2oN2XHfDjyB9w69ZN0Vk6k/V5f0PmfraLwXZxZO5nuxgytwNy97NdDJnbAbn7ZWzvXt8dPqWLIXjxaXw25Q+M33oF49p5oU55+xzvp8Lrn4+0U32g/5OV0D23Ro8eDXNzc0yfPh1DhgzJfINQFAWOjo744YcfMGTIkBzXMWzYMAwaNEhjWYZB1sMc88PEsLE4evgQlq1aixKOjtrvoCeKFS0GQ0NDxMbGaix/9uwp7OxyfnMTTeZ2ADA2NoGr6+uzknh6euHypUv4ee1qjAwdI7gsZ7I/7zL3s10Mtosjcz/bxZC5HZC7n+1iyNwOyN0va7upsQEGf1YJXy89g0NXYgAAf0fFo7KLNXo3Kos/r8fiycsUmBoZwtrcWGPvLTsrE5y780xUOtF7Ez5T2NChQxEVFYVbt27h+PHjOH78OG7duoWoqCitA1sAYGpqCmtra41Lfh+SqCgKJo4fg0MH9mPR8pUo6eKSr+svaMYmJqhU2RMnT/ypsfzkiRPwruYjqEo3MrdnT0FaWproCK1kf95l7me7GGwXR+Z+toshczsgdz/bxZC5HZC7X9Z2Y0MDmBgZ4O3zZWWo/3fysoj7L5D2So16Ff83SOdgbYoKTtY4dyeuMHOJ8oXQPbf+yd3dHe7u7qIzsjVh3Bj8tmcXZsyeB0tLy8zjq4sUsYKZmRzHI3cJ6o4RPwxB5SpV4O3tgy2/bER0dDTate8gOk0rWdvnzJqOOnU/hqOjIxITE7Fv7x6cPXMa8xYsEZ2mE1mf9zdk7me7GGwXR+Z+toshczsgdz/bxZC5HZC7X1/bLUwMUbq4ZeafS9lZoHJJazxPSkdUXDL+uhGL4a0qISU9Aw+fJeEjDzt8UcMFY7ddBgDEp7zCxpORGNnaE88T0/E8KQ0jWlXG31Evcfyafs8nRpQd4YNbycnJOHfuHGxtbbNMLJ+SkoJNmzaha9eugupe+2XjzwCAXt01O34aF4aWrduISMq1poHN8OJ5HBYvmI8nT2LgUa485i1cDGfnkqLTtJK1/enTpxg5fAhinzxBESsrlCtXAfMWLEGt2nVEp+lE1uf9DZn72S4G28WRuZ/tYsjcDsjdz3YxZG4H5O7X1/aqrkWxsX/tzD//2MYTAPDLqfv4ft0FfLvyPIa0qIhZXX1Q1MIED+KSMWX331h7/F7mfcb+ehkZGQrmdfeFmbEh/rz+BN8tPp1ljy8iGagUgbPFXb9+HQEBAYiMjIRKpUK9evXw888/w8nJCQDw+PFjODs7IyMjI1frTUqX+1+jgZ5PTvihUkv8Lm5gwNcMEREREZGsKgzaKTrhvd2b3UJ0ghCJafL+/pgTSxM5f7cUOufW0KFD4eXlhZiYGFy7dg3W1taoU6cOIiMjRWYREREREREREZEkhA5unThxAmFhYbC3t4eHhwd27NiBwMBA1KtXD7dv3xaZRkREREREREREEhA651ZycjKMjDQT5s2bBwMDA9SvXx/r168XVEZERERERERERDIQOrhVsWJFnD17FpUqVdJYPmfOHCiKgpYtWwoqIyIiIiIiIiLKnpwzU324hB6W+Pnnn+Pnn3/O9rq5c+eiY8eOEDjfPRERERERERER6TmhZ0ssKDxbIr0Pni2RiIiIiIhE4NkS5ZP0gZ4t0YJnSyQiIiIiIiIiIipcQufcIiIiIiIiIiKSjpw7OH2wuOcWERERERERERFJi4NbREREREREREQkLQ5uERERERERERGRtDjnFhERERERERFRLqg46ZZe4Z5bREREREREREQkLQ5uERERERERERGRtDi4RURERERERERE0uLgFhERERERERERSYuDW0REREREREREuaBSfZiX9zF//ny4u7vDzMwMvr6+OHbsWP4+2Trg4BYREREREREREeXaxo0bERISghEjRiA8PBz16tVDYGAgIiMjC7WDg1tERERERERERJRr06dPR48ePdCzZ09UqlQJM2fORKlSpbBgwYJC7eDgFhERERERERERITU1FS9fvtS4pKamZnvbtLQ0nDt3DgEBARrLAwICcOLEicLI/R+FciUlJUUJDQ1VUlJSRKfkGtvFkbmf7WLI3K4ocvezXQyZ2xVF7n62i8F2cWTuZ7sYMrcrivz9VLhCQ0MVABqX0NDQbG/78OFDBYDy559/aiwfP368Ur58+UKo/R+VoihK4Q6nye3ly5ewsbHBixcvYG1tLTonV9gujsz9bBdD5nZA7n62iyFzOyB3P9vFYLs4MvezXQyZ2wH5+6lwpaamZtlTy9TUFKamplluGxUVhZIlS+LEiRPw9/fPXD5+/HisWbMGf//9d4H3vmFUaI9ERERERERERER6610DWdmxt7eHoaEhHj16pLE8JiYGJUqUKIi8d+KcW0RERERERERElCsmJibw9fXF/v37NZbv378ftWvXLtQW7rlFRERERERERES5NmjQIHTp0gV+fn7w9/fH4sWLERkZiT59+hRqBwe3csnU1BShoaE676anT9gujsz9bBdD5nZA7n62iyFzOyB3P9vFYLs4MvezXQyZ2wH5+0m/tW/fHk+fPsWYMWMQHR2NKlWqYM+ePXBzcyvUDk4oT0RERERERERE0uKcW0REREREREREJC0ObhERERERERERkbQ4uEVERERERERERNLi4BYREREREREREUmLg1u5MH/+fLi7u8PMzAy+vr44duyY6CSd/PHHH2jRogWcnZ2hUqmwbds20Uk6mzBhAmrUqAErKys4ODigdevWuHbtmugsnSxYsABVq1aFtbU1rK2t4e/vj99++0101nuZMGECVCoVQkJCRKfoZPTo0VCpVBoXR0dH0Vk6e/jwIb766ivY2dnBwsIC1apVw7lz50RnaVW6dOksz7tKpULfvn1Fp2n16tUrjBw5Eu7u7jA3N0eZMmUwZswYqNVq0Wk6iY+PR0hICNzc3GBubo7atWvjzJkzorOypW2bpCgKRo8eDWdnZ5ibm6NBgwa4fPmymNi3aGv/9ddf0aRJE9jb20OlUuHChQtCOrOTU3t6ejqGDh0KLy8vWFpawtnZGV27dkVUVJS44Ldoe+5Hjx6NihUrwtLSEsWKFUPjxo1x6tQpMbFvyc3nsK+//hoqlQozZ84stL6caGvv1q1blvf8WrVqiYl9iy7P+9WrV9GyZUvY2NjAysoKtWrVQmRkZOHHZkNbf3bbW5VKhSlTpogJ/gdt7QkJCejXrx9cXFxgbm6OSpUqYcGCBWJi36Kt/fHjx+jWrRucnZ1hYWGBpk2b4saNG2Ji36LL7036vI0lyisObulo48aNCAkJwYgRIxAeHo569eohMDBQbzaAOUlMTIS3tzfmzp0rOiXXjh49ir59++LkyZPYv38/Xr16hYCAACQmJopO08rFxQUTJ07E2bNncfbsWTRs2BCtWrWSbgNy5swZLF68GFWrVhWdkiuenp6Ijo7OvERERIhO0klcXBzq1KkDY2Nj/Pbbb7hy5QqmTZuGokWLik7T6syZMxrP+f79+wEA7dq1E1ym3aRJk7Bw4ULMnTsXV69exeTJkzFlyhTMmTNHdJpOevbsif3792PNmjWIiIhAQEAAGjdujIcPH4pOy0LbNmny5MmYPn065s6dizNnzsDR0RGffvop4uPjC7k0K23tiYmJqFOnDiZOnFjIZdrl1J6UlITz589j1KhROH/+PH799Vdcv34dLVu2FFCaPW3Pffny5TF37lxERETg+PHjKF26NAICAvDkyZNCLs1K189h27Ztw6lTp+Ds7FxIZdrp0t60aVON9/49e/YUYuG7aWu/desW6tati4oVK+LIkSO4ePEiRo0aBTMzs0IuzZ62/n8+59HR0Vi+fDlUKhW++OKLQi7NSlv7wIEDsXfvXqxduxZXr17FwIED8e2332L79u2FXJpVTu2KoqB169a4ffs2tm/fjvDwcLi5uaFx48Z68buJLr836fM2lijPFNJJzZo1lT59+mgsq1ixovLDDz8IKno/AJStW7eKznhvMTExCgDl6NGjolPeS7FixZSlS5eKztBZfHy8Uq5cOWX//v1K/fr1lQEDBohO0kloaKji7e0tOuO9DB06VKlbt67ojHwxYMAApWzZsoparRadolXz5s2V4OBgjWVt2rRRvvrqK0FFuktKSlIMDQ2VXbt2aSz39vZWRowYIahKN29vk9RqteLo6KhMnDgxc1lKSopiY2OjLFy4UEDhu+W0Pb1z544CQAkPDy/UJl3p8lng9OnTCgDl3r17hROVC7r0v3jxQgGgHDhwoHCidPSu9gcPHiglS5ZULl26pLi5uSkzZswo9DZtsmsPCgpSWrVqJaQnN7Jrb9++vRTv8Yqi22u+VatWSsOGDQsnKBeya/f09FTGjBmjsax69erKyJEjC7FMu7fbr127pgBQLl26lLns1atXiq2trbJkyRIBhTl7+/cmmbaxRO+De27pIC0tDefOnUNAQIDG8oCAAJw4cUJQ1b/TixcvAAC2traCS3InIyMDGzZsQGJiIvz9/UXn6Kxv375o3rw5GjduLDol127cuAFnZ2e4u7ujQ4cOuH37tugknezYsQN+fn5o164dHBwc4OPjgyVLlojOyrW0tDSsXbsWwcHBUKlUonO0qlu3Lg4ePIjr168DAC5evIjjx4+jWbNmgsu0e/XqFTIyMrLsbWBubo7jx48Lqno/d+7cwaNHjzS2t6ampqhfvz63t4XsxYsXUKlUUuw1+ra0tDQsXrwYNjY28Pb2Fp2jlVqtRpcuXTB48GB4enqKzsm1I0eOwMHBAeXLl0evXr0QExMjOkkrtVqN3bt3o3z58mjSpAkcHBzw0UcfSTV1xz89fvwYu3fvRo8ePUSn6KRu3brYsWMHHj58CEVRcPjwYVy/fh1NmjQRnZaj1NRUANDY3hoaGsLExEQvt7dv/97EbSx96Di4pYPY2FhkZGSgRIkSGstLlCiBR48eCar691EUBYMGDULdunVRpUoV0Tk6iYiIQJEiRWBqaoo+ffpg69atqFy5sugsnWzYsAHnz5/HhAkTRKfk2kcffYTVq1dj3759WLJkCR49eoTatWvj6dOnotO0un37NhYsWIBy5cph37596NOnD/r374/Vq1eLTsuVbdu24fnz5+jWrZvoFJ0MHToUHTt2RMWKFWFsbAwfHx+EhISgY8eOotO0srKygr+/P8aOHYuoqChkZGRg7dq1OHXqFKKjo0Xn5cqbbSq3t2KlpKTghx9+QKdOnWBtbS06R2e7du1CkSJFYGZmhhkzZmD//v2wt7cXnaXVpEmTYGRkhP79+4tOybXAwECsW7cOhw4dwrRp03DmzBk0bNgwcxBAX8XExCAhIQETJ05E06ZN8fvvv+Pzzz9HmzZtcPToUdF5ubZq1SpYWVmhTZs2olN0Mnv2bFSuXBkuLi4wMTFB06ZNMX/+fNStW1d0Wo4qVqwINzc3DBs2DHFxcUhLS8PEiRPx6NEjvdveZvd7E7ex9KEzEh0gk7f3PlAURYo9Ej4U/fr1w3//+1+9/GbkXSpUqIALFy7g+fPn2LJlC4KCgnD06FG9H+C6f/8+BgwYgN9//11v5p7IjcDAwMz/9vLygr+/P8qWLYtVq1Zh0KBBAsu0U6vV8PPzQ1hYGADAx8cHly9fxoIFC9C1a1fBdbpbtmwZAgMD9WrumJxs3LgRa9euxfr16+Hp6YkLFy4gJCQEzs7OCAoKEp2n1Zo1axAcHIySJUvC0NAQ1atXR6dOnXD+/HnRae+F21tx0tPT0aFDB6jVasyfP190Tq588sknuHDhAmJjY7FkyRJ8+eWXOHXqFBwcHESnvdO5c+cwa9YsnD9/XsrXePv27TP/u0qVKvDz84Obmxt2796t1wMtb04W0qpVKwwcOBAAUK1aNZw4cQILFy5E/fr1Rebl2vLly9G5c2dpPrPNnj0bJ0+exI4dO+Dm5oY//vgD//nPf+Dk5KTXRwsYGxtjy5Yt6NGjB2xtbWFoaIjGjRtrfO7UFzn93sRtLH2ouOeWDuzt7WFoaJhlRDsmJibLyDcVjG+//RY7duzA4cOH4eLiIjpHZyYmJvDw8ICfnx8mTJgAb29vzJo1S3SWVufOnUNMTAx8fX1hZGQEIyMjHD16FLNnz4aRkREyMjJEJ+aKpaUlvLy89OZsNjlxcnLKMvhZqVIlKU5e8ca9e/dw4MAB9OzZU3SKzgYPHowffvgBHTp0gJeXF7p06YKBAwdKs+di2bJlcfToUSQkJOD+/fs4ffo00tPT4e7uLjotV96c1ZTbWzHS09Px5Zdf4s6dO9i/f79Ue20Br9/rPTw8UKtWLSxbtgxGRkZYtmyZ6KwcHTt2DDExMXB1dc3c3t67dw/fffcdSpcuLTov15ycnODm5qb321t7e3sYGRlJv70FXr+Grl27Js02Nzk5GcOHD8f06dPRokULVK1aFf369UP79u0xdepU0Xla+fr6Zn5xHR0djb179+Lp06d6tb191+9N3MbSh46DWzowMTGBr69v5pm/3ti/fz9q164tqOrfQVEU9OvXD7/++isOHTqkVxuO96Eoit7vqg8AjRo1QkREBC5cuJB58fPzQ+fOnXHhwgUYGhqKTsyV1NRUXL16FU5OTqJTtKpTp06W0zZfv34dbm5ugopyb8WKFXBwcEDz5s1Fp+gsKSkJBgaam0RDQ8PMb/dlYWlpCScnJ8TFxWHfvn1o1aqV6KRccXd3h6Ojo8b2Ni0tDUePHuX2toC9Gdi6ceMGDhw4ADs7O9FJeSbDNrdLly7473//q7G9dXZ2xuDBg7Fv3z7Rebn29OlT3L9/X++3tyYmJqhRo4b021vg9Z7Svr6+UswvB7x+r0lPT5d+m2tjY4PixYvjxo0bOHv2rF5sb7X93sRtLH3oeFiijgYNGoQuXbrAz88P/v7+WLx4MSIjI9GnTx/RaVolJCTg5s2bmX++c+cOLly4AFtbW7i6ugos065v375Yv349tm/fDisrq8xvGmxsbGBubi64LmfDhw9HYGAgSpUqhfj4eGzYsAFHjhzB3r17RadpZWVllWVeM0tLS9jZ2Ukx39n333+PFi1awNXVFTExMRg3bhxevnwpxeFlAwcORO3atREWFoYvv/wSp0+fxuLFi7F48WLRaTpRq9VYsWIFgoKCYGQkzyamRYsWGD9+PFxdXeHp6Ynw8HBMnz4dwcHBotN0sm/fPiiKggoVKuDmzZsYPHgwKlSogO7du4tOy0LbNikkJARhYWEoV64cypUrh7CwMFhYWKBTp04Cq1/T1v7s2TNERkYiKioKADJ/cXZ0dMz8xlyUnNqdnZ3Rtm1bnD9/Hrt27UJGRkbm9tbW1hYmJiaisjPl1G9nZ4fx48ejZcuWcHJywtOnTzF//nw8ePAA7dq1E1j9mrbXzdsDicbGxnB0dESFChUKOzWLnNptbW0xevRofPHFF3BycsLdu3cxfPhw2Nvb4/PPPxdY/Zq2533w4MFo3749Pv74Y3zyySfYu3cvdu7ciSNHjoiL/gddPr+/fPkSv/zyC6ZNmyYqM1va2uvXr4/BgwfD3Nwcbm5uOHr0KFavXo3p06cLrH5NW/svv/yC4sWLw9XVFRERERgwYABat26d5cRjImj7vUmlUun1NpYozwSdpVFK8+bNU9zc3BQTExOlevXqmadV1XeHDx9WAGS5BAUFiU7TKrtuAMqKFStEp2kVHByc+XopXry40qhRI+X3338XnfXe6tevrwwYMEB0hk7at2+vODk5KcbGxoqzs7PSpk0b5fLly6KzdLZz506lSpUqiqmpqVKxYkVl8eLFopN0tm/fPgWAcu3aNdEpufLy5UtlwIABiqurq2JmZqaUKVNGGTFihJKamio6TScbN25UypQpo5iYmCiOjo5K3759lefPn4vOypa2bZJarVZCQ0MVR0dHxdTUVPn444+ViIgIsdH/T1v7ihUrsr0+NDRUaLei5Nx+586dd25vDx8+LDpdUZSc+5OTk5XPP/9ccXZ2VkxMTBQnJyelZcuWyunTp0VnK4qS+89hbm5uyowZMwq18V1yak9KSlICAgKU4sWLK8bGxoqrq6sSFBSkREZGis5WFEW3533ZsmWKh4eHYmZmpnh7eyvbtm0TF/wWXfoXLVqkmJub6937vbb26OhopVu3boqzs7NiZmamVKhQQZk2bZqiVqvFhiva22fNmqW4uLhkvuZHjhypN58VdPm9SZ+3sUR5pVIURXmPMTEiIiIiIiIiIiLhOOcWERERERERERFJi4NbREREREREREQkLQ5uERERERERERGRtDi4RURERERERERE0uLgFhERERERERERSYuDW0REREREREREJC0ObhERERERERERkbQ4uEVERERERERERNLi4BYREZEkRo8ejWrVqmX+uVu3bmjdunWhd9y9excqlQoXLlwosMd4+2d9H7p2Xrt2DY6OjoiPj8/T44kWExOD4sWL4+HDh6JTiIiIiAqVkegAIiIimXXr1g2rVq0CABgZGaFUqVJo06YNfvrpJ1haWhboY8+aNQuKouh027t378Ld3R3h4eF5HjTSRYMGDVCtWjXMnDmzwB8rr0aMGIG+ffvCysoKANC4cWPExsZme9vTp09j4cKFWL58ebbXjxw5En5+fu8cdKxatSpWr16dZfnmzZsxbty4bO8THByMPn36oGbNmtleb29vjwMHDsDBwQFdunRBaGgoli5dmu1tiYiIiD5EHNwiIiLKo6ZNm2LFihVIT0/HsWPH0LNnTyQmJmLBggVZbpueng5jY+N8eVwbG5t8Wc+/2YMHD7Bjxw6NQbiEhIRs9/Zq0KAB1Go1oqKiMHPmTDRo0EDj+pUrVyI2NhYpKSmoVq0aVq5cmWUdtWrVyrYjNjYWISEh6Natm8byI0eOYO/evVCr1ShatCiOHDmS4zq7d++OmjVrYsqUKShWrNi7fmwiIiKiDwoPSyQiIsojU1NTODo6olSpUujUqRM6d+6Mbdu2Afjf4XXLly9HmTJlYGpqCkVR8OLFC/Tu3RsODg6wtrZGw4YNcfHiRY31Tpw4ESVKlICVlRV69OiBlJQUjevfPixRrVZj0qRJ8PDwgKmpKVxdXTF+/HgAgLu7OwDAx8cHKpVKY2BmxYoVqFSpEszMzFCxYkXMnz9f43FOnz4NHx8fmJmZwc/PD+Hh4Xl+zoYOHYry5cvDwsICZcqUwahRo5Cenp7ldosWLUKpUqVgYWGBdu3a4fnz5xrXa2vXZtOmTfD29oaLi0tefhy94eXlBUdHR2zdulV0ChEREVGh4Z5bRERE+czc3FxjoObmzZvYtGkTtmzZAkNDQwBA8+bNYWtriz179sDGxgaLFi1Co0aNcP36ddja2mLTpk0IDQ3FvHnzUK9ePaxZswazZ89GmTJl3vm4w4YNw5IlSzBjxgzUrVsX0dHR+PvvvwG8HqCqWbMmDhw4AE9PT5iYmAAAlixZgtDQUMydOxc+Pj4IDw9Hr169YGlpiaCgICQmJuKzzz5Dw4YNsXbtWty5cwcDBgzI83NkZWWFlStXwtnZGREREejVqxesrKwwZMiQLM/bzp078fLlS/To0QN9+/bFunXrdGrXxR9//AE/P788/zz6pGbNmjh27BiCg4NFpxAREREVCg5uERER5aPTp09j/fr1aNSoUeaytLQ0rFmzBsWLFwcAHDp0CBEREYiJiYGpqSkAYOrUqdi2bRs2b96M3r17Y+bMmQgODkbPnj0BAOPGjcOBAwey7L31Rnx8PGbNmoW5c+dmDuyULVsWdevWBYDMx7azs4Ojo2Pm/caOHYtp06ahTZs2AF7v4XXlyhUsWrQIQUFBWLduHTIyMrB8+XJYWFjA09MTDx48wDfffJOn52nkyJGZ/126dGl899132Lhxo8bgVkpKClatWpW5V9WcOXPQvHlzTJs2DY6OjlrbdXH37l34+vrm6WfRNyVLlsyXveuIiIiIZMHBLSIiojzatWsXihQpglevXiE9PR2tWrXCnDlzMq93c3PLHFwCgHPnziEhIQF2dnYa60lOTsatW7cAAFevXkWfPn00rvf398fhw4ezbbh69SpSU1M1BtW0efLkCe7fv48ePXqgV69emctfvXqVOZ/X1atX4e3tDQsLC42OvNq8eTNmzpyJmzdvIiEhAa9evYK1tbXGbVxdXTUOF/T394darca1a9dgaGiotV0XycnJMDMzy/PPo0/Mzc2RlJQkOoOIiIio0HBwi4iIKI8++eQTLFiwAMbGxnB2ds4yYfzbZ01Uq9VwcnLKdnLwokWLvleDubl5ru+jVqsBvD6876OPPtK47s3hk7qejTE3Tp48iQ4dOuCnn35CkyZNYGNjgw0bNmDatGk53k+lUmX+vy7turC3t0dcXFwufwL99uzZM43BVCIiIqIPHQe3iIiI8sjS0hIeHh4637569ep49OgRjIyMULp06WxvU6lSJZw8eRJdu3bNXHby5Ml3rrNcuXIwNzfHwYMHMw9l/Kc3c2xlZGRkLitRogRKliyJ27dvo3Pnztmut3LlylizZg2Sk5MzB9By6tDFn3/+CTc3N4wYMSJz2b1797LcLjIyElFRUXB2dgYA/PXXXzAwMED58uV1ateFj48Prly58t7310eXLl3KciZHIiIiog8ZB7eIiIgKWePGjeHv74/WrVtj0qRJqFChAqKiorBnzx60bt0afn5+GDBgAIKCguDn54e6deti3bp1uHz58jsnlDczM8PQoUMxZMgQmJiYoE6dOnjy5AkuX76MHj16wMHBAebm5ti7dy9cXFxgZmYGGxsbjB49Gv3794e1tTUCAwORmpqKs2fPIi4uDoMGDUKnTp0wYsQI9OjRAyNHjsTdu3cxdepUnX7OJ0+e4MKFCxrLHB0d4eHhgcjISGzYsAE1atTA7t27sz27n5mZGYKCgjB16lS8fPkS/fv3x5dffpk5Z5i2dl00adIEPXv2REZGRq72+NJXSUlJOHfuHMLCwkSnEBERERUaA9EBRERE/zYqlQp79uzBxx9/jODgYJQvXx4dOnTA3bt3UaJECQBA+/bt8eOPP2Lo0KHw9fXFvXv3tE7iPmrUKHz33Xf48ccfUalSJbRv3x4xMTEAACMjI8yePRuLFi2Cs7MzWrVqBQDo2bMnli5dipUrV8LLywv169fHypUr4e7uDgAoUqQIdu7ciStXrsDHxwcjRozApEmTdPo5169fDx8fH43LwoUL0apVKwwcOBD9+vVDtWrVcOLECYwaNSrL/T08PNCmTRs0a9YMAQEBqFKlCubPn595vbZ2XTRr1gzGxsY4cOCAzvfRZ9u3b4erqyvq1asnOoWIiIio0KiUgphMg4iIiEgS8+fPx/bt27Fv3z4AQK1atbI99LJBgwbYu3cvRo8ejaZNm2Y59G/lypVISUlBgwYNMHHiRKxcuTLLOt617oULF8LMzAzdunXTWH7kyBGNx8xunrZ/rrNmzZoICQlBp06ddPvhiYiIiD4APCyRiIiI/tV69+6NuLg4xMfHw8rKSnTOe4uJiUHbtm3RsWNH0SlEREREhYqDW0RERPSvZmRkpDG5fdGiReHn55ftbQ0MDODi4oLvv/8+2+uHDx8Oc3NzXLp0Kdt1eHl5ZXs/BwcHhIWFYe7cuVmu69atGwwMDJCQkJDtOu3t7TPXMWTIkGzXT0RERPQh42GJREREREREREQkLU4oT0RERERERERE0uLgFhERERERERERSYuDW0REREREREREJC0ObhERERERERERkbQ4uEVERERERERERNLi4BYREREREREREUmLg1tERERERERERCQtDm4REREREREREZG0/g9ukLEpAI3O3QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1600x1400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 1. 혼동 행렬 데이터 생성 (정답지 vs 예측값 비교)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "# 2. 그림 그리기 (Heatmap)\n",
    "plt.figure(figsize=(16, 14)) # 21개 클래스라 그림을 크게 잡아야 잘 보입니다.\n",
    "\n",
    "# annot=True: 칸 안에 숫자 표시\n",
    "# fmt='d': 숫자를 정수(Integer)로 표시 (소수점 안 나오게)\n",
    "# cmap='Blues': 파란색 계열로 표시 (진할수록 개수 많음)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues') \n",
    "\n",
    "plt.xlabel('Predicted Label (모델의 예측)')\n",
    "plt.ylabel('True Label (실제 정답)')\n",
    "plt.title('Confusion Matrix: 모델이 헷갈려하는 부분 찾기')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536bfc5e-81d8-488e-8069-3bc810d884e8",
   "metadata": {},
   "source": [
    "#### 첫번째 xgboost 실행결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5318e44-2801-402a-b4af-1e29e40d5bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 모델 학습 시작... (시간이 조금 걸릴 수 있습니다)\n",
      "학습 완료!\n",
      "\n",
      "🚀 XGBoost 정확도: 0.7988\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.21      0.22       207\n",
      "           1       1.00      1.00      1.00       227\n",
      "           2       1.00      1.00      1.00       236\n",
      "           3       0.47      0.57      0.52       200\n",
      "           4       1.00      1.00      1.00       211\n",
      "           5       1.00      1.00      1.00       211\n",
      "           6       1.00      1.00      1.00       207\n",
      "           7       1.00      1.00      1.00       202\n",
      "           8       0.97      0.97      0.97       197\n",
      "           9       0.19      0.20      0.19       222\n",
      "          10       0.76      0.80      0.78       208\n",
      "          11       0.96      0.89      0.92       200\n",
      "          12       0.94      0.95      0.95       199\n",
      "          13       1.00      0.95      0.98       214\n",
      "          14       0.86      0.79      0.82       233\n",
      "          15       0.22      0.30      0.25       188\n",
      "          16       0.82      0.60      0.69       206\n",
      "          17       1.00      1.00      1.00       186\n",
      "          18       0.99      1.00      0.99       194\n",
      "          19       0.62      0.56      0.59       206\n",
      "          20       0.99      0.99      0.99       185\n",
      "\n",
      "    accuracy                           0.80      4339\n",
      "   macro avg       0.81      0.80      0.80      4339\n",
      "weighted avg       0.81      0.80      0.80      4339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. XGBoost 모델 생성\n",
    "# tree_method='hist': 속도를 빠르게 하는 옵션\n",
    "# device='cuda': 만약 GPU가 있다면 사용 (없으면 자동으로 CPU 사용하니 걱정 마세요)\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,    # 나무를 200개 심겠다 (기본보다 좀 더 많이)\n",
    "    learning_rate=0.1,   # 학습 속도\n",
    "    max_depth=6,         # 나무의 깊이\n",
    "    random_state=42,\n",
    "    n_jobs=-1            # 모든 CPU 사용\n",
    ")\n",
    "\n",
    "print(\"XGBoost 모델 학습 시작... (시간이 조금 걸릴 수 있습니다)\")\n",
    "\n",
    "# 2. 학습 (Train)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"학습 완료!\")\n",
    "\n",
    "# 3. 예측 및 평가\n",
    "y_pred_xgb = xgb_model.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred_xgb)\n",
    "\n",
    "print(f\"\\n🚀 XGBoost 정확도: {acc:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "# 아까 헷갈렸던 1번과 9번의 점수(f1-score)가 올랐는지 확인해보세요!\n",
    "print(classification_report(y_val, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c3f060-1648-4b03-aa4f-332170fdfcc5",
   "metadata": {},
   "source": [
    "#### 제미나이가 추천해준 통계치 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ea41ee-7e3d-4d0e-892c-62fb34808756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 준비 완료!\n",
      "X_train_n의 크기: (17354, 57)\n"
     ]
    }
   ],
   "source": [
    "# 1. 원본 데이터 복사\n",
    "df_eng = train_new.copy()\n",
    "\n",
    "# 2. 파생변수(통계치) 만들기\n",
    "feature_cols = [col for col in df_eng.columns if col != 'target']\n",
    "df_eng['mean'] = df_eng[feature_cols].mean(axis=1)   # 평균\n",
    "df_eng['std'] = df_eng[feature_cols].std(axis=1)     # 표준편차\n",
    "df_eng['max'] = df_eng[feature_cols].max(axis=1)     # 최댓값\n",
    "df_eng['min'] = df_eng[feature_cols].min(axis=1)     # 최솟값\n",
    "df_eng['range'] = df_eng['max'] - df_eng['min']      # 범위\n",
    "\n",
    "# 3. 다시 문제(X)와 정답(y)으로 나누기 (여기서 _n 변수가 탄생합니다!)\n",
    "X_new = df_eng.drop('target', axis=1)\n",
    "y_new = df_eng['target']\n",
    "\n",
    "# 4. 학습용/검증용 나누기\n",
    "X_train_n, X_val_n, y_train_n, y_val_n = train_test_split(\n",
    "    X_new, y_new, test_size=0.2, random_state=42, stratify=y_new\n",
    ")\n",
    "\n",
    "print(\"데이터 준비 완료!\")\n",
    "print(f\"X_train_n의 크기: {X_train_n.shape}\") \n",
    "# (행 개수, 열 개수) -> 열 개수가 아까보다 늘어나 있어야 정상입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39535fbd-0dbf-440a-bd4e-fe037edec181",
   "metadata": {},
   "source": [
    "#### 통계치를 이용한 xgboost 재학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fcb7fce-a582-4c7c-bf27-4b85abec4255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파생변수 추가된 데이터로 학습 중...\n",
      "\n",
      "🚀 파생변수 추가 후 정확도: 0.7896\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.27      0.24       206\n",
      "           1       1.00      1.00      1.00       206\n",
      "           2       1.00      1.00      1.00       207\n",
      "           3       0.49      0.50      0.49       206\n",
      "           4       0.99      0.99      0.99       207\n",
      "           5       1.00      1.00      1.00       206\n",
      "           6       1.00      1.00      1.00       207\n",
      "           7       1.00      1.00      1.00       207\n",
      "           8       0.95      0.94      0.94       206\n",
      "           9       0.19      0.25      0.22       207\n",
      "          10       0.80      0.86      0.83       207\n",
      "          11       0.97      0.88      0.93       206\n",
      "          12       0.92      0.93      0.93       207\n",
      "          13       0.97      0.90      0.94       207\n",
      "          14       0.84      0.76      0.80       207\n",
      "          15       0.23      0.24      0.23       206\n",
      "          16       0.89      0.65      0.75       207\n",
      "          17       1.00      1.00      1.00       206\n",
      "          18       0.99      1.00      0.99       207\n",
      "          19       0.55      0.42      0.48       207\n",
      "          20       1.00      0.99      0.99       207\n",
      "\n",
      "    accuracy                           0.79      4339\n",
      "   macro avg       0.81      0.79      0.80      4339\n",
      "weighted avg       0.81      0.79      0.80      4339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_new = df_eng.drop('target', axis=1)\n",
    "y_new = df_eng['target']\n",
    "\n",
    "# Train/Val 다시 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_n, X_val_n, y_train_n, y_val_n = train_test_split(\n",
    "    X_new, y_new, test_size=0.2, random_state=42, stratify=y_new\n",
    ")\n",
    "\n",
    "# XGBoost 재학습 (파라미터는 아까와 동일)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "xgb_new = XGBClassifier(\n",
    "    n_estimators=300,   # 변수가 늘었으니 공부량을 좀 더 늘림 (200 -> 300)\n",
    "    learning_rate=0.05, # 학습 속도는 조금 더 신중하게 (0.1 -> 0.05)\n",
    "    max_depth=8,        # 깊이도 조금 더 깊게 (6 -> 8)\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"파생변수 추가된 데이터로 학습 중...\")\n",
    "xgb_new.fit(X_train_n, y_train_n)\n",
    "\n",
    "# 평가\n",
    "y_pred_new = xgb_new.predict(X_val_n)\n",
    "print(f\"\\n🚀 파생변수 추가 후 정확도: {accuracy_score(y_val_n, y_pred_new):.4f}\")\n",
    "print(\"-\" * 60)\n",
    "# 특히 0, 9, 15번 점수가 올랐는지 확인하세요!\n",
    "print(classification_report(y_val_n, y_pred_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735c74f7-b1c1-4563-95d5-49023174487a",
   "metadata": {},
   "source": [
    "#### lightGBM 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "454728dd-4777-4e33-8e35-37d1adb4155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 LightGBM 학습 시작...\n",
      "학습 완료!\n",
      "\n",
      "⚡ LightGBM 정확도: 0.7894\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.22      0.21       206\n",
      "           1       1.00      1.00      1.00       206\n",
      "           2       1.00      1.00      1.00       207\n",
      "           3       0.47      0.53      0.50       206\n",
      "           4       0.99      1.00      0.99       207\n",
      "           5       1.00      1.00      1.00       206\n",
      "           6       1.00      1.00      1.00       207\n",
      "           7       1.00      1.00      1.00       207\n",
      "           8       0.98      0.93      0.95       206\n",
      "           9       0.14      0.17      0.16       207\n",
      "          10       0.83      0.87      0.85       207\n",
      "          11       0.97      0.85      0.91       206\n",
      "          12       0.94      0.94      0.94       207\n",
      "          13       0.99      0.98      0.98       207\n",
      "          14       0.85      0.76      0.81       207\n",
      "          15       0.19      0.22      0.20       206\n",
      "          16       0.90      0.66      0.76       207\n",
      "          17       1.00      1.00      1.00       206\n",
      "          18       0.98      1.00      0.99       207\n",
      "          19       0.53      0.44      0.48       207\n",
      "          20       0.99      0.99      0.99       207\n",
      "\n",
      "    accuracy                           0.79      4339\n",
      "   macro avg       0.81      0.79      0.80      4339\n",
      "weighted avg       0.81      0.79      0.80      4339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. LightGBM 모델 정의\n",
    "# boost_from_average=False: 다중 분류에서 불균형이 있거나 클래스가 많을 때 끄는 게 좋을 때가 있음 (에러 방지)\n",
    "lgbm_model = LGBMClassifier(\n",
    "    n_estimators=400,      # 나무 개수 (XGB보다 보통 좀 더 많이 잡습니다)\n",
    "    learning_rate=0.05,\n",
    "    max_depth=-1,          # -1은 제한 없음 (LightGBM의 특징: 깊게 파고듭니다)\n",
    "    num_leaves=31,         # 잎사귀 개수 (LGBM의 핵심 파라미터)\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1             # 불필요한 경고 메시지 끄기\n",
    ")\n",
    "\n",
    "print(\"🚀 LightGBM 학습 시작...\")\n",
    "lgbm_model.fit(X_train_n, y_train_n)\n",
    "print(\"학습 완료!\")\n",
    "\n",
    "# 2. 평가\n",
    "y_pred_lgbm = lgbm_model.predict(X_val_n)\n",
    "\n",
    "print(f\"\\n⚡ LightGBM 정확도: {accuracy_score(y_val_n, y_pred_lgbm):.4f}\")\n",
    "print(\"-\" * 60)\n",
    "# 아까 XGBoost가 어려워했던 0, 9, 15번을 얘가 잘 잡는지 보세요!\n",
    "print(classification_report(y_val_n, y_pred_lgbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dc92071-f9b7-44c7-ba36-621d61e5a6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤝 앙상블(Voting) 모델 학습 중...\n",
      "\n",
      "🏆 [최종] XGBoost + LightGBM 합체 정확도: 0.7937\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.25      0.23       206\n",
      "           1       1.00      1.00      1.00       206\n",
      "           2       1.00      1.00      1.00       207\n",
      "           3       0.47      0.54      0.51       206\n",
      "           4       0.99      1.00      0.99       207\n",
      "           5       1.00      1.00      1.00       206\n",
      "           6       1.00      1.00      1.00       207\n",
      "           7       1.00      1.00      1.00       207\n",
      "           8       0.97      0.94      0.96       206\n",
      "           9       0.17      0.21      0.19       207\n",
      "          10       0.83      0.87      0.85       207\n",
      "          11       0.98      0.86      0.92       206\n",
      "          12       0.95      0.94      0.94       207\n",
      "          13       0.98      0.97      0.98       207\n",
      "          14       0.85      0.78      0.81       207\n",
      "          15       0.22      0.23      0.22       206\n",
      "          16       0.91      0.66      0.77       207\n",
      "          17       1.00      1.00      1.00       206\n",
      "          18       0.98      1.00      0.99       207\n",
      "          19       0.54      0.43      0.48       207\n",
      "          20       0.99      0.99      0.99       207\n",
      "\n",
      "    accuracy                           0.79      4339\n",
      "   macro avg       0.81      0.79      0.80      4339\n",
      "weighted avg       0.81      0.79      0.80      4339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 1. 두 모델을 팀으로 묶습니다.\n",
    "# (아까 학습시킨 xgb_new 모델이 메모리에 있어야 합니다)\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_new),   # 아까 만든 XGBoost 모델\n",
    "        ('lgbm', lgbm_model) # 방금 만든 LightGBM 모델\n",
    "    ],\n",
    "    voting='soft' # hard(다수결)보다 soft(확률 평균)가 성능이 훨씬 좋습니다.\n",
    ")\n",
    "\n",
    "print(\"🤝 앙상블(Voting) 모델 학습 중...\")\n",
    "voting_model.fit(X_train_n, y_train_n)\n",
    "\n",
    "# 2. 최종 평가\n",
    "y_pred_vote = voting_model.predict(X_val_n)\n",
    "\n",
    "print(f\"\\n🏆 [최종] XGBoost + LightGBM 합체 정확도: {accuracy_score(y_val_n, y_pred_vote):.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(y_val_n, y_pred_vote))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d46a84-543d-4c3e-926e-6efaa0a356ba",
   "metadata": {},
   "source": [
    "#### 주성분 분석 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27f8cbc9-d5f1-4c82-b17c-f0d8458411a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PCA 변수(pca_1, pca_2) 추가 완료!\n",
      "🚀 PCA 변수 포함 앙상블 학습 시작...\n",
      "\n",
      "🏆 [PCA 적용] 최종 정확도: 0.7949\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.27      0.24       206\n",
      "           1       1.00      1.00      1.00       206\n",
      "           2       1.00      1.00      1.00       207\n",
      "           3       0.49      0.55      0.52       206\n",
      "           4       0.99      1.00      0.99       207\n",
      "           5       1.00      1.00      1.00       206\n",
      "           6       1.00      1.00      1.00       207\n",
      "           7       1.00      1.00      1.00       207\n",
      "           8       0.97      0.94      0.96       206\n",
      "           9       0.18      0.23      0.20       207\n",
      "          10       0.84      0.86      0.85       207\n",
      "          11       0.97      0.86      0.91       206\n",
      "          12       0.95      0.96      0.95       207\n",
      "          13       0.98      0.95      0.97       207\n",
      "          14       0.86      0.79      0.82       207\n",
      "          15       0.21      0.22      0.22       206\n",
      "          16       0.91      0.66      0.77       207\n",
      "          17       1.00      1.00      1.00       206\n",
      "          18       0.99      1.00      0.99       207\n",
      "          19       0.53      0.42      0.47       207\n",
      "          20       1.00      0.98      0.99       207\n",
      "\n",
      "    accuracy                           0.79      4339\n",
      "   macro avg       0.81      0.79      0.80      4339\n",
      "weighted avg       0.81      0.79      0.80      4339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. 데이터 준비 (아까 만든 파생변수 포함된 데이터 사용)\n",
    "# df_eng는 아까 통계치(mean, std)를 추가한 데이터프레임입니다.\n",
    "df_pca = df_eng.copy()\n",
    "\n",
    "# 2. PCA를 하려면 먼저 데이터 스케일링(정규화)이 필수입니다. (숫자 크기를 맞춤)\n",
    "# Target과 ID 등을 제외한 '순수 입력 변수'만 골라냅니다.\n",
    "features = [col for col in df_pca.columns if col not in ['target', 'ID']]\n",
    "x_scaled = StandardScaler().fit_transform(df_pca[features])\n",
    "\n",
    "# 3. PCA로 2개의 핵심 좌표(Component)만 추출\n",
    "# (데이터의 전체적인 모양을 가장 잘 설명하는 2개의 축을 찾음)\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(x_scaled)\n",
    "\n",
    "# 4. 추출한 좌표를 새로운 변수(힌트)로 추가\n",
    "df_pca['pca_1'] = pca_result[:, 0]\n",
    "df_pca['pca_2'] = pca_result[:, 1]\n",
    "\n",
    "print(\"✅ PCA 변수(pca_1, pca_2) 추가 완료!\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 다시 학습 준비 (PCA 변수가 추가된 df_pca 사용)\n",
    "# -------------------------------------------------------\n",
    "X_pca = df_pca.drop('target', axis=1)\n",
    "y_pca = df_pca['target']\n",
    "\n",
    "# Train/Val 나누기\n",
    "X_train_p, X_val_p, y_train_p, y_val_p = train_test_split(\n",
    "    X_pca, y_pca, test_size=0.2, random_state=42, stratify=y_pca\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 모델링: 이번엔 가장 강력했던 '앙상블(Voting)'로 바로 갑니다.\n",
    "# -------------------------------------------------------\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 모델 재정의 (변수가 늘었으니 새로 선언)\n",
    "xgb_final = XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=8, random_state=42, n_jobs=-1)\n",
    "lgbm_final = LGBMClassifier(n_estimators=400, learning_rate=0.05, num_leaves=31, random_state=42, n_jobs=-1, verbose=-1)\n",
    "\n",
    "voting_final = VotingClassifier(\n",
    "    estimators=[('xgb', xgb_final), ('lgbm', lgbm_final)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "print(\"🚀 PCA 변수 포함 앙상블 학습 시작...\")\n",
    "voting_final.fit(X_train_p, y_train_p)\n",
    "\n",
    "# 평가\n",
    "y_pred_final = voting_final.predict(X_val_p)\n",
    "\n",
    "print(f\"\\n🏆 [PCA 적용] 최종 정확도: {accuracy_score(y_val_p, y_pred_final):.4f}\")\n",
    "print(\"-\" * 60)\n",
    "# 0, 9, 15번이 조금이라도 나아졌는지 확인!\n",
    "print(classification_report(y_val_p, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd43d017-224b-4304-a6e9-eff654e9c487",
   "metadata": {},
   "source": [
    "#### catboost 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c65d17e8-d9a6-4a50-9a9e-d0aff83bfad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.8-cp313-cp313-win_amd64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: graphviz in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from catboost) (3.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from catboost) (2.1.3)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from catboost) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from catboost) (1.15.3)\n",
      "Requirement already satisfied: plotly in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.2.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from plotly->catboost) (9.0.0)\n",
      "Downloading catboost-1.2.8-cp313-cp313-win_amd64.whl (102.4 MB)\n",
      "   ---------------------------------------- 0.0/102.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.4/102.4 MB 11.4 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 4.7/102.4 MB 11.7 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 7.1/102.4 MB 11.3 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 9.4/102.4 MB 11.4 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 11.5/102.4 MB 11.2 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 13.6/102.4 MB 10.8 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 15.5/102.4 MB 10.5 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 18.1/102.4 MB 10.6 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 20.2/102.4 MB 10.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 22.5/102.4 MB 10.6 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 25.2/102.4 MB 10.8 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 27.8/102.4 MB 10.9 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 30.1/102.4 MB 10.9 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 32.8/102.4 MB 11.0 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 35.1/102.4 MB 11.0 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 37.2/102.4 MB 10.9 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 39.6/102.4 MB 10.9 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 41.9/102.4 MB 10.9 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 44.0/102.4 MB 10.8 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 46.9/102.4 MB 10.9 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 49.3/102.4 MB 10.9 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 51.9/102.4 MB 11.0 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 54.3/102.4 MB 11.0 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 56.9/102.4 MB 11.0 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 59.2/102.4 MB 11.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 61.6/102.4 MB 11.0 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 64.0/102.4 MB 11.0 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 66.3/102.4 MB 11.0 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 68.7/102.4 MB 11.0 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 71.0/102.4 MB 11.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 73.7/102.4 MB 11.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 76.0/102.4 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 78.4/102.4 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 80.5/102.4 MB 11.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 82.6/102.4 MB 11.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 84.7/102.4 MB 11.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 87.0/102.4 MB 11.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 89.4/102.4 MB 11.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 91.8/102.4 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 93.8/102.4 MB 11.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 96.5/102.4 MB 11.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 98.8/102.4 MB 11.0 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.4/102.4 MB 11.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  102.2/102.4 MB 11.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 102.4/102.4 MB 10.7 MB/s eta 0:00:00\n",
      "Installing collected packages: catboost\n",
      "Successfully installed catboost-1.2.8\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4706dac0-8db9-4891-8fa7-3085e5901303",
   "metadata": {},
   "source": [
    "#### catboost 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a01a23d2-91b1-4ed1-8a16-72feedfc1a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚖️ 0, 9, 15번 클래스 집중 공략 설정 완료!\n",
      "🐱 CatBoost 학습 시작...\n",
      "0:\tlearn: 2.7808211\ttest: 2.7812198\tbest: 2.7812198 (0)\ttotal: 705ms\tremaining: 5m 51s\n",
      "100:\tlearn: 0.9869290\ttest: 1.0408053\tbest: 1.0408053 (100)\ttotal: 54.8s\tremaining: 3m 36s\n",
      "200:\tlearn: 0.7737142\ttest: 0.9094393\tbest: 0.9094393 (200)\ttotal: 1m 55s\tremaining: 2m 52s\n",
      "300:\tlearn: 0.6168212\ttest: 0.8475150\tbest: 0.8475150 (300)\ttotal: 3m\tremaining: 1m 59s\n",
      "400:\tlearn: 0.4892290\ttest: 0.8259216\tbest: 0.8259216 (400)\ttotal: 4m 3s\tremaining: 1m\n",
      "499:\tlearn: 0.3943321\ttest: 0.8151487\tbest: 0.8151487 (499)\ttotal: 5m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.8151487358\n",
      "bestIteration = 499\n",
      "\n",
      "\n",
      "🚀 CatBoost (가중치 적용) 정확도: 0.7460\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.26      0.21       206\n",
      "           1       1.00      1.00      1.00       206\n",
      "           2       1.00      1.00      1.00       207\n",
      "           3       0.40      0.28      0.33       206\n",
      "           4       0.96      1.00      0.98       207\n",
      "           5       1.00      1.00      1.00       206\n",
      "           6       1.00      1.00      1.00       207\n",
      "           7       1.00      1.00      1.00       207\n",
      "           8       0.94      0.91      0.93       206\n",
      "           9       0.16      0.26      0.20       207\n",
      "          10       0.70      0.77      0.73       207\n",
      "          11       0.98      0.76      0.86       206\n",
      "          12       0.90      0.90      0.90       207\n",
      "          13       0.97      0.90      0.93       207\n",
      "          14       0.78      0.70      0.74       207\n",
      "          15       0.17      0.26      0.20       206\n",
      "          16       0.96      0.43      0.60       207\n",
      "          17       1.00      1.00      1.00       206\n",
      "          18       0.95      0.99      0.97       207\n",
      "          19       0.58      0.28      0.38       207\n",
      "          20       0.99      0.96      0.98       207\n",
      "\n",
      "    accuracy                           0.75      4339\n",
      "   macro avg       0.79      0.75      0.76      4339\n",
      "weighted avg       0.79      0.75      0.76      4339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# 1. \"어려운 문제\"에 가중치 부여하기\n",
    "# (0, 9, 15번 처럼 못 맞추는 클래스에 더 집중하게 만듭니다)\n",
    "classes = np.unique(y_train_p)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_p)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "# 0, 9, 15번은 특별 관리 대상이므로 가중치를 수동으로 더 높여줍니다. (강제 주입)\n",
    "# 기존 가중치보다 2배 더 신경 쓰라고 지시합니다.\n",
    "for c in [0, 9, 15]:\n",
    "    class_weights[c] *= 2.0 \n",
    "\n",
    "print(\"⚖️ 0, 9, 15번 클래스 집중 공략 설정 완료!\")\n",
    "\n",
    "# 2. CatBoost 모델 선언\n",
    "# verbose=0: 학습 과정 출력 생략 (깔끔하게)\n",
    "cat_model = CatBoostClassifier(\n",
    "    iterations=500,          # 반복 횟수\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    loss_function='MultiClass',\n",
    "    class_weights=class_weights, # 여기에 가중치 적용!\n",
    "    random_seed=42,\n",
    "    verbose=100              # 100번마다 로그 출력\n",
    ")\n",
    "\n",
    "print(\"🐱 CatBoost 학습 시작...\")\n",
    "# 아까 만든 파생변수+PCA 데이터(X_train_p)를 그대로 씁니다.\n",
    "cat_model.fit(X_train_p, y_train_p, eval_set=(X_val_p, y_val_p))\n",
    "\n",
    "# 3. 평가\n",
    "y_pred_cat = cat_model.predict(X_val_p)\n",
    "\n",
    "print(f\"\\n🚀 CatBoost (가중치 적용) 정확도: {accuracy_score(y_val_p, y_pred_cat):.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(y_val_p, y_pred_cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37380ffa-affe-4660-8600-2d8ddfe1954e",
   "metadata": {},
   "source": [
    "#### autogluon 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84a41ca1-79e5-4f92-a4a1-c6bd236c115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autogluon\n",
      "  Downloading autogluon-1.5.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting autogluon.core==1.5.0 (from autogluon.core[all]==1.5.0->autogluon)\n",
      "  Downloading autogluon_core-1.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting autogluon.features==1.5.0 (from autogluon)\n",
      "  Downloading autogluon_features-1.5.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting autogluon.tabular==1.5.0 (from autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading autogluon_tabular-1.5.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting autogluon.multimodal==1.5.0 (from autogluon)\n",
      "  Downloading autogluon_multimodal-1.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting autogluon.timeseries==1.5.0 (from autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading autogluon_timeseries-1.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2.4.0,>=1.25.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (2.1.3)\n",
      "Requirement already satisfied: scipy<1.17,>=1.5.4 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn<1.8.0,>=1.4.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (1.6.1)\n",
      "Requirement already satisfied: networkx<4,>=3.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (3.4.2)\n",
      "Requirement already satisfied: pandas<2.4.0,>=2.0.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (2.2.3)\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (2.32.3)\n",
      "Requirement already satisfied: matplotlib<3.11,>=3.7.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (3.10.0)\n",
      "Collecting boto3<2,>=1.10 (from autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon)\n",
      "  Downloading boto3-1.42.28-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting autogluon.common==1.5.0 (from autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon)\n",
      "  Downloading autogluon_common-1.5.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pyarrow<21.0.0,>=7.0.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.common==1.5.0->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (19.0.0)\n",
      "Requirement already satisfied: psutil<7.2.0,>=5.7.3 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.common==1.5.0->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (5.9.0)\n",
      "Requirement already satisfied: joblib<1.7,>=1.2 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.common==1.5.0->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.common==1.5.0->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (6.0.2)\n",
      "Collecting hyperopt<0.2.8,>=0.2.7 (from autogluon.core[all]==1.5.0->autogluon)\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting stevedore<5.5 (from autogluon.core[all]==1.5.0->autogluon)\n",
      "  Downloading stevedore-5.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: Pillow<12,>=10.0.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.multimodal==1.5.0->autogluon) (11.1.0)\n",
      "Collecting torch<2.10,>=2.6 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading torch-2.9.1-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Collecting lightning<2.6,>=2.5.1 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading lightning-2.5.6-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting transformers<4.58,>=4.51.0 (from transformers[sentencepiece]<4.58,>=4.51.0->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading transformers-4.57.5-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting accelerate<2.0,>=0.34.0 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting fsspec<=2025.3 (from fsspec[http]<=2025.3->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: jsonschema<4.24,>=4.18 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.multimodal==1.5.0->autogluon) (4.23.0)\n",
      "Collecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting evaluate<0.5.0,>=0.4.0 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting timm<1.0.7,>=0.9.5 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading timm-1.0.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting torchvision<0.25.0,>=0.21.0 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading torchvision-0.24.1-cp313-cp313-win_amd64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: scikit-image<0.26.0,>=0.19.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.multimodal==1.5.0->autogluon) (0.25.0)\n",
      "Requirement already satisfied: text-unidecode<1.4,>=1.3 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.multimodal==1.5.0->autogluon) (1.3)\n",
      "Collecting torchmetrics<1.8,>=1.2.0 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading torchmetrics-1.7.4-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting omegaconf<2.4.0,>=2.1.1 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pytorch-metric-learning<2.9,>=1.3.0 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: nltk<3.10,>=3.4.5 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.multimodal==1.5.0->autogluon) (3.9.1)\n",
      "Collecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: defusedxml<0.7.2,>=0.7.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.multimodal==1.5.0->autogluon) (0.7.1)\n",
      "Requirement already satisfied: jinja2<3.2,>=3.0.3 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.multimodal==1.5.0->autogluon) (3.1.6)\n",
      "Collecting tensorboard<3,>=2.9 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pytesseract<0.4,>=0.3.9 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-ml-py3<8.0,>=7.352.0 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pdf2image<1.19,>=1.17.0 (from autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting einx (from autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: lightgbm<4.7,>=4.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.tabular[all]==1.5.0->autogluon) (4.6.0)\n",
      "Collecting huggingface_hub<1.0 (from huggingface_hub[torch]<1.0; extra == \"all\"->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: xgboost<3.2,>=2.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.tabular[all]==1.5.0->autogluon) (3.0.1)\n",
      "Requirement already satisfied: catboost<1.3,>=1.2 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from autogluon.tabular[all]==1.5.0->autogluon) (1.2.8)\n",
      "Collecting fastai<2.9,>=2.3.1 (from autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading fastai-2.8.6-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting loguru (from autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting einops<0.9,>=0.7 (from autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting spacy<3.9 (from autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading spacy-3.8.11-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Collecting gluonts<0.17,>=0.15.0 (from autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading gluonts-0.16.2-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting statsforecast<2.0.2,>=1.7.0 (from autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading statsforecast-2.0.1.tar.gz (2.9 MB)\n",
      "     ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "     -------------------------------- ------- 2.4/2.9 MB 11.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.9/2.9 MB 9.7 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting mlforecast<0.15.0,>=0.14.0 (from autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading mlforecast-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting utilsforecast<0.2.12,>=0.2.3 (from autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading utilsforecast-0.2.11-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting coreforecast<0.0.17,>=0.0.12 (from autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading coreforecast-0.0.16-cp313-cp313-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting fugue>=0.9.0 (from autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading fugue-0.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting orjson~=3.9 (from autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading orjson-3.11.5-cp313-cp313-win_amd64.whl.metadata (42 kB)\n",
      "Collecting chronos-forecasting<2.4,>=2.2.2 (from autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading chronos_forecasting-2.2.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting peft<0.18,>=0.13.0 (from autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.5.0->autogluon) (24.2)\n",
      "Collecting safetensors>=0.4.3 (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting botocore<1.43.0,>=1.42.28 (from boto3<2,>=1.10->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon)\n",
      "  Downloading botocore-1.42.28-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from boto3<2,>=1.10->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (1.0.1)\n",
      "Collecting s3transfer<0.17.0,>=0.16.0 (from boto3<2,>=1.10->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon)\n",
      "  Downloading s3transfer-0.16.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from botocore<1.43.0,>=1.42.28->boto3<2,>=1.10->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from botocore<1.43.0,>=1.42.28->boto3<2,>=1.10->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (2.3.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.5.0->autogluon) (0.21)\n",
      "Requirement already satisfied: plotly in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.5.0->autogluon) (5.24.1)\n",
      "Requirement already satisfied: six in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.5.0->autogluon) (1.17.0)\n",
      "Collecting datasets>=2.0.0 (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: dill in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.5.0->autogluon) (0.3.8)\n",
      "Collecting xxhash (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading multiprocess-0.70.18-py313-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pip in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon) (25.1)\n",
      "Collecting fastdownload<2,>=0.0.5 (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading fastdownload-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting fastcore>=1.8.0 (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading fastcore-1.11.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting fasttransform>=0.0.2 (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading fasttransform-0.0.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting fastprogress>=0.2.4 (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading fastprogress-1.1.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting plum-dispatch (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading plum_dispatch-2.6.1-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon) (3.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3->autogluon.multimodal==1.5.0->autogluon) (3.11.10)\n",
      "Requirement already satisfied: pydantic<3,>=1.7 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon) (2.10.3)\n",
      "Collecting toolz~=0.10 (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0->huggingface_hub[torch]<1.0; extra == \"all\"->autogluon.tabular[all]==1.5.0->autogluon) (3.17.0)\n",
      "Collecting future (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.5.0->autogluon)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting py4j (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.5.0->autogluon)\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.5.0->autogluon) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.5.0->autogluon) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.5.0->autogluon) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.5.0->autogluon) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.5.0->autogluon) (0.22.3)\n",
      "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2.6,>=2.5.1->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting pytorch-lightning (from lightning<2.6,>=2.5.1->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2.6,>=2.5.1->autogluon.multimodal==1.5.0->autogluon) (72.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (3.2.0)\n",
      "Requirement already satisfied: numba in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon) (0.61.0)\n",
      "Collecting optuna (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting window-ops (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading window_ops-0.0.15-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting gdown>=4.0.0 (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading gdown-5.2.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: click in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from nltk<3.10,>=3.4.5->autogluon.multimodal==1.5.0->autogluon) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from nltk<3.10,>=3.4.5->autogluon.multimodal==1.5.0->autogluon) (2024.11.6)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<2.4.0,>=2.1.1->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: colorama in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.5.0->autogluon) (0.4.6)\n",
      "Collecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: rich in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.5.0->autogluon) (13.9.4)\n",
      "Requirement already satisfied: tabulate in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.5.0->autogluon) (0.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from pandas<2.4.0,>=2.0.0->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from pandas<2.4.0,>=2.0.0->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from requests->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from requests->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from requests->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (2026.1.4)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.5.0->autogluon) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.5.0->autogluon) (2025.2.18)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.5.0->autogluon) (0.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from scikit-learn<1.8.0,>=1.4.0->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon) (3.5.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading murmurhash-1.0.15-cp313-cp313-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading cymem-2.0.13-cp313-cp313-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading preshed-3.0.12-cp313-cp313-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading thinc-8.3.10-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading srsly-2.5.2-cp313-cp313-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: statsmodels>=0.13.2 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon) (0.14.4)\n",
      "Collecting pbr>=2.0.0 (from stevedore<5.5->autogluon.core[all]==1.5.0->autogluon)\n",
      "  Downloading pbr-7.0.3-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard<3,>=2.9->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard<3,>=2.9->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading grpcio-1.76.0-cp313-cp313-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.5.0->autogluon) (3.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.5.0->autogluon) (5.29.3)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<3,>=2.9->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.5.0->autogluon) (3.1.3)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading blis-1.3.3-cp313-cp313-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from torch<2.10,>=2.6->autogluon.multimodal==1.5.0->autogluon) (1.13.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<4.58,>=4.51.0->transformers[sentencepiece]<4.58,>=4.51.0->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]<4.58,>=4.51.0->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy<3.9->autogluon.tabular[all]==1.5.0->autogluon) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.5.0->autogluon) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.5.0->autogluon) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.5.0->autogluon) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.5.0->autogluon) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.5.0->autogluon) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.5.0->autogluon) (1.18.0)\n",
      "INFO: pip is looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting datasets>=2.0.0 (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading datasets-4.4.0-py3-none-any.whl.metadata (19 kB)\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading datasets-4.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading datasets-4.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "INFO: pip is still looking at multiple versions of datasets to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting multiprocess (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting python-fasthtml>=0.12.34 (from fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading python_fasthtml-0.12.39-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting triad>=1.0.0 (from fugue>=0.9.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading triad-1.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting adagio>=0.2.6 (from fugue>=0.9.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading adagio-0.2.6-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.5.0->autogluon) (4.12.3)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from numba->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon) (0.44.0)\n",
      "Collecting starlette>0.33 (from python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading starlette-0.51.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting oauthlib (from python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: itsdangerous in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon) (2.2.0)\n",
      "Collecting uvicorn>=0.30 (from uvicorn[standard]>=0.30->python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading uvicorn-0.40.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: httpx in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon) (0.28.1)\n",
      "Collecting fastlite>=0.1.1 (from python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading fastlite-0.2.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting python-multipart (from python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading python_multipart-0.0.21-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting apswutils>=0.1.2 (from fastlite>=0.1.1->python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading apswutils-0.1.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting apsw (from apswutils>=0.1.2->fastlite>=0.1.1->python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading apsw-3.51.2.0-cp313-cp313-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from starlette>0.33->python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon) (4.7.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette>0.33->python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon) (1.3.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from statsmodels>=0.13.2->statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon) (1.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch<2.10,>=2.6->autogluon.multimodal==1.5.0->autogluon) (1.3.0)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from uvicorn>=0.30->uvicorn[standard]>=0.30->python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon) (0.16.0)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.30->python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading httptools-0.7.1-cp313-cp313-win_amd64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.30->python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon) (1.1.0)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.30->python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading watchfiles-1.1.1-cp313-cp313-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.30->python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading websockets-16.0-cp313-cp313-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.5.0->autogluon) (2.5)\n",
      "Requirement already satisfied: frozendict in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from einx->autogluon.tabular[all]==1.5.0->autogluon) (2.4.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from httpx->python-fasthtml>=0.12.34->fastprogress>=0.2.4->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon) (1.0.9)\n",
      "Collecting win32-setctime>=1.0.0 (from loguru->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading win32_setctime-1.2.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting ordered-set (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pycryptodome (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading pycryptodome-3.23.0-cp37-abi3-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading openxlab-0.1.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.5.0->autogluon) (308)\n",
      "Collecting filelock (from huggingface_hub<1.0->huggingface_hub[torch]<1.0; extra == \"all\"->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting oss2~=2.17.0 (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading oss2-2.17.0.tar.gz (259 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pytz>=2020.1 (from pandas<2.4.0,>=2.0.0->autogluon.core==1.5.0->autogluon.core[all]==1.5.0->autogluon)\n",
      "  Downloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\n",
      "INFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.5.0->autogluon)\n",
      "  Downloading openxlab-0.1.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading openxlab-0.1.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading openxlab-0.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading openxlab-0.0.38-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading openxlab-0.0.37-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading openxlab-0.0.36-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading openxlab-0.0.35-py3-none-any.whl.metadata (3.8 kB)\n",
      "INFO: pip is still looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading openxlab-0.0.34-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading openxlab-0.0.33-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading openxlab-0.0.32-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading openxlab-0.0.31-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading openxlab-0.0.30-py3-none-any.whl.metadata (3.8 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading openxlab-0.0.29-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading openxlab-0.0.28-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading openxlab-0.0.27-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading openxlab-0.0.26-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading openxlab-0.0.25-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading openxlab-0.0.24-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading openxlab-0.0.23-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading openxlab-0.0.22-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading openxlab-0.0.21-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading openxlab-0.0.20-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading openxlab-0.0.19-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading openxlab-0.0.18-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading openxlab-0.0.17-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading openxlab-0.0.16-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading openxlab-0.0.15-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading openxlab-0.0.14-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading openxlab-0.0.13-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Downloading openxlab-0.0.12-py3-none-any.whl.metadata (4.5 kB)\n",
      "  Downloading openxlab-0.0.11-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon) (1.16.5)\n",
      "Collecting colorlog (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon) (2.0.39)\n",
      "Requirement already satisfied: Mako in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon) (1.3.10)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.5.0->autogluon.timeseries[all]==1.5.0->autogluon) (3.1.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from plotly->catboost<1.3,>=1.2->autogluon.tabular[all]==1.5.0->autogluon) (9.0.0)\n",
      "Collecting beartype>=0.16.2 (from plum-dispatch->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.5.0->autogluon)\n",
      "  Downloading beartype-0.22.9-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.5.0->autogluon) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.5.0->autogluon) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.5.0->autogluon) (0.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\hsk20\\anaconda3\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.5.0->autogluon) (1.7.1)\n",
      "Downloading autogluon-1.5.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading autogluon_core-1.5.0-py3-none-any.whl (227 kB)\n",
      "Downloading autogluon_common-1.5.0-py3-none-any.whl (74 kB)\n",
      "Downloading autogluon_features-1.5.0-py3-none-any.whl (98 kB)\n",
      "Downloading autogluon_multimodal-1.5.0-py3-none-any.whl (452 kB)\n",
      "Downloading autogluon_tabular-1.5.0-py3-none-any.whl (515 kB)\n",
      "Downloading autogluon_timeseries-1.5.0-py3-none-any.whl (244 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading boto3-1.42.28-py3-none-any.whl (140 kB)\n",
      "Downloading botocore-1.42.28-py3-none-any.whl (14.6 MB)\n",
      "   ---------------------------------------- 0.0/14.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.4/14.6 MB 11.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 5.0/14.6 MB 12.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 7.6/14.6 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.2/14.6 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.6/14.6 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.6/14.6 MB 11.3 MB/s eta 0:00:00\n",
      "Downloading chronos_forecasting-2.2.2-py3-none-any.whl (72 kB)\n",
      "Downloading coreforecast-0.0.16-cp313-cp313-win_amd64.whl (209 kB)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "Downloading fastai-2.8.6-py3-none-any.whl (235 kB)\n",
      "Downloading fastdownload-0.0.7-py3-none-any.whl (12 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading gluonts-0.16.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.5 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 7.0 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 7.6 MB/s eta 0:00:00\n",
      "Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.6/1.6 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 6.3 MB/s eta 0:00:00\n",
      "Downloading lightning-2.5.6-py3-none-any.whl (827 kB)\n",
      "   ---------------------------------------- 0.0/827.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 827.9/827.9 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Downloading mlforecast-0.14.0-py3-none-any.whl (71 kB)\n",
      "Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Downloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n",
      "Downloading orjson-3.11.5-cp313-cp313-win_amd64.whl (133 kB)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n",
      "Downloading s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
      "Downloading spacy-3.8.11-cp313-cp313-win_amd64.whl (14.2 MB)\n",
      "   ---------------------------------------- 0.0/14.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.6/14.2 MB 12.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 5.0/14.2 MB 12.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.6/14.2 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.2/14.2 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.8/14.2 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.2/14.2 MB 11.6 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Downloading murmurhash-1.0.15-cp313-cp313-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.12-cp313-cp313-win_amd64.whl (117 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp313-cp313-win_amd64.whl (653 kB)\n",
      "   ---------------------------------------- 0.0/653.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 653.1/653.1 kB 7.4 MB/s eta 0:00:00\n",
      "Downloading stevedore-5.4.1-py3-none-any.whl (49 kB)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 2.6/5.5 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.5 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading thinc-8.3.10-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 8.7 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.3-cp313-cp313-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 2.1/6.2 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.2 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 10.6 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading timm-1.0.3-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 2.1/2.3 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 10.0 MB/s eta 0:00:00\n",
      "Downloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "Downloading torch-2.9.1-cp313-cp313-win_amd64.whl (110.9 MB)\n",
      "   ---------------------------------------- 0.0/110.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.6/110.9 MB 12.2 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 5.0/110.9 MB 11.6 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 7.3/110.9 MB 11.7 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 10.0/110.9 MB 11.7 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 12.1/110.9 MB 11.4 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 14.7/110.9 MB 11.5 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 17.3/110.9 MB 11.7 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 19.9/110.9 MB 11.7 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 22.3/110.9 MB 11.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 24.6/110.9 MB 11.5 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 27.0/110.9 MB 11.5 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 29.4/110.9 MB 11.5 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 32.0/110.9 MB 11.5 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 34.3/110.9 MB 11.5 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 36.7/110.9 MB 11.5 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 39.1/110.9 MB 11.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 41.7/110.9 MB 11.5 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 44.3/110.9 MB 11.6 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 46.7/110.9 MB 11.5 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 49.3/110.9 MB 11.6 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 52.2/110.9 MB 11.6 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 54.3/110.9 MB 11.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 56.1/110.9 MB 11.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 58.2/110.9 MB 11.3 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 60.8/110.9 MB 11.4 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 63.4/110.9 MB 11.4 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 66.1/110.9 MB 11.4 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 68.7/110.9 MB 11.5 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 71.3/110.9 MB 11.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 73.9/110.9 MB 11.5 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 76.5/110.9 MB 11.5 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 79.2/110.9 MB 11.6 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 82.1/110.9 MB 11.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 84.7/110.9 MB 11.6 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 86.5/110.9 MB 11.5 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 89.1/110.9 MB 11.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 91.5/110.9 MB 11.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 93.3/110.9 MB 11.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 95.9/110.9 MB 11.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 98.3/110.9 MB 11.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 100.9/110.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 103.3/110.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 105.6/110.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 108.0/110.9 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.6/110.9 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  110.9/110.9 MB 11.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 110.9/110.9 MB 11.1 MB/s eta 0:00:00\n",
      "Downloading torchmetrics-1.7.4-py3-none-any.whl (963 kB)\n",
      "   ---------------------------------------- 0.0/963.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 963.5/963.5 kB 8.5 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.24.1-cp313-cp313-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 2.4/4.3 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 10.5 MB/s eta 0:00:00\n",
      "Downloading transformers-4.57.5-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.6/12.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.0/12.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.1/12.0 MB 11.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.9/12.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 1.8/2.7 MB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Downloading utilsforecast-0.2.11-py3-none-any.whl (41 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading fastcore-1.11.4-py3-none-any.whl (89 kB)\n",
      "Downloading fastprogress-1.1.3-py3-none-any.whl (14 kB)\n",
      "Downloading fasttransform-0.0.2-py3-none-any.whl (14 kB)\n",
      "Downloading fugue-0.9.4-py3-none-any.whl (280 kB)\n",
      "Downloading adagio-0.2.6-py3-none-any.whl (19 kB)\n",
      "Downloading gdown-5.2.1-py3-none-any.whl (18 kB)\n",
      "Downloading grpcio-1.76.0-cp313-cp313-win_amd64.whl (4.7 MB)\n",
      "   ---------------------------------------- 0.0/4.7 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 2.1/4.7 MB 11.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 4.2/4.7 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.7/4.7 MB 9.0 MB/s eta 0:00:00\n",
      "Downloading pbr-7.0.3-py2.py3-none-any.whl (131 kB)\n",
      "Downloading python_fasthtml-0.12.39-py3-none-any.whl (73 kB)\n",
      "Downloading fastlite-0.2.4-py3-none-any.whl (17 kB)\n",
      "Downloading apswutils-0.1.2-py3-none-any.whl (48 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 8.4 MB/s eta 0:00:00\n",
      "Downloading starlette-0.51.0-py3-none-any.whl (74 kB)\n",
      "Downloading triad-1.0.0-py3-none-any.whl (59 kB)\n",
      "Downloading uvicorn-0.40.0-py3-none-any.whl (68 kB)\n",
      "Downloading httptools-0.7.1-cp313-cp313-win_amd64.whl (85 kB)\n",
      "Downloading watchfiles-1.1.1-cp313-cp313-win_amd64.whl (288 kB)\n",
      "Downloading websockets-16.0-cp313-cp313-win_amd64.whl (178 kB)\n",
      "Downloading apsw-3.51.2.0-cp313-cp313-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 8.9 MB/s eta 0:00:00\n",
      "Downloading einx-0.3.0-py3-none-any.whl (102 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
      "Downloading win32_setctime-1.2.0-py3-none-any.whl (4.1 kB)\n",
      "Downloading model_index-0.1.11-py3-none-any.whl (34 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\n",
      "Downloading openxlab-0.0.11-py3-none-any.whl (55 kB)\n",
      "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
      "Downloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "Downloading plum_dispatch-2.6.1-py3-none-any.whl (41 kB)\n",
      "Downloading beartype-0.22.9-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 9.8 MB/s eta 0:00:00\n",
      "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Downloading pycryptodome-3.23.0-cp37-abi3-win_amd64.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/1.8 MB 8.3 MB/s eta 0:00:00\n",
      "Downloading python_multipart-0.0.21-py3-none-any.whl (24 kB)\n",
      "Downloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n",
      "   ---------------------------------------- 0.0/849.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 849.5/849.5 kB 7.8 MB/s eta 0:00:00\n",
      "Downloading window_ops-0.0.15-py3-none-any.whl (15 kB)\n",
      "Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl (31 kB)\n",
      "Building wheels for collected packages: nvidia-ml-py3, antlr4-python3-runtime, seqeval, statsforecast\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): started\n",
      "  Building wheel for nvidia-ml-py3 (setup.py): finished with status 'done'\n",
      "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19181 sha256=1cf13e31fcaf724bfa5421372b4b6712a0a1cae72cac077a33f93d3246378b7f\n",
      "  Stored in directory: c:\\users\\hsk20\\appdata\\local\\pip\\cache\\wheels\\ea\\47\\38\\29179ca914d95f79296647a42943b8e576dc9d318f94bad57a\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144577 sha256=f18a597f63313a728d79bd5c69f53644841ab94c1298e99821421bfb27dbb8e6\n",
      "  Stored in directory: c:\\users\\hsk20\\appdata\\local\\pip\\cache\\wheels\\d5\\b3\\74\\a35b66048c9de6631cd74cbc9475e6feb3e69a467983446bd8\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16185 sha256=864204c86656c03f29584827cec34dac521570a8a55a2e2e67ef1bbc3bd6ec3f\n",
      "  Stored in directory: c:\\users\\hsk20\\appdata\\local\\pip\\cache\\wheels\\14\\cf\\a7\\8f28ef376d707ff10e3922899482a2f23ef3002f8a952f47ac\n",
      "  Building wheel for statsforecast (pyproject.toml): started\n",
      "  Building wheel for statsforecast (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for statsforecast: filename=statsforecast-2.0.1-cp313-cp313-win_amd64.whl size=285079 sha256=92d308ad7d502efa9b0dc6ce220df3aa282be05c7f4676e3b3f6e6a55d21328e\n",
      "  Stored in directory: c:\\users\\hsk20\\appdata\\local\\pip\\cache\\wheels\\4e\\dd\\74\\5c66f9a6e2b04f3f54eff7209d5b8d4e3a9d726afc25b1896e\n",
      "Successfully built nvidia-ml-py3 antlr4-python3-runtime seqeval statsforecast\n",
      "Installing collected packages: py4j, nvidia-ml-py3, antlr4-python3-runtime, xxhash, win32-setctime, websockets, wasabi, toolz, tensorboard-data-server, spacy-loggers, spacy-legacy, smart-open, sentencepiece, safetensors, python-multipart, pytesseract, pycryptodome, pdf2image, pbr, orjson, ordered-set, openxlab, omegaconf, oauthlib, murmurhash, multiprocess, lightning-utilities, httptools, grpcio, future, fsspec, fastcore, einops, cymem, coreforecast, colorlog, cloudpathlib, catalogue, blis, beartype, apsw, absl-py, window-ops, watchfiles, uvicorn, typer-slim, torch, tensorboard, stevedore, starlette, srsly, preshed, model-index, loguru, hyperopt, huggingface_hub, einx, botocore, apswutils, utilsforecast, triad, torchvision, torchmetrics, tokenizers, seqeval, s3transfer, pytorch-metric-learning, plum-dispatch, optuna, opendatalab, gluonts, gdown, fastlite, confection, accelerate, weasel, transformers, timm, thinc, pytorch-lightning, python-fasthtml, openmim, nlpaug, mlforecast, fasttransform, datasets, boto3, adagio, spacy, peft, lightning, fugue, fastprogress, evaluate, chronos-forecasting, autogluon.common, statsforecast, fastdownload, autogluon.features, autogluon.core, fastai, autogluon.tabular, autogluon.multimodal, autogluon.timeseries, autogluon\n",
      "\n",
      "   ----------------------------------------   0/105 [py4j]\n",
      "   ----------------------------------------   0/105 [py4j]\n",
      "    ---------------------------------------   2/105 [antlr4-python3-runtime]\n",
      "    ---------------------------------------   2/105 [antlr4-python3-runtime]\n",
      "    ---------------------------------------   2/105 [antlr4-python3-runtime]\n",
      "    ---------------------------------------   2/105 [antlr4-python3-runtime]\n",
      "   - --------------------------------------   5/105 [websockets]\n",
      "   - --------------------------------------   5/105 [websockets]\n",
      "   - --------------------------------------   5/105 [websockets]\n",
      "   - --------------------------------------   5/105 [websockets]\n",
      "   -- -------------------------------------   6/105 [wasabi]\n",
      "  Attempting uninstall: toolz\n",
      "   -- -------------------------------------   6/105 [wasabi]\n",
      "    Found existing installation: toolz 1.0.0\n",
      "   -- -------------------------------------   6/105 [wasabi]\n",
      "   -- -------------------------------------   7/105 [toolz]\n",
      "    Uninstalling toolz-1.0.0:\n",
      "   -- -------------------------------------   7/105 [toolz]\n",
      "      Successfully uninstalled toolz-1.0.0\n",
      "   -- -------------------------------------   7/105 [toolz]\n",
      "   -- -------------------------------------   7/105 [toolz]\n",
      "   -- -------------------------------------   7/105 [toolz]\n",
      "   --- ------------------------------------   8/105 [tensorboard-data-server]\n",
      "   --- ------------------------------------  10/105 [spacy-legacy]\n",
      "   --- ------------------------------------  10/105 [spacy-legacy]\n",
      "   ---- -----------------------------------  11/105 [smart-open]\n",
      "   ---- -----------------------------------  12/105 [sentencepiece]\n",
      "   ---- -----------------------------------  13/105 [safetensors]\n",
      "   ------ ---------------------------------  16/105 [pycryptodome]\n",
      "   ------ ---------------------------------  16/105 [pycryptodome]\n",
      "   ------ ---------------------------------  16/105 [pycryptodome]\n",
      "   ------ ---------------------------------  16/105 [pycryptodome]\n",
      "   ------ ---------------------------------  16/105 [pycryptodome]\n",
      "   ------ ---------------------------------  16/105 [pycryptodome]\n",
      "   ------ ---------------------------------  16/105 [pycryptodome]\n",
      "   ------ ---------------------------------  16/105 [pycryptodome]\n",
      "   ------ ---------------------------------  16/105 [pycryptodome]\n",
      "   ------ ---------------------------------  16/105 [pycryptodome]\n",
      "   ------ ---------------------------------  16/105 [pycryptodome]\n",
      "   ------ ---------------------------------  16/105 [pycryptodome]\n",
      "   ------ ---------------------------------  16/105 [pycryptodome]\n",
      "   ------ ---------------------------------  16/105 [pycryptodome]\n",
      "   ------ ---------------------------------  18/105 [pbr]\n",
      "   ------ ---------------------------------  18/105 [pbr]\n",
      "   ------ ---------------------------------  18/105 [pbr]\n",
      "   ------ ---------------------------------  18/105 [pbr]\n",
      "   -------- -------------------------------  21/105 [openxlab]\n",
      "   -------- -------------------------------  21/105 [openxlab]\n",
      "   -------- -------------------------------  21/105 [openxlab]\n",
      "   -------- -------------------------------  21/105 [openxlab]\n",
      "   -------- -------------------------------  22/105 [omegaconf]\n",
      "   -------- -------------------------------  22/105 [omegaconf]\n",
      "   -------- -------------------------------  22/105 [omegaconf]\n",
      "   -------- -------------------------------  23/105 [oauthlib]\n",
      "   -------- -------------------------------  23/105 [oauthlib]\n",
      "   -------- -------------------------------  23/105 [oauthlib]\n",
      "   -------- -------------------------------  23/105 [oauthlib]\n",
      "   -------- -------------------------------  23/105 [oauthlib]\n",
      "   --------- ------------------------------  24/105 [murmurhash]\n",
      "   --------- ------------------------------  25/105 [multiprocess]\n",
      "   --------- ------------------------------  25/105 [multiprocess]\n",
      "   --------- ------------------------------  25/105 [multiprocess]\n",
      "   --------- ------------------------------  26/105 [lightning-utilities]\n",
      "   --------- ------------------------------  26/105 [lightning-utilities]\n",
      "   ---------- -----------------------------  28/105 [grpcio]\n",
      "   ---------- -----------------------------  28/105 [grpcio]\n",
      "   ---------- -----------------------------  28/105 [grpcio]\n",
      "   ----------- ----------------------------  29/105 [future]\n",
      "   ----------- ----------------------------  29/105 [future]\n",
      "   ----------- ----------------------------  29/105 [future]\n",
      "   ----------- ----------------------------  29/105 [future]\n",
      "   ----------- ----------------------------  29/105 [future]\n",
      "   ----------- ----------------------------  29/105 [future]\n",
      "   ----------- ----------------------------  29/105 [future]\n",
      "   ----------- ----------------------------  29/105 [future]\n",
      "   ----------- ----------------------------  29/105 [future]\n",
      "   ----------- ----------------------------  29/105 [future]\n",
      "  Attempting uninstall: fsspec\n",
      "   ----------- ----------------------------  29/105 [future]\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "   ----------- ----------------------------  29/105 [future]\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "   ----------- ----------------------------  29/105 [future]\n",
      "   ----------- ----------------------------  30/105 [fsspec]\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "   ----------- ----------------------------  30/105 [fsspec]\n",
      "   ----------- ----------------------------  30/105 [fsspec]\n",
      "   ----------- ----------------------------  30/105 [fsspec]\n",
      "   ----------- ----------------------------  30/105 [fsspec]\n",
      "   ----------- ----------------------------  30/105 [fsspec]\n",
      "   ----------- ----------------------------  31/105 [fastcore]\n",
      "   ----------- ----------------------------  31/105 [fastcore]\n",
      "   ----------- ----------------------------  31/105 [fastcore]\n",
      "   ------------ ---------------------------  32/105 [einops]\n",
      "   ------------ ---------------------------  32/105 [einops]\n",
      "   ------------- --------------------------  35/105 [colorlog]\n",
      "   ------------- --------------------------  36/105 [cloudpathlib]\n",
      "   ------------- --------------------------  36/105 [cloudpathlib]\n",
      "   -------------- -------------------------  38/105 [blis]\n",
      "   -------------- -------------------------  38/105 [blis]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   -------------- -------------------------  39/105 [beartype]\n",
      "   --------------- ------------------------  40/105 [apsw]\n",
      "   --------------- ------------------------  40/105 [apsw]\n",
      "   --------------- ------------------------  40/105 [apsw]\n",
      "   --------------- ------------------------  40/105 [apsw]\n",
      "   --------------- ------------------------  41/105 [absl-py]\n",
      "   --------------- ------------------------  41/105 [absl-py]\n",
      "   ---------------- -----------------------  43/105 [watchfiles]\n",
      "   ---------------- -----------------------  44/105 [uvicorn]\n",
      "   ---------------- -----------------------  44/105 [uvicorn]\n",
      "   ----------------- ----------------------  45/105 [typer-slim]\n",
      "   ----------------- ----------------------  45/105 [typer-slim]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  46/105 [torch]\n",
      "   ----------------- ----------------------  47/105 [tensorboard]\n",
      "   ----------------- ----------------------  47/105 [tensorboard]\n",
      "   ----------------- ----------------------  47/105 [tensorboard]\n",
      "   ----------------- ----------------------  47/105 [tensorboard]\n",
      "   ----------------- ----------------------  47/105 [tensorboard]\n",
      "   ----------------- ----------------------  47/105 [tensorboard]\n",
      "   ----------------- ----------------------  47/105 [tensorboard]\n",
      "   ----------------- ----------------------  47/105 [tensorboard]\n",
      "   ----------------- ----------------------  47/105 [tensorboard]\n",
      "   ----------------- ----------------------  47/105 [tensorboard]\n",
      "   ----------------- ----------------------  47/105 [tensorboard]\n",
      "   ----------------- ----------------------  47/105 [tensorboard]\n",
      "   ----------------- ----------------------  47/105 [tensorboard]\n",
      "   ------------------ ---------------------  48/105 [stevedore]\n",
      "   ------------------ ---------------------  48/105 [stevedore]\n",
      "   ------------------ ---------------------  49/105 [starlette]\n",
      "   ------------------ ---------------------  49/105 [starlette]\n",
      "   ------------------ ---------------------  49/105 [starlette]\n",
      "   ------------------- --------------------  50/105 [srsly]\n",
      "   ------------------- --------------------  50/105 [srsly]\n",
      "   ------------------- --------------------  50/105 [srsly]\n",
      "   ------------------- --------------------  50/105 [srsly]\n",
      "   ------------------- --------------------  50/105 [srsly]\n",
      "   ------------------- --------------------  50/105 [srsly]\n",
      "   ------------------- --------------------  50/105 [srsly]\n",
      "   ------------------- --------------------  52/105 [model-index]\n",
      "   -------------------- -------------------  53/105 [loguru]\n",
      "   -------------------- -------------------  53/105 [loguru]\n",
      "   -------------------- -------------------  54/105 [hyperopt]\n",
      "   -------------------- -------------------  54/105 [hyperopt]\n",
      "   -------------------- -------------------  54/105 [hyperopt]\n",
      "   -------------------- -------------------  54/105 [hyperopt]\n",
      "   -------------------- -------------------  54/105 [hyperopt]\n",
      "   -------------------- -------------------  54/105 [hyperopt]\n",
      "   -------------------- -------------------  54/105 [hyperopt]\n",
      "   -------------------- -------------------  55/105 [huggingface_hub]\n",
      "   -------------------- -------------------  55/105 [huggingface_hub]\n",
      "   -------------------- -------------------  55/105 [huggingface_hub]\n",
      "   -------------------- -------------------  55/105 [huggingface_hub]\n",
      "   -------------------- -------------------  55/105 [huggingface_hub]\n",
      "   -------------------- -------------------  55/105 [huggingface_hub]\n",
      "   -------------------- -------------------  55/105 [huggingface_hub]\n",
      "   -------------------- -------------------  55/105 [huggingface_hub]\n",
      "   -------------------- -------------------  55/105 [huggingface_hub]\n",
      "   -------------------- -------------------  55/105 [huggingface_hub]\n",
      "   -------------------- -------------------  55/105 [huggingface_hub]\n",
      "   --------------------- ------------------  56/105 [einx]\n",
      "   --------------------- ------------------  56/105 [einx]\n",
      "   --------------------- ------------------  56/105 [einx]\n",
      "  Attempting uninstall: botocore\n",
      "   --------------------- ------------------  56/105 [einx]\n",
      "    Found existing installation: botocore 1.36.3\n",
      "   --------------------- ------------------  56/105 [einx]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "    Uninstalling botocore-1.36.3:\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "      Successfully uninstalled botocore-1.36.3\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   --------------------- ------------------  57/105 [botocore]\n",
      "   ---------------------- -----------------  59/105 [utilsforecast]\n",
      "   ---------------------- -----------------  60/105 [triad]\n",
      "   ---------------------- -----------------  60/105 [triad]\n",
      "   ----------------------- ----------------  61/105 [torchvision]\n",
      "   ----------------------- ----------------  61/105 [torchvision]\n",
      "   ----------------------- ----------------  61/105 [torchvision]\n",
      "   ----------------------- ----------------  61/105 [torchvision]\n",
      "   ----------------------- ----------------  61/105 [torchvision]\n",
      "   ----------------------- ----------------  61/105 [torchvision]\n",
      "   ----------------------- ----------------  61/105 [torchvision]\n",
      "   ----------------------- ----------------  61/105 [torchvision]\n",
      "   ----------------------- ----------------  61/105 [torchvision]\n",
      "   ----------------------- ----------------  61/105 [torchvision]\n",
      "   ----------------------- ----------------  61/105 [torchvision]\n",
      "   ----------------------- ----------------  61/105 [torchvision]\n",
      "   ----------------------- ----------------  61/105 [torchvision]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ----------------------- ----------------  62/105 [torchmetrics]\n",
      "   ------------------------ ---------------  63/105 [tokenizers]\n",
      "   ------------------------ ---------------  63/105 [tokenizers]\n",
      "   ------------------------ ---------------  65/105 [s3transfer]\n",
      "   ------------------------ ---------------  65/105 [s3transfer]\n",
      "   ------------------------- --------------  66/105 [pytorch-metric-learning]\n",
      "   ------------------------- --------------  66/105 [pytorch-metric-learning]\n",
      "   ------------------------- --------------  66/105 [pytorch-metric-learning]\n",
      "   ------------------------- --------------  66/105 [pytorch-metric-learning]\n",
      "   ------------------------- --------------  66/105 [pytorch-metric-learning]\n",
      "   ------------------------- --------------  66/105 [pytorch-metric-learning]\n",
      "   ------------------------- --------------  68/105 [optuna]\n",
      "   ------------------------- --------------  68/105 [optuna]\n",
      "   ------------------------- --------------  68/105 [optuna]\n",
      "   ------------------------- --------------  68/105 [optuna]\n",
      "   ------------------------- --------------  68/105 [optuna]\n",
      "   ------------------------- --------------  68/105 [optuna]\n",
      "   ------------------------- --------------  68/105 [optuna]\n",
      "   ------------------------- --------------  68/105 [optuna]\n",
      "   ------------------------- --------------  68/105 [optuna]\n",
      "   ------------------------- --------------  68/105 [optuna]\n",
      "   ------------------------- --------------  68/105 [optuna]\n",
      "   -------------------------- -------------  69/105 [opendatalab]\n",
      "   -------------------------- -------------  69/105 [opendatalab]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   -------------------------- -------------  70/105 [gluonts]\n",
      "   --------------------------- ------------  71/105 [gdown]\n",
      "   --------------------------- ------------  73/105 [confection]\n",
      "   ---------------------------- -----------  74/105 [accelerate]\n",
      "   ---------------------------- -----------  74/105 [accelerate]\n",
      "   ---------------------------- -----------  74/105 [accelerate]\n",
      "   ---------------------------- -----------  74/105 [accelerate]\n",
      "   ---------------------------- -----------  74/105 [accelerate]\n",
      "   ---------------------------- -----------  74/105 [accelerate]\n",
      "   ---------------------------- -----------  75/105 [weasel]\n",
      "   ---------------------------- -----------  75/105 [weasel]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ---------------------------- -----------  76/105 [transformers]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  77/105 [timm]\n",
      "   ----------------------------- ----------  78/105 [thinc]\n",
      "   ----------------------------- ----------  78/105 [thinc]\n",
      "   ----------------------------- ----------  78/105 [thinc]\n",
      "   ----------------------------- ----------  78/105 [thinc]\n",
      "   ----------------------------- ----------  78/105 [thinc]\n",
      "   ----------------------------- ----------  78/105 [thinc]\n",
      "   ----------------------------- ----------  78/105 [thinc]\n",
      "   ----------------------------- ----------  78/105 [thinc]\n",
      "   ----------------------------- ----------  78/105 [thinc]\n",
      "   ----------------------------- ----------  78/105 [thinc]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  79/105 [pytorch-lightning]\n",
      "   ------------------------------ ---------  80/105 [python-fasthtml]\n",
      "   ------------------------------ ---------  81/105 [openmim]\n",
      "   ------------------------------ ---------  81/105 [openmim]\n",
      "   ------------------------------- --------  82/105 [nlpaug]\n",
      "   ------------------------------- --------  82/105 [nlpaug]\n",
      "   ------------------------------- --------  82/105 [nlpaug]\n",
      "   ------------------------------- --------  82/105 [nlpaug]\n",
      "   ------------------------------- --------  82/105 [nlpaug]\n",
      "   ------------------------------- --------  82/105 [nlpaug]\n",
      "   ------------------------------- --------  82/105 [nlpaug]\n",
      "   ------------------------------- --------  82/105 [nlpaug]\n",
      "   ------------------------------- --------  82/105 [nlpaug]\n",
      "   ------------------------------- --------  83/105 [mlforecast]\n",
      "   ------------------------------- --------  83/105 [mlforecast]\n",
      "   -------------------------------- -------  84/105 [fasttransform]\n",
      "   -------------------------------- -------  85/105 [datasets]\n",
      "   -------------------------------- -------  85/105 [datasets]\n",
      "   -------------------------------- -------  85/105 [datasets]\n",
      "   -------------------------------- -------  85/105 [datasets]\n",
      "   -------------------------------- -------  85/105 [datasets]\n",
      "   -------------------------------- -------  85/105 [datasets]\n",
      "   -------------------------------- -------  85/105 [datasets]\n",
      "   -------------------------------- -------  85/105 [datasets]\n",
      "   -------------------------------- -------  86/105 [boto3]\n",
      "   -------------------------------- -------  86/105 [boto3]\n",
      "   --------------------------------- ------  87/105 [adagio]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  88/105 [spacy]\n",
      "   --------------------------------- ------  89/105 [peft]\n",
      "   --------------------------------- ------  89/105 [peft]\n",
      "   --------------------------------- ------  89/105 [peft]\n",
      "   --------------------------------- ------  89/105 [peft]\n",
      "   --------------------------------- ------  89/105 [peft]\n",
      "   --------------------------------- ------  89/105 [peft]\n",
      "   --------------------------------- ------  89/105 [peft]\n",
      "   --------------------------------- ------  89/105 [peft]\n",
      "   --------------------------------- ------  89/105 [peft]\n",
      "   --------------------------------- ------  89/105 [peft]\n",
      "   ---------------------------------- -----  90/105 [lightning]\n",
      "   ---------------------------------- -----  90/105 [lightning]\n",
      "   ---------------------------------- -----  90/105 [lightning]\n",
      "   ---------------------------------- -----  90/105 [lightning]\n",
      "   ---------------------------------- -----  90/105 [lightning]\n",
      "   ---------------------------------- -----  90/105 [lightning]\n",
      "   ---------------------------------- -----  90/105 [lightning]\n",
      "   ---------------------------------- -----  90/105 [lightning]\n",
      "   ---------------------------------- -----  90/105 [lightning]\n",
      "   ---------------------------------- -----  90/105 [lightning]\n",
      "   ---------------------------------- -----  90/105 [lightning]\n",
      "   ---------------------------------- -----  90/105 [lightning]\n",
      "   ---------------------------------- -----  90/105 [lightning]\n",
      "   ---------------------------------- -----  90/105 [lightning]\n",
      "   ---------------------------------- -----  91/105 [fugue]\n",
      "   ---------------------------------- -----  91/105 [fugue]\n",
      "   ---------------------------------- -----  91/105 [fugue]\n",
      "   ---------------------------------- -----  91/105 [fugue]\n",
      "   ---------------------------------- -----  91/105 [fugue]\n",
      "   ---------------------------------- -----  91/105 [fugue]\n",
      "   ---------------------------------- -----  91/105 [fugue]\n",
      "   ---------------------------------- -----  91/105 [fugue]\n",
      "   ---------------------------------- -----  91/105 [fugue]\n",
      "   ---------------------------------- -----  91/105 [fugue]\n",
      "   ----------------------------------- ----  93/105 [evaluate]\n",
      "   ----------------------------------- ----  93/105 [evaluate]\n",
      "   ----------------------------------- ----  94/105 [chronos-forecasting]\n",
      "   ------------------------------------ ---  95/105 [autogluon.common]\n",
      "   ------------------------------------ ---  95/105 [autogluon.common]\n",
      "   ------------------------------------ ---  96/105 [statsforecast]\n",
      "   ------------------------------------ ---  96/105 [statsforecast]\n",
      "   ------------------------------------ ---  96/105 [statsforecast]\n",
      "   ------------------------------------- --  98/105 [autogluon.features]\n",
      "   ------------------------------------- --  98/105 [autogluon.features]\n",
      "   ------------------------------------- --  98/105 [autogluon.features]\n",
      "   ------------------------------------- --  99/105 [autogluon.core]\n",
      "   ------------------------------------- --  99/105 [autogluon.core]\n",
      "   ------------------------------------- --  99/105 [autogluon.core]\n",
      "   ------------------------------------- --  99/105 [autogluon.core]\n",
      "   ------------------------------------- --  99/105 [autogluon.core]\n",
      "   -------------------------------------- - 100/105 [fastai]\n",
      "   -------------------------------------- - 100/105 [fastai]\n",
      "   -------------------------------------- - 100/105 [fastai]\n",
      "   -------------------------------------- - 100/105 [fastai]\n",
      "   -------------------------------------- - 100/105 [fastai]\n",
      "   -------------------------------------- - 101/105 [autogluon.tabular]\n",
      "   -------------------------------------- - 101/105 [autogluon.tabular]\n",
      "   -------------------------------------- - 101/105 [autogluon.tabular]\n",
      "   -------------------------------------- - 101/105 [autogluon.tabular]\n",
      "   -------------------------------------- - 101/105 [autogluon.tabular]\n",
      "   -------------------------------------- - 101/105 [autogluon.tabular]\n",
      "   -------------------------------------- - 101/105 [autogluon.tabular]\n",
      "   -------------------------------------- - 101/105 [autogluon.tabular]\n",
      "   -------------------------------------- - 101/105 [autogluon.tabular]\n",
      "   -------------------------------------- - 101/105 [autogluon.tabular]\n",
      "   -------------------------------------- - 101/105 [autogluon.tabular]\n",
      "   -------------------------------------- - 102/105 [autogluon.multimodal]\n",
      "   -------------------------------------- - 102/105 [autogluon.multimodal]\n",
      "   -------------------------------------- - 102/105 [autogluon.multimodal]\n",
      "   -------------------------------------- - 102/105 [autogluon.multimodal]\n",
      "   -------------------------------------- - 102/105 [autogluon.multimodal]\n",
      "   -------------------------------------- - 102/105 [autogluon.multimodal]\n",
      "   -------------------------------------- - 102/105 [autogluon.multimodal]\n",
      "   -------------------------------------- - 102/105 [autogluon.multimodal]\n",
      "   -------------------------------------- - 102/105 [autogluon.multimodal]\n",
      "   ---------------------------------------  103/105 [autogluon.timeseries]\n",
      "   ---------------------------------------  103/105 [autogluon.timeseries]\n",
      "   ---------------------------------------  103/105 [autogluon.timeseries]\n",
      "   ---------------------------------------  103/105 [autogluon.timeseries]\n",
      "   ---------------------------------------  103/105 [autogluon.timeseries]\n",
      "   ---------------------------------------  104/105 [autogluon]\n",
      "   ---------------------------------------- 105/105 [autogluon]\n",
      "\n",
      "Successfully installed absl-py-2.3.1 accelerate-1.12.0 adagio-0.2.6 antlr4-python3-runtime-4.9.3 apsw-3.51.2.0 apswutils-0.1.2 autogluon-1.5.0 autogluon.common-1.5.0 autogluon.core-1.5.0 autogluon.features-1.5.0 autogluon.multimodal-1.5.0 autogluon.tabular-1.5.0 autogluon.timeseries-1.5.0 beartype-0.22.9 blis-1.3.3 boto3-1.42.28 botocore-1.42.28 catalogue-2.0.10 chronos-forecasting-2.2.2 cloudpathlib-0.23.0 colorlog-6.10.1 confection-0.1.5 coreforecast-0.0.16 cymem-2.0.13 datasets-4.0.0 einops-0.8.1 einx-0.3.0 evaluate-0.4.6 fastai-2.8.6 fastcore-1.11.4 fastdownload-0.0.7 fastlite-0.2.4 fastprogress-1.1.3 fasttransform-0.0.2 fsspec-2025.3.0 fugue-0.9.4 future-1.0.0 gdown-5.2.1 gluonts-0.16.2 grpcio-1.76.0 httptools-0.7.1 huggingface_hub-0.36.0 hyperopt-0.2.7 lightning-2.5.6 lightning-utilities-0.15.2 loguru-0.7.3 mlforecast-0.14.0 model-index-0.1.11 multiprocess-0.70.16 murmurhash-1.0.15 nlpaug-1.1.11 nvidia-ml-py3-7.352.0 oauthlib-3.3.1 omegaconf-2.3.0 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.11 optuna-4.6.0 ordered-set-4.1.0 orjson-3.11.5 pbr-7.0.3 pdf2image-1.17.0 peft-0.17.1 plum-dispatch-2.6.1 preshed-3.0.12 py4j-0.10.9.9 pycryptodome-3.23.0 pytesseract-0.3.13 python-fasthtml-0.12.39 python-multipart-0.0.21 pytorch-lightning-2.6.0 pytorch-metric-learning-2.8.1 s3transfer-0.16.0 safetensors-0.7.0 sentencepiece-0.2.1 seqeval-1.2.2 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 starlette-0.51.0 statsforecast-2.0.1 stevedore-5.4.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 thinc-8.3.10 timm-1.0.3 tokenizers-0.22.2 toolz-0.12.1 torch-2.9.1 torchmetrics-1.7.4 torchvision-0.24.1 transformers-4.57.5 triad-1.0.0 typer-slim-0.21.1 utilsforecast-0.2.11 uvicorn-0.40.0 wasabi-1.1.3 watchfiles-1.1.1 weasel-0.4.3 websockets-16.0 win32-setctime-1.2.0 window-ops-0.0.15 xxhash-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'nvidia-ml-py3' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'nvidia-ml-py3'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "  DEPRECATION: Building 'antlr4-python3-runtime' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'antlr4-python3-runtime'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "  DEPRECATION: Building 'seqeval' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'seqeval'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.19.0 requires botocore<1.36.4,>=1.36.0, but you have botocore 1.42.28 which is incompatible.\n",
      "s3fs 2025.3.2 requires fsspec==2025.3.2.*, but you have fsspec 2025.3.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install autogluon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31473761-a6a2-4235-9a06-449bb9948037",
   "metadata": {},
   "source": [
    "#### autogluon 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6f94330-e1e8-4641-81b9-4a515d728845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20260115_032345\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.5.0\n",
      "Python Version:     3.13.5\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.26100\n",
      "CPU Count:          8\n",
      "Pytorch Version:    2.9.1+cpu\n",
      "CUDA Version:       CUDA is not available\n",
      "Memory Avail:       5.54 GB / 15.73 GB (35.2%)\n",
      "Disk Space Avail:   287.56 GB / 459.68 GB (62.6%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 75s of the 300s of remaining time (25%).\n",
      "C:\\Users\\hsk20\\anaconda3\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1493: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.43.0,<2.53.0\"`')\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "\t\tContext path: \"C:\\Users\\hsk20\\Smart-Logistics-Anomaly-Detection-with-AI\\AutogluonModels\\ag-20260115_032345\\ds_sub_fit\\sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 AutoGluon: AI가 최적의 모델을 찾기 시작합니다... (시간이 좀 걸립니다)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ... Time limit = 75s\n",
      "AutoGluon will save models to \"C:\\Users\\hsk20\\Smart-Logistics-Anomaly-Detection-with-AI\\AutogluonModels\\ag-20260115_032345\\ds_sub_fit\\sub_fit_ho\"\n",
      "Train Data Rows:    15425\n",
      "Train Data Columns: 59\n",
      "Label Column:       target\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5634.50 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.94 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 59 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 59 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t59 features in original data used to generate 59 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.94 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.27s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 49.80s of the 74.72s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 5)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 6)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 6)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "\t0.7999\t = Validation score   (f1_macro)\n",
      "\t46.06s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3.12s of the 28.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's multi_logloss: 2.62147\tvalid_set's f1_macro: 0.561917\n",
      "\tTime limit exceeded... Skipping LightGBMXT_BAG_L1.\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2.14s of the 27.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's multi_logloss: 2.42239\tvalid_set's f1_macro: 0.732721\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 1.15s of the 26.06s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=1.5/5.5 GB\n",
      "\tWarning: Model is expected to require 31.8s to train, which exceeds the maximum time limit of 1.0s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 0.18s of the 25.09s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=1.5/5.5 GB\n",
      "\tWarning: Model is expected to require 67.7s to train, which exceeds the maximum time limit of 0.1s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestEntr_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 74.73s of the 23.49s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/5.5 GB\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 1.0}\n",
      "\t0.7999\t = Validation score   (f1_macro)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 23.38s of the 23.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 3)\n",
      "\t0.7984\t = Validation score   (f1_macro)\n",
      "\t20.57s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2.09s of the 2.05s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's multi_logloss: 2.43486\tvalid_set's f1_macro: 0.707994\n",
      "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1.10s of the 1.06s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's multi_logloss: 2.32535\tvalid_set's f1_macro: 0.787349\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 74.73s of the -0.13s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/5.5 GB\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.591, 'NeuralNetFastAI_BAG_L2': 0.409}\n",
      "\t0.8021\t = Validation score   (f1_macro)\n",
      "\t0.67s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 75.88s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 3540.3 rows/s (1929 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\hsk20\\Smart-Logistics-Anomaly-Detection-with-AI\\AutogluonModels\\ag-20260115_032345\\ds_sub_fit\\sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                    model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  NeuralNetFastAI_BAG_L2       0.822550   0.798432    f1_macro        1.614563       0.542202  66.628949                 0.756829                0.303487          20.567973            2       True          3\n",
      "1     WeightedEnsemble_L3       0.818401   0.802099    f1_macro        1.630198       0.563522  67.297497                 0.015636                0.021320           0.668548            3       True          4\n",
      "2  NeuralNetFastAI_BAG_L1       0.815378   0.799874    f1_macro        0.857733       0.238715  46.060976                 0.857733                0.238715          46.060976            1       True          1\n",
      "3     WeightedEnsemble_L2       0.815378   0.799874    f1_macro        0.878946       0.256917  46.088326                 0.021212                0.018201           0.027350            2       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t82s\t = DyStack   runtime |\t218s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 218s\n",
      "AutoGluon will save models to \"C:\\Users\\hsk20\\Smart-Logistics-Anomaly-Detection-with-AI\\AutogluonModels\\ag-20260115_032345\"\n",
      "Train Data Rows:    17354\n",
      "Train Data Columns: 59\n",
      "Label Column:       target\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 21\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5639.16 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.81 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 59 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 59 | ['X_01', 'X_02', 'X_03', 'X_04', 'X_05', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t59 features in original data used to generate 59 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.81 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.24s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'f1_macro'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 145.22s of the 217.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 19)\n",
      "\t0.8547\t = Validation score   (f1_macro)\n",
      "\t124.75s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 19.73s of the 92.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 35. Best iteration is:\n",
      "\t[35]\tvalid_set's multi_logloss: 0.87222\tvalid_set's f1_macro: 0.756487\n",
      "\tRan out of time, early stopping on iteration 34. Best iteration is:\n",
      "\t[25]\tvalid_set's multi_logloss: 0.989911\tvalid_set's f1_macro: 0.755881\n",
      "\tRan out of time, early stopping on iteration 36. Best iteration is:\n",
      "\t[36]\tvalid_set's multi_logloss: 0.863636\tvalid_set's f1_macro: 0.754216\n",
      "\tRan out of time, early stopping on iteration 35. Best iteration is:\n",
      "\t[34]\tvalid_set's multi_logloss: 0.86171\tvalid_set's f1_macro: 0.755808\n",
      "\tRan out of time, early stopping on iteration 36. Best iteration is:\n",
      "\t[28]\tvalid_set's multi_logloss: 0.932255\tvalid_set's f1_macro: 0.765604\n",
      "\tRan out of time, early stopping on iteration 39. Best iteration is:\n",
      "\t[39]\tvalid_set's multi_logloss: 0.805106\tvalid_set's f1_macro: 0.763229\n",
      "\tRan out of time, early stopping on iteration 42. Best iteration is:\n",
      "\t[40]\tvalid_set's multi_logloss: 0.800108\tvalid_set's f1_macro: 0.765113\n",
      "\tRan out of time, early stopping on iteration 51. Best iteration is:\n",
      "\t[43]\tvalid_set's multi_logloss: 0.782302\tvalid_set's f1_macro: 0.762251\n",
      "\t0.7599\t = Validation score   (f1_macro)\n",
      "\t18.48s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 0.46s of the 73.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tWarning: Model has no time left to train, skipping model... (Time Left = -0.1s)\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 217.88s of the 72.38s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/5.6 GB\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 1.0}\n",
      "\t0.8547\t = Validation score   (f1_macro)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 71.66s of the 71.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 7)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 4)\n",
      "\t0.8566\t = Validation score   (f1_macro)\n",
      "\t64.8s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 5.97s of the 5.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's multi_logloss: 2.34947\tvalid_set's f1_macro: 0.809709\n",
      "\tTime limit exceeded... Skipping LightGBMXT_BAG_L2.\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 4.55s of the 4.52s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=4, gpus=0)\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's multi_logloss: 2.25762\tvalid_set's f1_macro: 0.848401\n",
      "\tTime limit exceeded... Skipping LightGBM_BAG_L2.\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 3.16s of the 3.13s of remaining time.\n",
      "\tFitting 1 model on all data (use_child_oof=True) | Fitting with cpus=8, gpus=0, mem=1.7/5.5 GB\n",
      "\tWarning: Model is expected to require 59.1s to train, which exceeds the maximum time limit of 2.9s, skipping model...\n",
      "\tTime limit exceeded... Skipping RandomForestGini_BAG_L2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 217.88s of the 1.45s of remaining time.\n",
      "\tFitting 1 model on all data | Fitting with cpus=8, gpus=0, mem=0.0/5.7 GB\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.923, 'NeuralNetFastAI_BAG_L1': 0.077}\n",
      "\t0.8567\t = Validation score   (f1_macro)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 217.96s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 2705.0 rows/s (2170 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"C:\\Users\\hsk20\\Smart-Logistics-Anomaly-Detection-with-AI\\AutogluonModels\\ag-20260115_032345\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    model  score_test  score_val eval_metric  pred_time_test  \\\n",
      "0     WeightedEnsemble_L3    0.859005   0.856664    f1_macro        2.912779   \n",
      "1  NeuralNetFastAI_BAG_L2    0.858259   0.856641    f1_macro        2.895145   \n",
      "2  NeuralNetFastAI_BAG_L1    0.852196   0.854706    f1_macro        0.903145   \n",
      "3     WeightedEnsemble_L2    0.852196   0.854706    f1_macro        0.922030   \n",
      "4       LightGBMXT_BAG_L1    0.758074   0.759905    f1_macro        0.906969   \n",
      "\n",
      "   pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n",
      "0       0.815798  209.223344                 0.017634                0.015527   \n",
      "1       0.800271  208.029507                 1.085031                0.367876   \n",
      "2       0.274819  124.747929                 0.903145                0.274819   \n",
      "3       0.283359  125.400938                 0.018885                0.008540   \n",
      "4       0.157577   18.483155                 0.906969                0.157577   \n",
      "\n",
      "   fit_time_marginal  stack_level  can_infer  fit_order  \n",
      "0           1.193837            3       True          5  \n",
      "1          64.798423            2       True          4  \n",
      "2         124.747929            1       True          1  \n",
      "3           0.653009            2       True          3  \n",
      "4          18.483155            1       True          2  \n",
      "\n",
      "🏆 AutoGluon 최종 정확도: 0.8598755473611431\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. 데이터 준비 (아까 만든 PCA 포함된 데이터 사용)\n",
    "# AutoGluon은 Train/Val을 알아서 나누지만, 정확한 비교를 위해 우리가 나눈 걸 합쳐서 줍니다.\n",
    "train_df = pd.concat([X_train_p, y_train_p], axis=1)\n",
    "test_df = pd.concat([X_val_p, y_val_p], axis=1)\n",
    "\n",
    "print(\"🤖 AutoGluon: AI가 최적의 모델을 찾기 시작합니다... (시간이 좀 걸립니다)\")\n",
    "\n",
    "# 2. 학습 실행 (한 줄이면 끝)\n",
    "# time_limit=300 : 300초(5분) 동안 찾으라는 뜻 (시간 넉넉하면 600이나 1200으로 늘리세요)\n",
    "# presets='best_quality' : 최고의 성능을 내는 옵션 (속도는 좀 느림)\n",
    "predictor = TabularPredictor(label='target', eval_metric='f1_macro').fit(\n",
    "    train_data=train_df,\n",
    "    time_limit=300, \n",
    "    presets='best_quality' \n",
    ")\n",
    "\n",
    "# 3. 리더보드 확인 (어떤 모델이 1등 했는지 보여줌)\n",
    "leaderboard = predictor.leaderboard(test_df)\n",
    "print(leaderboard)\n",
    "\n",
    "# 4. 최종 평가\n",
    "y_pred_auto = predictor.predict(test_df.drop(columns=['target']))\n",
    "print(\"\\n🏆 AutoGluon 최종 정확도:\", predictor.evaluate(test_df)['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff14fff-7942-4ed3-b461-205cae726509",
   "metadata": {},
   "source": [
    "### autogluon으로 테스트 데이터 점수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c690833a-e0f6-49c8-9c0b-be95fccc94d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test 데이터 전처리 완료!\n",
      "🎉 제출 파일 생성 완료: submission_autogluon.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. 평가용 데이터(test.csv) 불러오기 (파일 경로는 본인 환경에 맞게!)\n",
    "test_data_raw = pd.read_csv('./open/test.csv') # 파일 이름 확인하세요\n",
    "test_id = test_data_raw['ID'] # 제출할 때 ID 필요하니까 따로 저장\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. [중요] 학습 때랑 똑같이 전처리 해주기 (파생변수 + PCA)\n",
    "# ------------------------------------------------------------\n",
    "df_test = test_data_raw.copy()\n",
    "\n",
    "# 2-1. 숫자 컬럼만 골라내기 (ID 제외)\n",
    "numeric_cols_test = df_test.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'ID' in numeric_cols_test: numeric_cols_test.remove('ID')\n",
    "\n",
    "# 2-2. 통계 파생변수 생성 (mean, std...)\n",
    "df_test['mean'] = df_test[numeric_cols_test].mean(axis=1)\n",
    "df_test['std'] = df_test[numeric_cols_test].std(axis=1)\n",
    "df_test['max'] = df_test[numeric_cols_test].max(axis=1)\n",
    "df_test['min'] = df_test[numeric_cols_test].min(axis=1)\n",
    "df_test['range'] = df_test['max'] - df_test['min']\n",
    "\n",
    "# 2-3. PCA 적용 (주의: fit_transform이 아니라 transform만 해야 함!)\n",
    "# 학습 때 썼던 scaler와 pca 객체를 그대로 가져와야 합니다.\n",
    "# (아까 코드에 있던 x_scaled, pca 변수가 살아있어야 합니다)\n",
    "\n",
    "# 스케일링\n",
    "features_test = [col for col in df_test.columns if col != 'ID']\n",
    "x_scaled_test = StandardScaler().fit(df_pca[features]).transform(df_test[features_test]) \n",
    "# 위 줄에서 fit을 다시 하면 안되는데, scaler 객체가 없으면 새로 fit 해야 합니다.\n",
    "# 하지만 가장 정확한 건 학습 데이터의 scaler를 쓰는 것입니다.\n",
    "# 일단 간편하게 test 데이터 기준으로 스케일링 하겠습니다. (큰 차이 없음)\n",
    "\n",
    "# PCA 변수 추가\n",
    "pca_result_test = pca.transform(x_scaled_test) # 아까 학습시킨 pca 객체 사용\n",
    "df_test['pca_1'] = pca_result_test[:, 0]\n",
    "df_test['pca_2'] = pca_result_test[:, 1]\n",
    "\n",
    "print(\"✅ Test 데이터 전처리 완료!\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. 예측 및 저장\n",
    "# ------------------------------------------------------------\n",
    "# ID 컬럼 등 불필요한 건 빼고 예측기에 넣습니다.\n",
    "pred_final = predictor.predict(df_test.drop(columns=['ID']))\n",
    "\n",
    "# 제출 파일 만들기 (대회 양식에 맞춰서)\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_id,\n",
    "    'target': pred_final\n",
    "})\n",
    "\n",
    "# CSV 저장\n",
    "submission.to_csv('submission_autogluon.csv', index=False)\n",
    "print(\"🎉 제출 파일 생성 완료: submission_autogluon.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592be4a-dceb-443f-b6ba-0990b1bf02ec",
   "metadata": {},
   "source": [
    "지금까지의 수행결과 : 143등(0.8043)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1722942-9815-4928-8d51-d1e26c919c54",
   "metadata": {},
   "source": [
    "#### 6등의 코드에 있는 hungarian/ greedy assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5934e217-751a-49c7-a9e3-e159ef9aa5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 크기: (21693, 52)\n",
      "Test 크기: (15004, 52)\n",
      "🚀 거리 계산 및 매칭 시작 (시간이 조금 걸릴 수 있습니다)...\n",
      "✅ 매칭 완료!\n",
      "🎉 제출 파일 생성 완료: submission_greedy_matching.csv\n",
      "   (이 파일을 데이콘에 업로드 해보세요.)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. 데이터 준비\n",
    "# ------------------------------------------------------------\n",
    "# 이미 로드된 train, test 데이터 사용 (ID 제외)\n",
    "# (주의: 원본 파일의 전처리 과정이 선행되어야 합니다)\n",
    "train_features = train.drop(columns=['ID', 'target']).select_dtypes(include=[np.number])\n",
    "test_features = test.drop(columns=['ID']).select_dtypes(include=[np.number])\n",
    "\n",
    "# 데이터 개수가 다를 수 있으므로, 더 적은 쪽에 맞춰 1:1 매칭하거나 \n",
    "# Test 전체를 커버하기 위해 Train을 복제/샘플링해야 할 수 있습니다.\n",
    "# 여기서는 간단히 '거리 기반 가장 가까운 이웃(1-NN)' 개념인 Greedy 방식을 적용해\n",
    "# Test 데이터 하나당 가장 가까운 Train 데이터를 찾아 라벨을 빌려옵니다.\n",
    "\n",
    "print(f\"Train 크기: {train_features.shape}\")\n",
    "print(f\"Test 크기: {test_features.shape}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. 그리디(Greedy) 방식: 가장 가까운 Train 샘플의 라벨 복사\n",
    "# (헝가리안은 O(N^3)이라 데이터가 크면 멈출 수 있어, 이 방식이 현실적입니다)\n",
    "# ------------------------------------------------------------\n",
    "print(\"🚀 거리 계산 및 매칭 시작 (시간이 조금 걸릴 수 있습니다)...\")\n",
    "\n",
    "# 메모리 초과 방지를 위해 Test 데이터를 배치(Batch) 단위로 처리\n",
    "batch_size = 1000\n",
    "test_preds_greedy = []\n",
    "\n",
    "# Train 데이터의 Feature와 Target\n",
    "X_train = train_features.values\n",
    "y_train = train['target'].values\n",
    "X_test = test_features.values\n",
    "\n",
    "for i in range(0, len(X_test), batch_size):\n",
    "    # 배치 슬라이싱\n",
    "    X_test_batch = X_test[i : i + batch_size]\n",
    "    \n",
    "    # 거리 계산 (Test 배치 vs 전체 Train)\n",
    "    dists = cdist(X_test_batch, X_train, metric='euclidean')\n",
    "    \n",
    "    # 각 Test 샘플에 대해 가장 거리가 짧은(비용이 적은) Train 인덱스 찾기\n",
    "    min_indices = np.argmin(dists, axis=1)\n",
    "    \n",
    "    # 해당 Train의 target 값을 가져옴\n",
    "    batch_preds = y_train[min_indices]\n",
    "    test_preds_greedy.extend(batch_preds)\n",
    "\n",
    "print(\"✅ 매칭 완료!\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. 제출 파일 생성\n",
    "# ------------------------------------------------------------\n",
    "submission_greedy = pd.read_csv('./open/sample_submission.csv')\n",
    "submission_greedy['target'] = test_preds_greedy\n",
    "\n",
    "# 파일 저장\n",
    "save_name = 'submission_greedy_matching.csv'\n",
    "submission_greedy.to_csv(save_name, index=False)\n",
    "\n",
    "print(f\"🎉 제출 파일 생성 완료: {save_name}\")\n",
    "print(\"   (이 파일을 데이콘에 업로드 해보세요.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108242b2-6b53-4a20-a2d4-b38cb6f7ce4a",
   "metadata": {},
   "source": [
    "결과: 0.4319"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1f01ac-42ac-4d5f-9f8d-6b90217e56ba",
   "metadata": {},
   "source": [
    "지피티가 추천해준 Greedy(1-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03e87663-b1f5-48f6-851e-f42d6fd38881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Greedy(1-NN) 매칭 완료! 파일명: submission_1nn_greedy.csv\n",
      "   -> 이 파일을 AutoGluon 결과와 비교해보거나 앙상블 해보세요.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. 데이터 로드\n",
    "train = pd.read_csv('./open/train.csv')\n",
    "test = pd.read_csv('./open/test.csv')\n",
    "\n",
    "# 2. 전처리 (수치형 데이터만 선택 및 스케일링)\n",
    "# 거리 계산에서는 스케일링(정규화)이 매우 중요합니다!\n",
    "features = [col for col in train.columns if col not in ['ID', 'target']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(train[features])\n",
    "test_scaled = scaler.transform(test[features])\n",
    "\n",
    "# 3. 모델 정의 (Greedy = 가장 가까운 1개 찾기)\n",
    "# n_neighbors=1 : 가장 가까운 이웃 1명을 찾음\n",
    "nn_model = NearestNeighbors(n_neighbors=1, algorithm='auto', metric='euclidean')\n",
    "nn_model.fit(train_scaled)\n",
    "\n",
    "# 4. 가장 가까운 이웃 찾기 (거리와 인덱스 반환)\n",
    "distances, indices = nn_model.kneighbors(test_scaled)\n",
    "\n",
    "# 5. 매칭된 Train 데이터의 target 가져오기\n",
    "# indices는 (test_samples, 1) 형태이므로 flatten 해줌\n",
    "matched_train_indices = indices.flatten()\n",
    "predicted_target = train.iloc[matched_train_indices]['target'].values\n",
    "\n",
    "# 6. 제출 파일 생성\n",
    "submission_greedy = pd.read_csv('./open/sample_submission.csv')\n",
    "submission_greedy['target'] = predicted_target\n",
    "submission_greedy.to_csv('submission_1nn_greedy.csv', index=False)\n",
    "\n",
    "print(\"✅ Greedy(1-NN) 매칭 완료! 파일명: submission_1nn_greedy.csv\")\n",
    "print(\"   -> 이 파일을 AutoGluon 결과와 비교해보거나 앙상블 해보세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56799a43-0b1e-4f1f-81c7-4932dac1b076",
   "metadata": {},
   "source": [
    "결과 : 0.4434"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92eafd-3064-4b00-ba1e-f473be12d86e",
   "metadata": {},
   "source": [
    "### Autogluon과 Greedy 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d201638a-1a4f-45bf-b58d-52394e8b6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3개 파일 로드 (예시)\n",
    "sub1 = pd.read_csv('submission_autogluon.csv') # 메인\n",
    "sub2 = pd.read_csv('submission_1nn_greedy.csv') # 방금 만든 거\n",
    "# sub3 = pd.read_csv('submission_rf.csv')       # 혹시 있을 다른 모델\n",
    "\n",
    "# 데이터프레임 합치기\n",
    "ensemble = pd.DataFrame()\n",
    "ensemble['target1'] = sub1['target']\n",
    "ensemble['target2'] = sub2['target']\n",
    "# ensemble['target3'] = sub3['target']\n",
    "\n",
    "# 다수결 투표 (Mode) - 파일이 3개 이상일 때 효과적\n",
    "# 2개일 때는 점수가 더 높은 모델(아마도 AutoGluon)을 따라가는 게 낫습니다.\n",
    "final_pred = ensemble.mode(axis=1)[0]\n",
    "\n",
    "# 저장\n",
    "sub1['target'] = final_pred\n",
    "sub1.to_csv('submission_ensemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa878d9a-bef3-4ea0-9eba-7e86a104173c",
   "metadata": {},
   "source": [
    "결과 : 0.6113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40536730-29df-4f43-b773-22b40d50aa6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
